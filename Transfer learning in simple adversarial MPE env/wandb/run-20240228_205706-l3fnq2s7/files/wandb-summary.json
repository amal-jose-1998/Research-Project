{"rewards for each step": {"adversary_0": -0.6100190055882132, "agent_0": -0.5730140112436864, "agent_1": -0.5730140112436864, "agent_2": -0.5730140112436864}, "_timestamp": 1709150279.749328, "_runtime": 52.913015842437744, "_step": 638, "total episode rewards during training": {"adversary_0": -29.636035919189453, "agent_0": -4.5718770027160645, "agent_1": -4.5718770027160645, "agent_2": -4.5718770027160645}, "training Loss for agent0": 0.5738092064857483, "training Loss for agent1": 0.28766465187072754, "training Loss for agent2": 0.31230783462524414, "training Loss for agent3": 0.22214572131633759, "training step": 85, "evaluation_env  avg_reward": {"adversary_0": -68.88511110320522, "agent_0": 42.06054288007107, "agent_1": 42.06054288007107, "agent_2": 42.06054288007107}, "total steps": 186, "episode": 7}