{"rewards for each step": {"adversary_0": -1.597861511182583, "agent_0": -0.08487941916451036, "agent_1": -0.08487941916451036, "agent_2": -0.08487941916451036}, "_timestamp": 1709151520.3812435, "_runtime": 600.0323693752289, "_step": 14564, "total episode rewards during training": {"adversary_0": -29.242460250854492, "agent_0": -11.62733268737793, "agent_1": -11.62733268737793, "agent_2": -11.62733268737793}, "training Loss for agent0": 0.8298649787902832, "training Loss for agent1": 0.5072879791259766, "training Loss for agent2": 0.7074300646781921, "training Loss for agent3": 0.31233835220336914, "training step": 2301, "evaluation_env  avg_reward": {"adversary_0": -70.15125133981458, "agent_0": 46.44497750683666, "agent_1": 46.44497750683666, "agent_2": 46.44497750683666}, "total steps": 2498, "episode": 99, "_wandb": {"runtime": 599}}