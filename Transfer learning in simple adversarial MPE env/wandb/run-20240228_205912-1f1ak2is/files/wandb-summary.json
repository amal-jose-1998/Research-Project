{"rewards for each step": {"adversary_0": -1.5399962992887424, "agent_0": 0.3440896306713821, "agent_1": 0.3440896306713821, "agent_2": 0.3440896306713821}, "_timestamp": 1709150403.6050413, "_runtime": 51.073925256729126, "_step": 512, "total episode rewards during training": {"adversary_0": -22.297048568725586, "agent_0": -10.519055366516113, "agent_1": -10.519055366516113, "agent_2": -10.519055366516113}, "training Loss for agent0": 0.6103619933128357, "training Loss for agent1": 0.25872206687927246, "training Loss for agent2": 0.46870702505111694, "training Loss for agent3": 0.27916646003723145, "training step": 65, "evaluation_env  avg_reward": {"adversary_0": -70.22921395237957, "agent_0": 30.46430632030306, "agent_1": 30.46430632030306, "agent_2": 30.46430632030306}, "total steps": 165, "episode": 6}