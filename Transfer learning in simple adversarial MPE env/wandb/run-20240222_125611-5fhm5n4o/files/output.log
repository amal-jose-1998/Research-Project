episode: 0
episode 0 terminated at 25
episode: 1
episode 1 terminated at 50
episode: 2
episode 2 terminated at 75
episode: 3
episode 3 terminated at 100
episode: 4
episode:  4 training step:  1 loss of agent  0 :  0.2851502001285553
episode:  4 training step:  1 loss of agent  1 :  0.386062890291214
episode:  4 training step:  1 loss of agent  2 :  0.4064282476902008
episode:  4 training step:  2 loss of agent  0 :  0.2678391933441162
episode:  4 training step:  2 loss of agent  1 :  0.43517571687698364
episode:  4 training step:  2 loss of agent  2 :  0.22432884573936462
episode:  4 training step:  3 loss of agent  0 :  0.25903257727622986
episode:  4 training step:  3 loss of agent  1 :  0.2978244721889496
episode:  4 training step:  3 loss of agent  2 :  0.40250784158706665
episode:  4 training step:  4 loss of agent  0 :  0.22165429592132568
episode:  4 training step:  4 loss of agent  1 :  0.3465925455093384
episode:  4 training step:  4 loss of agent  2 :  0.2955285310745239
episode:  4 training step:  5 loss of agent  0 :  0.23258042335510254
episode:  4 training step:  5 loss of agent  1 :  0.31713584065437317
episode:  4 training step:  5 loss of agent  2 :  0.2919127643108368
episode:  4 training step:  6 loss of agent  0 :  0.2895933985710144
episode:  4 training step:  6 loss of agent  1 :  0.3448764979839325
episode:  4 training step:  6 loss of agent  2 :  0.2586818039417267
episode:  4 training step:  7 loss of agent  0 :  0.269684374332428
episode:  4 training step:  7 loss of agent  1 :  0.26233798265457153
episode:  4 training step:  7 loss of agent  2 :  0.3871418833732605
episode:  4 training step:  8 loss of agent  0 :  0.2656921148300171
episode:  4 training step:  8 loss of agent  1 :  0.300375759601593
episode:  4 training step:  8 loss of agent  2 :  0.30928516387939453
episode:  4 training step:  9 loss of agent  0 :  0.25723886489868164
episode:  4 training step:  9 loss of agent  1 :  0.25762131810188293
episode:  4 training step:  9 loss of agent  2 :  0.2836126387119293
episode:  4 training step:  10 loss of agent  0 :  0.2584127187728882
episode:  4 training step:  10 loss of agent  1 :  0.3325433135032654
episode:  4 training step:  10 loss of agent  2 :  0.34331509470939636
Evaluation
env seed: 6 evaluation score at the training step:  10 :  {'adversary_0': -32.40370364050509, 'agent_0': 17.835606237684747, 'agent_1': 17.835606237684747}
Traceback (most recent call last):
  File "c:\Users\amalj\OneDrive\Desktop\lecture materials\S3\Research Project\Local Git Repo\Adversarial-MATL\Transfer learning in simple adversarial MPE env\main.py", line 89, in <module>
    main()
  File "c:\Users\amalj\OneDrive\Desktop\lecture materials\S3\Research Project\Local Git Repo\Adversarial-MATL\Transfer learning in simple adversarial MPE env\main.py", line 69, in main
    loop_iteration(num_games, env_pretrain, eval_env_pretrain, opt, agent_models, agent_buffers)
  File "c:\Users\amalj\OneDrive\Desktop\lecture materials\S3\Research Project\Local Git Repo\Adversarial-MATL\Transfer learning in simple adversarial MPE env\utils.py", line 98, in loop_iteration
    model = agent_models[a]
IndexError: list index out of range