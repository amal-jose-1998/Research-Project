{"rewards for each step": {"adversary_0": -0.8926492109552285, "agent_0": -0.9262588171455649, "agent_1": -0.9262588171455649, "agent_2": -0.9262588171455649}, "_timestamp": 1709149788.6170948, "_runtime": 375.1787476539612, "_step": 8422, "total episode rewards during training": {"adversary_0": -21.09585952758789, "agent_0": -5.610581874847412, "agent_1": -5.610581874847412, "agent_2": -5.610581874847412}, "training Loss for agent0": 0.20407293736934662, "training Loss for agent1": 0.055803533643484116, "training Loss for agent2": 0.17838260531425476, "training Loss for agent3": 0.05005037784576416, "training step": 1323, "evaluation_env  avg_reward": {"adversary_0": -42.22350150960746, "agent_0": 19.764587988964557, "agent_1": 19.764587988964557, "agent_2": 19.764587988964557}, "total steps": 1478, "episode": 59}