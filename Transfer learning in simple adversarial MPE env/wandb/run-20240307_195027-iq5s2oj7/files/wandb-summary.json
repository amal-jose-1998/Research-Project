{"rewards for each step": {"adversary_0": -0.5260046314180314, "agent_0": -0.7194152068287981, "agent_1": -0.7194152068287981, "agent_2": -0.7194152068287981}, "_timestamp": 1709837491.0640233, "_runtime": 63.28357434272766, "_step": 608, "total episode rewards during training": {"adversary_0": -29.636035919189453, "agent_0": -4.5718770027160645, "agent_1": -4.5718770027160645, "agent_2": -4.5718770027160645}, "training Loss for agent0": 0.13221870362758636, "training Loss for agent1": 0.1890440285205841, "training Loss for agent2": 0.2044954150915146, "training Loss for agent3": 0.261741042137146, "training step": 80, "evaluation_env  avg_reward": {"adversary_0": -35.16373412400802, "agent_0": 10.731164640958392, "agent_1": 10.731164640958392, "agent_2": 10.731164640958392}, "total steps": 186, "episode": 3, "epoch": 1, "_wandb": {"runtime": 63}}