{"rewards for each step": {"adversary_0": -0.38663444322880414, "agent_0": -1.3227519831148777, "agent_1": -1.3227519831148777, "agent_2": -1.3227519831148777}, "_timestamp": 1709891687.4948084, "_runtime": 704.5306215286255, "_step": 7024, "total episode rewards during training": {"adversary_0": -14.54080581665039, "agent_0": -24.895265579223633, "agent_1": -24.895265579223633, "agent_2": -24.895265579223633}, "training Loss for agent0": 0.03503638878464699, "training Loss for agent1": 0.3056909441947937, "training Loss for agent2": 0.30408531427383423, "training Loss for agent3": 0.22981366515159607, "training step": 1101, "evaluation_env  avg_reward": {"adversary_0": -26.613778484781385, "agent_0": -2.7015821244814133, "agent_1": -2.7015821244814133, "agent_2": -2.7015821244814133}, "total steps": 1248, "episode": 24, "epoch": 1}