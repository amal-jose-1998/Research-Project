episode: 0
0.9999967333333334
episode:  0 training step:  1 loss of agent  0 :  {0: tensor(3.2268, grad_fn=<MseLossBackward0>)}
0.9999967333333334
episode:  0 training step:  1 loss of agent  1 :  {0: tensor(3.2268, grad_fn=<MseLossBackward0>), 1: tensor(0.7102, grad_fn=<MseLossBackward0>)}
0.9999967333333334
episode:  0 training step:  1 loss of agent  2 :  {0: tensor(3.2268, grad_fn=<MseLossBackward0>), 1: tensor(0.7102, grad_fn=<MseLossBackward0>), 2: tensor(0.6347, grad_fn=<MseLossBackward0>)}
0.9999934666666667
episode:  0 training step:  2 loss of agent  0 :  {0: tensor(3.1108, grad_fn=<MseLossBackward0>), 1: tensor(0.7102, grad_fn=<MseLossBackward0>), 2: tensor(0.6347, grad_fn=<MseLossBackward0>)}
0.9999934666666667
episode:  0 training step:  2 loss of agent  1 :  {0: tensor(3.1108, grad_fn=<MseLossBackward0>), 1: tensor(0.7124, grad_fn=<MseLossBackward0>), 2: tensor(0.6347, grad_fn=<MseLossBackward0>)}
0.9999934666666667
episode:  0 training step:  2 loss of agent  2 :  {0: tensor(3.1108, grad_fn=<MseLossBackward0>), 1: tensor(0.7124, grad_fn=<MseLossBackward0>), 2: tensor(0.6670, grad_fn=<MseLossBackward0>)}
0.9999902
episode:  0 training step:  3 loss of agent  0 :  {0: tensor(3.0137, grad_fn=<MseLossBackward0>), 1: tensor(0.7124, grad_fn=<MseLossBackward0>), 2: tensor(0.6670, grad_fn=<MseLossBackward0>)}
0.9999902
episode:  0 training step:  3 loss of agent  1 :  {0: tensor(3.0137, grad_fn=<MseLossBackward0>), 1: tensor(0.7013, grad_fn=<MseLossBackward0>), 2: tensor(0.6670, grad_fn=<MseLossBackward0>)}
0.9999902
episode:  0 training step:  3 loss of agent  2 :  {0: tensor(3.0137, grad_fn=<MseLossBackward0>), 1: tensor(0.7013, grad_fn=<MseLossBackward0>), 2: tensor(0.5209, grad_fn=<MseLossBackward0>)}
0.9999869333333333
episode:  0 training step:  4 loss of agent  0 :  {0: tensor(2.9062, grad_fn=<MseLossBackward0>), 1: tensor(0.7013, grad_fn=<MseLossBackward0>), 2: tensor(0.5209, grad_fn=<MseLossBackward0>)}
0.9999869333333333
episode:  0 training step:  4 loss of agent  1 :  {0: tensor(2.9062, grad_fn=<MseLossBackward0>), 1: tensor(0.7077, grad_fn=<MseLossBackward0>), 2: tensor(0.5209, grad_fn=<MseLossBackward0>)}
0.9999869333333333
episode:  0 training step:  4 loss of agent  2 :  {0: tensor(2.9062, grad_fn=<MseLossBackward0>), 1: tensor(0.7077, grad_fn=<MseLossBackward0>), 2: tensor(0.7250, grad_fn=<MseLossBackward0>)}
0.9999836666666667
episode:  0 training step:  5 loss of agent  0 :  {0: tensor(2.6891, grad_fn=<MseLossBackward0>), 1: tensor(0.7077, grad_fn=<MseLossBackward0>), 2: tensor(0.7250, grad_fn=<MseLossBackward0>)}
0.9999836666666667
episode:  0 training step:  5 loss of agent  1 :  {0: tensor(2.6891, grad_fn=<MseLossBackward0>), 1: tensor(0.5951, grad_fn=<MseLossBackward0>), 2: tensor(0.7250, grad_fn=<MseLossBackward0>)}
0.9999836666666667
episode:  0 training step:  5 loss of agent  2 :  {0: tensor(2.6891, grad_fn=<MseLossBackward0>), 1: tensor(0.5951, grad_fn=<MseLossBackward0>), 2: tensor(0.5800, grad_fn=<MseLossBackward0>)}
0.9999804
episode:  0 training step:  6 loss of agent  0 :  {0: tensor(2.7233, grad_fn=<MseLossBackward0>), 1: tensor(0.5951, grad_fn=<MseLossBackward0>), 2: tensor(0.5800, grad_fn=<MseLossBackward0>)}
0.9999804
episode:  0 training step:  6 loss of agent  1 :  {0: tensor(2.7233, grad_fn=<MseLossBackward0>), 1: tensor(0.4439, grad_fn=<MseLossBackward0>), 2: tensor(0.5800, grad_fn=<MseLossBackward0>)}
0.9999804
episode:  0 training step:  6 loss of agent  2 :  {0: tensor(2.7233, grad_fn=<MseLossBackward0>), 1: tensor(0.4439, grad_fn=<MseLossBackward0>), 2: tensor(0.5901, grad_fn=<MseLossBackward0>)}
0.9999771333333334
episode:  0 training step:  7 loss of agent  0 :  {0: tensor(2.9920, grad_fn=<MseLossBackward0>), 1: tensor(0.4439, grad_fn=<MseLossBackward0>), 2: tensor(0.5901, grad_fn=<MseLossBackward0>)}
0.9999771333333334
episode:  0 training step:  7 loss of agent  1 :  {0: tensor(2.9920, grad_fn=<MseLossBackward0>), 1: tensor(0.4789, grad_fn=<MseLossBackward0>), 2: tensor(0.5901, grad_fn=<MseLossBackward0>)}
0.9999771333333334
episode:  0 training step:  7 loss of agent  2 :  {0: tensor(2.9920, grad_fn=<MseLossBackward0>), 1: tensor(0.4789, grad_fn=<MseLossBackward0>), 2: tensor(0.5678, grad_fn=<MseLossBackward0>)}
0.9999738666666667
episode:  0 training step:  8 loss of agent  0 :  {0: tensor(3.0151, grad_fn=<MseLossBackward0>), 1: tensor(0.4789, grad_fn=<MseLossBackward0>), 2: tensor(0.5678, grad_fn=<MseLossBackward0>)}
0.9999738666666667
episode:  0 training step:  8 loss of agent  1 :  {0: tensor(3.0151, grad_fn=<MseLossBackward0>), 1: tensor(0.5260, grad_fn=<MseLossBackward0>), 2: tensor(0.5678, grad_fn=<MseLossBackward0>)}
0.9999738666666667
episode:  0 training step:  8 loss of agent  2 :  {0: tensor(3.0151, grad_fn=<MseLossBackward0>), 1: tensor(0.5260, grad_fn=<MseLossBackward0>), 2: tensor(0.6018, grad_fn=<MseLossBackward0>)}
0.9999706
episode:  0 training step:  9 loss of agent  0 :  {0: tensor(2.7754, grad_fn=<MseLossBackward0>), 1: tensor(0.5260, grad_fn=<MseLossBackward0>), 2: tensor(0.6018, grad_fn=<MseLossBackward0>)}
0.9999706
episode:  0 training step:  9 loss of agent  1 :  {0: tensor(2.7754, grad_fn=<MseLossBackward0>), 1: tensor(0.5155, grad_fn=<MseLossBackward0>), 2: tensor(0.6018, grad_fn=<MseLossBackward0>)}
0.9999706
episode:  0 training step:  9 loss of agent  2 :  {0: tensor(2.7754, grad_fn=<MseLossBackward0>), 1: tensor(0.5155, grad_fn=<MseLossBackward0>), 2: tensor(0.4614, grad_fn=<MseLossBackward0>)}
0.9999673333333333
episode:  0 training step:  10 loss of agent  0 :  {0: tensor(2.5985, grad_fn=<MseLossBackward0>), 1: tensor(0.5155, grad_fn=<MseLossBackward0>), 2: tensor(0.4614, grad_fn=<MseLossBackward0>)}
0.9999673333333333
episode:  0 training step:  10 loss of agent  1 :  {0: tensor(2.5985, grad_fn=<MseLossBackward0>), 1: tensor(0.5140, grad_fn=<MseLossBackward0>), 2: tensor(0.4614, grad_fn=<MseLossBackward0>)}
0.9999673333333333
episode:  0 training step:  10 loss of agent  2 :  {0: tensor(2.5985, grad_fn=<MseLossBackward0>), 1: tensor(0.5140, grad_fn=<MseLossBackward0>), 2: tensor(0.5174, grad_fn=<MseLossBackward0>)}
0.9999640666666667
episode:  0 training step:  11 loss of agent  0 :  {0: tensor(3.3643, grad_fn=<MseLossBackward0>), 1: tensor(0.5140, grad_fn=<MseLossBackward0>), 2: tensor(0.5174, grad_fn=<MseLossBackward0>)}
0.9999640666666667
episode:  0 training step:  11 loss of agent  1 :  {0: tensor(3.3643, grad_fn=<MseLossBackward0>), 1: tensor(0.6589, grad_fn=<MseLossBackward0>), 2: tensor(0.5174, grad_fn=<MseLossBackward0>)}
0.9999640666666667
episode:  0 training step:  11 loss of agent  2 :  {0: tensor(3.3643, grad_fn=<MseLossBackward0>), 1: tensor(0.6589, grad_fn=<MseLossBackward0>), 2: tensor(0.7161, grad_fn=<MseLossBackward0>)}
0.9999608
episode:  0 training step:  12 loss of agent  0 :  {0: tensor(3.2407, grad_fn=<MseLossBackward0>), 1: tensor(0.6589, grad_fn=<MseLossBackward0>), 2: tensor(0.7161, grad_fn=<MseLossBackward0>)}
0.9999608
episode:  0 training step:  12 loss of agent  1 :  {0: tensor(3.2407, grad_fn=<MseLossBackward0>), 1: tensor(0.8183, grad_fn=<MseLossBackward0>), 2: tensor(0.7161, grad_fn=<MseLossBackward0>)}
0.9999608
episode:  0 training step:  12 loss of agent  2 :  {0: tensor(3.2407, grad_fn=<MseLossBackward0>), 1: tensor(0.8183, grad_fn=<MseLossBackward0>), 2: tensor(0.7042, grad_fn=<MseLossBackward0>)}
0.9999575333333334
episode:  0 training step:  13 loss of agent  0 :  {0: tensor(3.1041, grad_fn=<MseLossBackward0>), 1: tensor(0.8183, grad_fn=<MseLossBackward0>), 2: tensor(0.7042, grad_fn=<MseLossBackward0>)}
0.9999575333333334
episode:  0 training step:  13 loss of agent  1 :  {0: tensor(3.1041, grad_fn=<MseLossBackward0>), 1: tensor(0.7580, grad_fn=<MseLossBackward0>), 2: tensor(0.7042, grad_fn=<MseLossBackward0>)}
0.9999575333333334
episode:  0 training step:  13 loss of agent  2 :  {0: tensor(3.1041, grad_fn=<MseLossBackward0>), 1: tensor(0.7580, grad_fn=<MseLossBackward0>), 2: tensor(0.7040, grad_fn=<MseLossBackward0>)}
0.9999542666666666
episode:  0 training step:  14 loss of agent  0 :  {0: tensor(3.1550, grad_fn=<MseLossBackward0>), 1: tensor(0.7580, grad_fn=<MseLossBackward0>), 2: tensor(0.7040, grad_fn=<MseLossBackward0>)}
0.9999542666666666
episode:  0 training step:  14 loss of agent  1 :  {0: tensor(3.1550, grad_fn=<MseLossBackward0>), 1: tensor(0.7895, grad_fn=<MseLossBackward0>), 2: tensor(0.7040, grad_fn=<MseLossBackward0>)}
0.9999542666666666
episode:  0 training step:  14 loss of agent  2 :  {0: tensor(3.1550, grad_fn=<MseLossBackward0>), 1: tensor(0.7895, grad_fn=<MseLossBackward0>), 2: tensor(0.6022, grad_fn=<MseLossBackward0>)}
0.999951
episode:  0 training step:  15 loss of agent  0 :  {0: tensor(3.0125, grad_fn=<MseLossBackward0>), 1: tensor(0.7895, grad_fn=<MseLossBackward0>), 2: tensor(0.6022, grad_fn=<MseLossBackward0>)}
0.999951
episode:  0 training step:  15 loss of agent  1 :  {0: tensor(3.0125, grad_fn=<MseLossBackward0>), 1: tensor(0.5108, grad_fn=<MseLossBackward0>), 2: tensor(0.6022, grad_fn=<MseLossBackward0>)}
0.999951
episode:  0 training step:  15 loss of agent  2 :  {0: tensor(3.0125, grad_fn=<MseLossBackward0>), 1: tensor(0.5108, grad_fn=<MseLossBackward0>), 2: tensor(0.6417, grad_fn=<MseLossBackward0>)}
0.9999477333333333
episode:  0 training step:  16 loss of agent  0 :  {0: tensor(2.9839, grad_fn=<MseLossBackward0>), 1: tensor(0.5108, grad_fn=<MseLossBackward0>), 2: tensor(0.6417, grad_fn=<MseLossBackward0>)}
0.9999477333333333
episode:  0 training step:  16 loss of agent  1 :  {0: tensor(2.9839, grad_fn=<MseLossBackward0>), 1: tensor(0.5264, grad_fn=<MseLossBackward0>), 2: tensor(0.6417, grad_fn=<MseLossBackward0>)}
0.9999477333333333
episode:  0 training step:  16 loss of agent  2 :  {0: tensor(2.9839, grad_fn=<MseLossBackward0>), 1: tensor(0.5264, grad_fn=<MseLossBackward0>), 2: tensor(0.5773, grad_fn=<MseLossBackward0>)}
0.9999444666666667
episode:  0 training step:  17 loss of agent  0 :  {0: tensor(2.9391, grad_fn=<MseLossBackward0>), 1: tensor(0.5264, grad_fn=<MseLossBackward0>), 2: tensor(0.5773, grad_fn=<MseLossBackward0>)}
0.9999444666666667
episode:  0 training step:  17 loss of agent  1 :  {0: tensor(2.9391, grad_fn=<MseLossBackward0>), 1: tensor(0.6529, grad_fn=<MseLossBackward0>), 2: tensor(0.5773, grad_fn=<MseLossBackward0>)}
0.9999444666666667
episode:  0 training step:  17 loss of agent  2 :  {0: tensor(2.9391, grad_fn=<MseLossBackward0>), 1: tensor(0.6529, grad_fn=<MseLossBackward0>), 2: tensor(0.5962, grad_fn=<MseLossBackward0>)}
0.9999412
episode:  0 training step:  18 loss of agent  0 :  {0: tensor(2.8926, grad_fn=<MseLossBackward0>), 1: tensor(0.6529, grad_fn=<MseLossBackward0>), 2: tensor(0.5962, grad_fn=<MseLossBackward0>)}
0.9999412
episode:  0 training step:  18 loss of agent  1 :  {0: tensor(2.8926, grad_fn=<MseLossBackward0>), 1: tensor(0.6062, grad_fn=<MseLossBackward0>), 2: tensor(0.5962, grad_fn=<MseLossBackward0>)}
0.9999412
episode:  0 training step:  18 loss of agent  2 :  {0: tensor(2.8926, grad_fn=<MseLossBackward0>), 1: tensor(0.6062, grad_fn=<MseLossBackward0>), 2: tensor(0.5356, grad_fn=<MseLossBackward0>)}
0.9999379333333334
episode:  0 training step:  19 loss of agent  0 :  {0: tensor(2.9395, grad_fn=<MseLossBackward0>), 1: tensor(0.6062, grad_fn=<MseLossBackward0>), 2: tensor(0.5356, grad_fn=<MseLossBackward0>)}
0.9999379333333334
episode:  0 training step:  19 loss of agent  1 :  {0: tensor(2.9395, grad_fn=<MseLossBackward0>), 1: tensor(0.5873, grad_fn=<MseLossBackward0>), 2: tensor(0.5356, grad_fn=<MseLossBackward0>)}
0.9999379333333334
episode:  0 training step:  19 loss of agent  2 :  {0: tensor(2.9395, grad_fn=<MseLossBackward0>), 1: tensor(0.5873, grad_fn=<MseLossBackward0>), 2: tensor(0.5951, grad_fn=<MseLossBackward0>)}
0.9999346666666666
episode:  0 training step:  20 loss of agent  0 :  {0: tensor(2.9034, grad_fn=<MseLossBackward0>), 1: tensor(0.5873, grad_fn=<MseLossBackward0>), 2: tensor(0.5951, grad_fn=<MseLossBackward0>)}
0.9999346666666666
episode:  0 training step:  20 loss of agent  1 :  {0: tensor(2.9034, grad_fn=<MseLossBackward0>), 1: tensor(0.4749, grad_fn=<MseLossBackward0>), 2: tensor(0.5951, grad_fn=<MseLossBackward0>)}
0.9999346666666666
episode:  0 training step:  20 loss of agent  2 :  {0: tensor(2.9034, grad_fn=<MseLossBackward0>), 1: tensor(0.4749, grad_fn=<MseLossBackward0>), 2: tensor(0.5576, grad_fn=<MseLossBackward0>)}
0.9999314
episode:  0 training step:  21 loss of agent  0 :  {0: tensor(3.1480, grad_fn=<MseLossBackward0>), 1: tensor(0.4749, grad_fn=<MseLossBackward0>), 2: tensor(0.5576, grad_fn=<MseLossBackward0>)}
0.9999314
episode:  0 training step:  21 loss of agent  1 :  {0: tensor(3.1480, grad_fn=<MseLossBackward0>), 1: tensor(0.7853, grad_fn=<MseLossBackward0>), 2: tensor(0.5576, grad_fn=<MseLossBackward0>)}
0.9999314
episode:  0 training step:  21 loss of agent  2 :  {0: tensor(3.1480, grad_fn=<MseLossBackward0>), 1: tensor(0.7853, grad_fn=<MseLossBackward0>), 2: tensor(0.7379, grad_fn=<MseLossBackward0>)}
0.9999281333333333
episode:  0 training step:  22 loss of agent  0 :  {0: tensor(3.2179, grad_fn=<MseLossBackward0>), 1: tensor(0.7853, grad_fn=<MseLossBackward0>), 2: tensor(0.7379, grad_fn=<MseLossBackward0>)}
0.9999281333333333
episode:  0 training step:  22 loss of agent  1 :  {0: tensor(3.2179, grad_fn=<MseLossBackward0>), 1: tensor(0.7987, grad_fn=<MseLossBackward0>), 2: tensor(0.7379, grad_fn=<MseLossBackward0>)}
0.9999281333333333
episode:  0 training step:  22 loss of agent  2 :  {0: tensor(3.2179, grad_fn=<MseLossBackward0>), 1: tensor(0.7987, grad_fn=<MseLossBackward0>), 2: tensor(0.7419, grad_fn=<MseLossBackward0>)}
0.9999248666666667
episode:  0 training step:  23 loss of agent  0 :  {0: tensor(3.0541, grad_fn=<MseLossBackward0>), 1: tensor(0.7987, grad_fn=<MseLossBackward0>), 2: tensor(0.7419, grad_fn=<MseLossBackward0>)}
0.9999248666666667
episode:  0 training step:  23 loss of agent  1 :  {0: tensor(3.0541, grad_fn=<MseLossBackward0>), 1: tensor(0.6233, grad_fn=<MseLossBackward0>), 2: tensor(0.7419, grad_fn=<MseLossBackward0>)}
0.9999248666666667
episode:  0 training step:  23 loss of agent  2 :  {0: tensor(3.0541, grad_fn=<MseLossBackward0>), 1: tensor(0.6233, grad_fn=<MseLossBackward0>), 2: tensor(0.6412, grad_fn=<MseLossBackward0>)}
0.9999216
episode:  0 training step:  24 loss of agent  0 :  {0: tensor(2.9871, grad_fn=<MseLossBackward0>), 1: tensor(0.6233, grad_fn=<MseLossBackward0>), 2: tensor(0.6412, grad_fn=<MseLossBackward0>)}
0.9999216
episode:  0 training step:  24 loss of agent  1 :  {0: tensor(2.9871, grad_fn=<MseLossBackward0>), 1: tensor(0.6357, grad_fn=<MseLossBackward0>), 2: tensor(0.6412, grad_fn=<MseLossBackward0>)}
0.9999216
episode:  0 training step:  24 loss of agent  2 :  {0: tensor(2.9871, grad_fn=<MseLossBackward0>), 1: tensor(0.6357, grad_fn=<MseLossBackward0>), 2: tensor(0.6980, grad_fn=<MseLossBackward0>)}
0.9999183333333334
episode:  0 training step:  25 loss of agent  0 :  {0: tensor(3.1024, grad_fn=<MseLossBackward0>), 1: tensor(0.6357, grad_fn=<MseLossBackward0>), 2: tensor(0.6980, grad_fn=<MseLossBackward0>)}
0.9999183333333334
episode:  0 training step:  25 loss of agent  1 :  {0: tensor(3.1024, grad_fn=<MseLossBackward0>), 1: tensor(0.6805, grad_fn=<MseLossBackward0>), 2: tensor(0.6980, grad_fn=<MseLossBackward0>)}
0.9999183333333334
episode:  0 training step:  25 loss of agent  2 :  {0: tensor(3.1024, grad_fn=<MseLossBackward0>), 1: tensor(0.6805, grad_fn=<MseLossBackward0>), 2: tensor(0.6033, grad_fn=<MseLossBackward0>)}
0.9999150666666666
episode:  0 training step:  26 loss of agent  0 :  {0: tensor(3.2128, grad_fn=<MseLossBackward0>), 1: tensor(0.6805, grad_fn=<MseLossBackward0>), 2: tensor(0.6033, grad_fn=<MseLossBackward0>)}
0.9999150666666666
episode:  0 training step:  26 loss of agent  1 :  {0: tensor(3.2128, grad_fn=<MseLossBackward0>), 1: tensor(0.6768, grad_fn=<MseLossBackward0>), 2: tensor(0.6033, grad_fn=<MseLossBackward0>)}
0.9999150666666666
episode:  0 training step:  26 loss of agent  2 :  {0: tensor(3.2128, grad_fn=<MseLossBackward0>), 1: tensor(0.6768, grad_fn=<MseLossBackward0>), 2: tensor(0.5677, grad_fn=<MseLossBackward0>)}
0.9999118
episode:  0 training step:  27 loss of agent  0 :  {0: tensor(2.7654, grad_fn=<MseLossBackward0>), 1: tensor(0.6768, grad_fn=<MseLossBackward0>), 2: tensor(0.5677, grad_fn=<MseLossBackward0>)}
0.9999118
episode:  0 training step:  27 loss of agent  1 :  {0: tensor(2.7654, grad_fn=<MseLossBackward0>), 1: tensor(0.6367, grad_fn=<MseLossBackward0>), 2: tensor(0.5677, grad_fn=<MseLossBackward0>)}
0.9999118
episode:  0 training step:  27 loss of agent  2 :  {0: tensor(2.7654, grad_fn=<MseLossBackward0>), 1: tensor(0.6367, grad_fn=<MseLossBackward0>), 2: tensor(0.5671, grad_fn=<MseLossBackward0>)}
0.9999085333333333
episode:  0 training step:  28 loss of agent  0 :  {0: tensor(2.7691, grad_fn=<MseLossBackward0>), 1: tensor(0.6367, grad_fn=<MseLossBackward0>), 2: tensor(0.5671, grad_fn=<MseLossBackward0>)}
0.9999085333333333
episode:  0 training step:  28 loss of agent  1 :  {0: tensor(2.7691, grad_fn=<MseLossBackward0>), 1: tensor(0.6623, grad_fn=<MseLossBackward0>), 2: tensor(0.5671, grad_fn=<MseLossBackward0>)}
0.9999085333333333
episode:  0 training step:  28 loss of agent  2 :  {0: tensor(2.7691, grad_fn=<MseLossBackward0>), 1: tensor(0.6623, grad_fn=<MseLossBackward0>), 2: tensor(0.4643, grad_fn=<MseLossBackward0>)}
0.9999052666666667
episode:  0 training step:  29 loss of agent  0 :  {0: tensor(2.8819, grad_fn=<MseLossBackward0>), 1: tensor(0.6623, grad_fn=<MseLossBackward0>), 2: tensor(0.4643, grad_fn=<MseLossBackward0>)}
0.9999052666666667
episode:  0 training step:  29 loss of agent  1 :  {0: tensor(2.8819, grad_fn=<MseLossBackward0>), 1: tensor(0.5697, grad_fn=<MseLossBackward0>), 2: tensor(0.4643, grad_fn=<MseLossBackward0>)}
0.9999052666666667
episode:  0 training step:  29 loss of agent  2 :  {0: tensor(2.8819, grad_fn=<MseLossBackward0>), 1: tensor(0.5697, grad_fn=<MseLossBackward0>), 2: tensor(0.4864, grad_fn=<MseLossBackward0>)}
0.999902
episode:  0 training step:  30 loss of agent  0 :  {0: tensor(2.7066, grad_fn=<MseLossBackward0>), 1: tensor(0.5697, grad_fn=<MseLossBackward0>), 2: tensor(0.4864, grad_fn=<MseLossBackward0>)}
0.999902
episode:  0 training step:  30 loss of agent  1 :  {0: tensor(2.7066, grad_fn=<MseLossBackward0>), 1: tensor(0.5869, grad_fn=<MseLossBackward0>), 2: tensor(0.4864, grad_fn=<MseLossBackward0>)}
0.999902
episode:  0 training step:  30 loss of agent  2 :  {0: tensor(2.7066, grad_fn=<MseLossBackward0>), 1: tensor(0.5869, grad_fn=<MseLossBackward0>), 2: tensor(0.5146, grad_fn=<MseLossBackward0>)}
0.9998987333333333
episode:  0 training step:  31 loss of agent  0 :  {0: tensor(3.3035, grad_fn=<MseLossBackward0>), 1: tensor(0.5869, grad_fn=<MseLossBackward0>), 2: tensor(0.5146, grad_fn=<MseLossBackward0>)}
0.9998987333333333
episode:  0 training step:  31 loss of agent  1 :  {0: tensor(3.3035, grad_fn=<MseLossBackward0>), 1: tensor(0.8609, grad_fn=<MseLossBackward0>), 2: tensor(0.5146, grad_fn=<MseLossBackward0>)}
0.9998987333333333
episode:  0 training step:  31 loss of agent  2 :  {0: tensor(3.3035, grad_fn=<MseLossBackward0>), 1: tensor(0.8609, grad_fn=<MseLossBackward0>), 2: tensor(0.7543, grad_fn=<MseLossBackward0>)}
0.9998954666666666
episode:  0 training step:  32 loss of agent  0 :  {0: tensor(3.1262, grad_fn=<MseLossBackward0>), 1: tensor(0.8609, grad_fn=<MseLossBackward0>), 2: tensor(0.7543, grad_fn=<MseLossBackward0>)}
0.9998954666666666
episode:  0 training step:  32 loss of agent  1 :  {0: tensor(3.1262, grad_fn=<MseLossBackward0>), 1: tensor(0.8027, grad_fn=<MseLossBackward0>), 2: tensor(0.7543, grad_fn=<MseLossBackward0>)}
0.9998954666666666
episode:  0 training step:  32 loss of agent  2 :  {0: tensor(3.1262, grad_fn=<MseLossBackward0>), 1: tensor(0.8027, grad_fn=<MseLossBackward0>), 2: tensor(0.6998, grad_fn=<MseLossBackward0>)}
0.9998922
episode:  0 training step:  33 loss of agent  0 :  {0: tensor(3.1989, grad_fn=<MseLossBackward0>), 1: tensor(0.8027, grad_fn=<MseLossBackward0>), 2: tensor(0.6998, grad_fn=<MseLossBackward0>)}
0.9998922
episode:  0 training step:  33 loss of agent  1 :  {0: tensor(3.1989, grad_fn=<MseLossBackward0>), 1: tensor(0.6769, grad_fn=<MseLossBackward0>), 2: tensor(0.6998, grad_fn=<MseLossBackward0>)}
0.9998922
episode:  0 training step:  33 loss of agent  2 :  {0: tensor(3.1989, grad_fn=<MseLossBackward0>), 1: tensor(0.6769, grad_fn=<MseLossBackward0>), 2: tensor(0.6410, grad_fn=<MseLossBackward0>)}
0.9998889333333333
episode:  0 training step:  34 loss of agent  0 :  {0: tensor(2.9359, grad_fn=<MseLossBackward0>), 1: tensor(0.6769, grad_fn=<MseLossBackward0>), 2: tensor(0.6410, grad_fn=<MseLossBackward0>)}
0.9998889333333333
episode:  0 training step:  34 loss of agent  1 :  {0: tensor(2.9359, grad_fn=<MseLossBackward0>), 1: tensor(0.6806, grad_fn=<MseLossBackward0>), 2: tensor(0.6410, grad_fn=<MseLossBackward0>)}
0.9998889333333333
episode:  0 training step:  34 loss of agent  2 :  {0: tensor(2.9359, grad_fn=<MseLossBackward0>), 1: tensor(0.6806, grad_fn=<MseLossBackward0>), 2: tensor(0.6883, grad_fn=<MseLossBackward0>)}
0.9998856666666667
episode:  0 training step:  35 loss of agent  0 :  {0: tensor(2.9975, grad_fn=<MseLossBackward0>), 1: tensor(0.6806, grad_fn=<MseLossBackward0>), 2: tensor(0.6883, grad_fn=<MseLossBackward0>)}
0.9998856666666667
episode:  0 training step:  35 loss of agent  1 :  {0: tensor(2.9975, grad_fn=<MseLossBackward0>), 1: tensor(0.6677, grad_fn=<MseLossBackward0>), 2: tensor(0.6883, grad_fn=<MseLossBackward0>)}
0.9998856666666667
episode:  0 training step:  35 loss of agent  2 :  {0: tensor(2.9975, grad_fn=<MseLossBackward0>), 1: tensor(0.6677, grad_fn=<MseLossBackward0>), 2: tensor(0.7322, grad_fn=<MseLossBackward0>)}
0.9998824
episode:  0 training step:  36 loss of agent  0 :  {0: tensor(2.9651, grad_fn=<MseLossBackward0>), 1: tensor(0.6677, grad_fn=<MseLossBackward0>), 2: tensor(0.7322, grad_fn=<MseLossBackward0>)}
0.9998824
episode:  0 training step:  36 loss of agent  1 :  {0: tensor(2.9651, grad_fn=<MseLossBackward0>), 1: tensor(0.7122, grad_fn=<MseLossBackward0>), 2: tensor(0.7322, grad_fn=<MseLossBackward0>)}
0.9998824
episode:  0 training step:  36 loss of agent  2 :  {0: tensor(2.9651, grad_fn=<MseLossBackward0>), 1: tensor(0.7122, grad_fn=<MseLossBackward0>), 2: tensor(0.6170, grad_fn=<MseLossBackward0>)}
0.9998791333333333
episode:  0 training step:  37 loss of agent  0 :  {0: tensor(2.8302, grad_fn=<MseLossBackward0>), 1: tensor(0.7122, grad_fn=<MseLossBackward0>), 2: tensor(0.6170, grad_fn=<MseLossBackward0>)}
0.9998791333333333
episode:  0 training step:  37 loss of agent  1 :  {0: tensor(2.8302, grad_fn=<MseLossBackward0>), 1: tensor(0.5118, grad_fn=<MseLossBackward0>), 2: tensor(0.6170, grad_fn=<MseLossBackward0>)}
0.9998791333333333
episode:  0 training step:  37 loss of agent  2 :  {0: tensor(2.8302, grad_fn=<MseLossBackward0>), 1: tensor(0.5118, grad_fn=<MseLossBackward0>), 2: tensor(0.5934, grad_fn=<MseLossBackward0>)}
0.9998758666666666
episode:  0 training step:  38 loss of agent  0 :  {0: tensor(2.8057, grad_fn=<MseLossBackward0>), 1: tensor(0.5118, grad_fn=<MseLossBackward0>), 2: tensor(0.5934, grad_fn=<MseLossBackward0>)}
0.9998758666666666
episode:  0 training step:  38 loss of agent  1 :  {0: tensor(2.8057, grad_fn=<MseLossBackward0>), 1: tensor(0.4870, grad_fn=<MseLossBackward0>), 2: tensor(0.5934, grad_fn=<MseLossBackward0>)}
0.9998758666666666
episode:  0 training step:  38 loss of agent  2 :  {0: tensor(2.8057, grad_fn=<MseLossBackward0>), 1: tensor(0.4870, grad_fn=<MseLossBackward0>), 2: tensor(0.5722, grad_fn=<MseLossBackward0>)}
0.9998726
episode:  0 training step:  39 loss of agent  0 :  {0: tensor(2.8737, grad_fn=<MseLossBackward0>), 1: tensor(0.4870, grad_fn=<MseLossBackward0>), 2: tensor(0.5722, grad_fn=<MseLossBackward0>)}
0.9998726
episode:  0 training step:  39 loss of agent  1 :  {0: tensor(2.8737, grad_fn=<MseLossBackward0>), 1: tensor(0.5309, grad_fn=<MseLossBackward0>), 2: tensor(0.5722, grad_fn=<MseLossBackward0>)}
0.9998726
episode:  0 training step:  39 loss of agent  2 :  {0: tensor(2.8737, grad_fn=<MseLossBackward0>), 1: tensor(0.5309, grad_fn=<MseLossBackward0>), 2: tensor(0.5869, grad_fn=<MseLossBackward0>)}
0.9998693333333334
episode:  0 training step:  40 loss of agent  0 :  {0: tensor(2.5997, grad_fn=<MseLossBackward0>), 1: tensor(0.5309, grad_fn=<MseLossBackward0>), 2: tensor(0.5869, grad_fn=<MseLossBackward0>)}
0.9998693333333334
episode:  0 training step:  40 loss of agent  1 :  {0: tensor(2.5997, grad_fn=<MseLossBackward0>), 1: tensor(0.4621, grad_fn=<MseLossBackward0>), 2: tensor(0.5869, grad_fn=<MseLossBackward0>)}
0.9998693333333334
episode:  0 training step:  40 loss of agent  2 :  {0: tensor(2.5997, grad_fn=<MseLossBackward0>), 1: tensor(0.4621, grad_fn=<MseLossBackward0>), 2: tensor(0.4297, grad_fn=<MseLossBackward0>)}
0.9998660666666667
episode:  0 training step:  41 loss of agent  0 :  {0: tensor(3.0259, grad_fn=<MseLossBackward0>), 1: tensor(0.4621, grad_fn=<MseLossBackward0>), 2: tensor(0.4297, grad_fn=<MseLossBackward0>)}
0.9998660666666667
episode:  0 training step:  41 loss of agent  1 :  {0: tensor(3.0259, grad_fn=<MseLossBackward0>), 1: tensor(0.9322, grad_fn=<MseLossBackward0>), 2: tensor(0.4297, grad_fn=<MseLossBackward0>)}
0.9998660666666667
episode:  0 training step:  41 loss of agent  2 :  {0: tensor(3.0259, grad_fn=<MseLossBackward0>), 1: tensor(0.9322, grad_fn=<MseLossBackward0>), 2: tensor(0.9454, grad_fn=<MseLossBackward0>)}
0.9998628
episode:  0 training step:  42 loss of agent  0 :  {0: tensor(2.9882, grad_fn=<MseLossBackward0>), 1: tensor(0.9322, grad_fn=<MseLossBackward0>), 2: tensor(0.9454, grad_fn=<MseLossBackward0>)}
0.9998628
episode:  0 training step:  42 loss of agent  1 :  {0: tensor(2.9882, grad_fn=<MseLossBackward0>), 1: tensor(0.6569, grad_fn=<MseLossBackward0>), 2: tensor(0.9454, grad_fn=<MseLossBackward0>)}
0.9998628
episode:  0 training step:  42 loss of agent  2 :  {0: tensor(2.9882, grad_fn=<MseLossBackward0>), 1: tensor(0.6569, grad_fn=<MseLossBackward0>), 2: tensor(0.7921, grad_fn=<MseLossBackward0>)}
0.9998595333333333
episode:  0 training step:  43 loss of agent  0 :  {0: tensor(3.0627, grad_fn=<MseLossBackward0>), 1: tensor(0.6569, grad_fn=<MseLossBackward0>), 2: tensor(0.7921, grad_fn=<MseLossBackward0>)}
0.9998595333333333
episode:  0 training step:  43 loss of agent  1 :  {0: tensor(3.0627, grad_fn=<MseLossBackward0>), 1: tensor(0.7503, grad_fn=<MseLossBackward0>), 2: tensor(0.7921, grad_fn=<MseLossBackward0>)}
0.9998595333333333
episode:  0 training step:  43 loss of agent  2 :  {0: tensor(3.0627, grad_fn=<MseLossBackward0>), 1: tensor(0.7503, grad_fn=<MseLossBackward0>), 2: tensor(0.8494, grad_fn=<MseLossBackward0>)}
0.9998562666666667
episode:  0 training step:  44 loss of agent  0 :  {0: tensor(2.8270, grad_fn=<MseLossBackward0>), 1: tensor(0.7503, grad_fn=<MseLossBackward0>), 2: tensor(0.8494, grad_fn=<MseLossBackward0>)}
0.9998562666666667
episode:  0 training step:  44 loss of agent  1 :  {0: tensor(2.8270, grad_fn=<MseLossBackward0>), 1: tensor(0.5915, grad_fn=<MseLossBackward0>), 2: tensor(0.8494, grad_fn=<MseLossBackward0>)}
0.9998562666666667
episode:  0 training step:  44 loss of agent  2 :  {0: tensor(2.8270, grad_fn=<MseLossBackward0>), 1: tensor(0.5915, grad_fn=<MseLossBackward0>), 2: tensor(0.7907, grad_fn=<MseLossBackward0>)}
0.999853
episode:  0 training step:  45 loss of agent  0 :  {0: tensor(2.8991, grad_fn=<MseLossBackward0>), 1: tensor(0.5915, grad_fn=<MseLossBackward0>), 2: tensor(0.7907, grad_fn=<MseLossBackward0>)}
0.999853
episode:  0 training step:  45 loss of agent  1 :  {0: tensor(2.8991, grad_fn=<MseLossBackward0>), 1: tensor(0.6064, grad_fn=<MseLossBackward0>), 2: tensor(0.7907, grad_fn=<MseLossBackward0>)}
0.999853
episode:  0 training step:  45 loss of agent  2 :  {0: tensor(2.8991, grad_fn=<MseLossBackward0>), 1: tensor(0.6064, grad_fn=<MseLossBackward0>), 2: tensor(0.7125, grad_fn=<MseLossBackward0>)}
0.9998497333333334
episode:  0 training step:  46 loss of agent  0 :  {0: tensor(2.8052, grad_fn=<MseLossBackward0>), 1: tensor(0.6064, grad_fn=<MseLossBackward0>), 2: tensor(0.7125, grad_fn=<MseLossBackward0>)}
0.9998497333333334
episode:  0 training step:  46 loss of agent  1 :  {0: tensor(2.8052, grad_fn=<MseLossBackward0>), 1: tensor(0.5232, grad_fn=<MseLossBackward0>), 2: tensor(0.7125, grad_fn=<MseLossBackward0>)}
0.9998497333333334
episode:  0 training step:  46 loss of agent  2 :  {0: tensor(2.8052, grad_fn=<MseLossBackward0>), 1: tensor(0.5232, grad_fn=<MseLossBackward0>), 2: tensor(0.7221, grad_fn=<MseLossBackward0>)}
0.9998464666666667
episode:  0 training step:  47 loss of agent  0 :  {0: tensor(2.8552, grad_fn=<MseLossBackward0>), 1: tensor(0.5232, grad_fn=<MseLossBackward0>), 2: tensor(0.7221, grad_fn=<MseLossBackward0>)}
0.9998464666666667
episode:  0 training step:  47 loss of agent  1 :  {0: tensor(2.8552, grad_fn=<MseLossBackward0>), 1: tensor(0.6540, grad_fn=<MseLossBackward0>), 2: tensor(0.7221, grad_fn=<MseLossBackward0>)}
0.9998464666666667
episode:  0 training step:  47 loss of agent  2 :  {0: tensor(2.8552, grad_fn=<MseLossBackward0>), 1: tensor(0.6540, grad_fn=<MseLossBackward0>), 2: tensor(0.5911, grad_fn=<MseLossBackward0>)}
0.9998432
episode:  0 training step:  48 loss of agent  0 :  {0: tensor(2.5327, grad_fn=<MseLossBackward0>), 1: tensor(0.6540, grad_fn=<MseLossBackward0>), 2: tensor(0.5911, grad_fn=<MseLossBackward0>)}
0.9998432
episode:  0 training step:  48 loss of agent  1 :  {0: tensor(2.5327, grad_fn=<MseLossBackward0>), 1: tensor(0.5655, grad_fn=<MseLossBackward0>), 2: tensor(0.5911, grad_fn=<MseLossBackward0>)}
0.9998432
episode:  0 training step:  48 loss of agent  2 :  {0: tensor(2.5327, grad_fn=<MseLossBackward0>), 1: tensor(0.5655, grad_fn=<MseLossBackward0>), 2: tensor(0.5945, grad_fn=<MseLossBackward0>)}
0.9998399333333333
episode:  0 training step:  49 loss of agent  0 :  {0: tensor(2.5080, grad_fn=<MseLossBackward0>), 1: tensor(0.5655, grad_fn=<MseLossBackward0>), 2: tensor(0.5945, grad_fn=<MseLossBackward0>)}
0.9998399333333333
episode:  0 training step:  49 loss of agent  1 :  {0: tensor(2.5080, grad_fn=<MseLossBackward0>), 1: tensor(0.4888, grad_fn=<MseLossBackward0>), 2: tensor(0.5945, grad_fn=<MseLossBackward0>)}
0.9998399333333333
episode:  0 training step:  49 loss of agent  2 :  {0: tensor(2.5080, grad_fn=<MseLossBackward0>), 1: tensor(0.4888, grad_fn=<MseLossBackward0>), 2: tensor(0.5195, grad_fn=<MseLossBackward0>)}
0.9998366666666667
episode:  0 training step:  50 loss of agent  0 :  {0: tensor(2.2651, grad_fn=<MseLossBackward0>), 1: tensor(0.4888, grad_fn=<MseLossBackward0>), 2: tensor(0.5195, grad_fn=<MseLossBackward0>)}
0.9998366666666667
episode:  0 training step:  50 loss of agent  1 :  {0: tensor(2.2651, grad_fn=<MseLossBackward0>), 1: tensor(0.4362, grad_fn=<MseLossBackward0>), 2: tensor(0.5195, grad_fn=<MseLossBackward0>)}
0.9998366666666667
episode:  0 training step:  50 loss of agent  2 :  {0: tensor(2.2651, grad_fn=<MseLossBackward0>), 1: tensor(0.4362, grad_fn=<MseLossBackward0>), 2: tensor(0.5789, grad_fn=<MseLossBackward0>)}
0.9998334
episode:  0 training step:  51 loss of agent  0 :  {0: tensor(3.1551, grad_fn=<MseLossBackward0>), 1: tensor(0.4362, grad_fn=<MseLossBackward0>), 2: tensor(0.5789, grad_fn=<MseLossBackward0>)}
0.9998334
episode:  0 training step:  51 loss of agent  1 :  {0: tensor(3.1551, grad_fn=<MseLossBackward0>), 1: tensor(0.9275, grad_fn=<MseLossBackward0>), 2: tensor(0.5789, grad_fn=<MseLossBackward0>)}
0.9998334
episode:  0 training step:  51 loss of agent  2 :  {0: tensor(3.1551, grad_fn=<MseLossBackward0>), 1: tensor(0.9275, grad_fn=<MseLossBackward0>), 2: tensor(1.1110, grad_fn=<MseLossBackward0>)}
0.9998301333333334
episode:  0 training step:  52 loss of agent  0 :  {0: tensor(3.0348, grad_fn=<MseLossBackward0>), 1: tensor(0.9275, grad_fn=<MseLossBackward0>), 2: tensor(1.1110, grad_fn=<MseLossBackward0>)}
0.9998301333333334
episode:  0 training step:  52 loss of agent  1 :  {0: tensor(3.0348, grad_fn=<MseLossBackward0>), 1: tensor(0.9103, grad_fn=<MseLossBackward0>), 2: tensor(1.1110, grad_fn=<MseLossBackward0>)}
0.9998301333333334
episode:  0 training step:  52 loss of agent  2 :  {0: tensor(3.0348, grad_fn=<MseLossBackward0>), 1: tensor(0.9103, grad_fn=<MseLossBackward0>), 2: tensor(0.9288, grad_fn=<MseLossBackward0>)}
0.9998268666666666
episode:  0 training step:  53 loss of agent  0 :  {0: tensor(2.9259, grad_fn=<MseLossBackward0>), 1: tensor(0.9103, grad_fn=<MseLossBackward0>), 2: tensor(0.9288, grad_fn=<MseLossBackward0>)}
0.9998268666666666
episode:  0 training step:  53 loss of agent  1 :  {0: tensor(2.9259, grad_fn=<MseLossBackward0>), 1: tensor(0.8389, grad_fn=<MseLossBackward0>), 2: tensor(0.9288, grad_fn=<MseLossBackward0>)}
0.9998268666666666
episode:  0 training step:  53 loss of agent  2 :  {0: tensor(2.9259, grad_fn=<MseLossBackward0>), 1: tensor(0.8389, grad_fn=<MseLossBackward0>), 2: tensor(1.0390, grad_fn=<MseLossBackward0>)}
0.9998236
episode:  0 training step:  54 loss of agent  0 :  {0: tensor(2.7388, grad_fn=<MseLossBackward0>), 1: tensor(0.8389, grad_fn=<MseLossBackward0>), 2: tensor(1.0390, grad_fn=<MseLossBackward0>)}
0.9998236
episode:  0 training step:  54 loss of agent  1 :  {0: tensor(2.7388, grad_fn=<MseLossBackward0>), 1: tensor(0.8466, grad_fn=<MseLossBackward0>), 2: tensor(1.0390, grad_fn=<MseLossBackward0>)}
0.9998236
episode:  0 training step:  54 loss of agent  2 :  {0: tensor(2.7388, grad_fn=<MseLossBackward0>), 1: tensor(0.8466, grad_fn=<MseLossBackward0>), 2: tensor(0.7575, grad_fn=<MseLossBackward0>)}
0.9998203333333333
episode:  0 training step:  55 loss of agent  0 :  {0: tensor(2.6348, grad_fn=<MseLossBackward0>), 1: tensor(0.8466, grad_fn=<MseLossBackward0>), 2: tensor(0.7575, grad_fn=<MseLossBackward0>)}
0.9998203333333333
episode:  0 training step:  55 loss of agent  1 :  {0: tensor(2.6348, grad_fn=<MseLossBackward0>), 1: tensor(0.7167, grad_fn=<MseLossBackward0>), 2: tensor(0.7575, grad_fn=<MseLossBackward0>)}
0.9998203333333333
episode:  0 training step:  55 loss of agent  2 :  {0: tensor(2.6348, grad_fn=<MseLossBackward0>), 1: tensor(0.7167, grad_fn=<MseLossBackward0>), 2: tensor(0.6600, grad_fn=<MseLossBackward0>)}
0.9998170666666667
episode:  0 training step:  56 loss of agent  0 :  {0: tensor(2.5834, grad_fn=<MseLossBackward0>), 1: tensor(0.7167, grad_fn=<MseLossBackward0>), 2: tensor(0.6600, grad_fn=<MseLossBackward0>)}
0.9998170666666667
episode:  0 training step:  56 loss of agent  1 :  {0: tensor(2.5834, grad_fn=<MseLossBackward0>), 1: tensor(0.5993, grad_fn=<MseLossBackward0>), 2: tensor(0.6600, grad_fn=<MseLossBackward0>)}
0.9998170666666667
episode:  0 training step:  56 loss of agent  2 :  {0: tensor(2.5834, grad_fn=<MseLossBackward0>), 1: tensor(0.5993, grad_fn=<MseLossBackward0>), 2: tensor(0.6448, grad_fn=<MseLossBackward0>)}
0.9998138
episode:  0 training step:  57 loss of agent  0 :  {0: tensor(2.4367, grad_fn=<MseLossBackward0>), 1: tensor(0.5993, grad_fn=<MseLossBackward0>), 2: tensor(0.6448, grad_fn=<MseLossBackward0>)}
0.9998138
episode:  0 training step:  57 loss of agent  1 :  {0: tensor(2.4367, grad_fn=<MseLossBackward0>), 1: tensor(0.6465, grad_fn=<MseLossBackward0>), 2: tensor(0.6448, grad_fn=<MseLossBackward0>)}
0.9998138
episode:  0 training step:  57 loss of agent  2 :  {0: tensor(2.4367, grad_fn=<MseLossBackward0>), 1: tensor(0.6465, grad_fn=<MseLossBackward0>), 2: tensor(0.5873, grad_fn=<MseLossBackward0>)}
0.9998105333333334
episode:  0 training step:  58 loss of agent  0 :  {0: tensor(2.3548, grad_fn=<MseLossBackward0>), 1: tensor(0.6465, grad_fn=<MseLossBackward0>), 2: tensor(0.5873, grad_fn=<MseLossBackward0>)}
0.9998105333333334
episode:  0 training step:  58 loss of agent  1 :  {0: tensor(2.3548, grad_fn=<MseLossBackward0>), 1: tensor(0.5775, grad_fn=<MseLossBackward0>), 2: tensor(0.5873, grad_fn=<MseLossBackward0>)}
0.9998105333333334
episode:  0 training step:  58 loss of agent  2 :  {0: tensor(2.3548, grad_fn=<MseLossBackward0>), 1: tensor(0.5775, grad_fn=<MseLossBackward0>), 2: tensor(0.5083, grad_fn=<MseLossBackward0>)}
0.9998072666666666
episode:  0 training step:  59 loss of agent  0 :  {0: tensor(2.1503, grad_fn=<MseLossBackward0>), 1: tensor(0.5775, grad_fn=<MseLossBackward0>), 2: tensor(0.5083, grad_fn=<MseLossBackward0>)}
0.9998072666666666
episode:  0 training step:  59 loss of agent  1 :  {0: tensor(2.1503, grad_fn=<MseLossBackward0>), 1: tensor(0.4881, grad_fn=<MseLossBackward0>), 2: tensor(0.5083, grad_fn=<MseLossBackward0>)}
0.9998072666666666
episode:  0 training step:  59 loss of agent  2 :  {0: tensor(2.1503, grad_fn=<MseLossBackward0>), 1: tensor(0.4881, grad_fn=<MseLossBackward0>), 2: tensor(0.3808, grad_fn=<MseLossBackward0>)}
0.999804
episode:  0 training step:  60 loss of agent  0 :  {0: tensor(2.0193, grad_fn=<MseLossBackward0>), 1: tensor(0.4881, grad_fn=<MseLossBackward0>), 2: tensor(0.3808, grad_fn=<MseLossBackward0>)}
0.999804
episode:  0 training step:  60 loss of agent  1 :  {0: tensor(2.0193, grad_fn=<MseLossBackward0>), 1: tensor(0.3870, grad_fn=<MseLossBackward0>), 2: tensor(0.3808, grad_fn=<MseLossBackward0>)}
0.999804
episode:  0 training step:  60 loss of agent  2 :  {0: tensor(2.0193, grad_fn=<MseLossBackward0>), 1: tensor(0.3870, grad_fn=<MseLossBackward0>), 2: tensor(0.3919, grad_fn=<MseLossBackward0>)}
0.9998007333333333
episode:  0 training step:  61 loss of agent  0 :  {0: tensor(2.9230, grad_fn=<MseLossBackward0>), 1: tensor(0.3870, grad_fn=<MseLossBackward0>), 2: tensor(0.3919, grad_fn=<MseLossBackward0>)}
0.9998007333333333
episode:  0 training step:  61 loss of agent  1 :  {0: tensor(2.9230, grad_fn=<MseLossBackward0>), 1: tensor(0.8949, grad_fn=<MseLossBackward0>), 2: tensor(0.3919, grad_fn=<MseLossBackward0>)}
0.9998007333333333
episode:  0 training step:  61 loss of agent  2 :  {0: tensor(2.9230, grad_fn=<MseLossBackward0>), 1: tensor(0.8949, grad_fn=<MseLossBackward0>), 2: tensor(1.3844, grad_fn=<MseLossBackward0>)}
0.9997974666666667
episode:  0 training step:  62 loss of agent  0 :  {0: tensor(2.8259, grad_fn=<MseLossBackward0>), 1: tensor(0.8949, grad_fn=<MseLossBackward0>), 2: tensor(1.3844, grad_fn=<MseLossBackward0>)}
0.9997974666666667
episode:  0 training step:  62 loss of agent  1 :  {0: tensor(2.8259, grad_fn=<MseLossBackward0>), 1: tensor(0.8778, grad_fn=<MseLossBackward0>), 2: tensor(1.3844, grad_fn=<MseLossBackward0>)}
0.9997974666666667
episode:  0 training step:  62 loss of agent  2 :  {0: tensor(2.8259, grad_fn=<MseLossBackward0>), 1: tensor(0.8778, grad_fn=<MseLossBackward0>), 2: tensor(1.1604, grad_fn=<MseLossBackward0>)}
0.9997942
episode:  0 training step:  63 loss of agent  0 :  {0: tensor(2.3850, grad_fn=<MseLossBackward0>), 1: tensor(0.8778, grad_fn=<MseLossBackward0>), 2: tensor(1.1604, grad_fn=<MseLossBackward0>)}
0.9997942
episode:  0 training step:  63 loss of agent  1 :  {0: tensor(2.3850, grad_fn=<MseLossBackward0>), 1: tensor(0.6404, grad_fn=<MseLossBackward0>), 2: tensor(1.1604, grad_fn=<MseLossBackward0>)}
0.9997942
episode:  0 training step:  63 loss of agent  2 :  {0: tensor(2.3850, grad_fn=<MseLossBackward0>), 1: tensor(0.6404, grad_fn=<MseLossBackward0>), 2: tensor(1.0447, grad_fn=<MseLossBackward0>)}
0.9997909333333334
episode:  0 training step:  64 loss of agent  0 :  {0: tensor(2.4511, grad_fn=<MseLossBackward0>), 1: tensor(0.6404, grad_fn=<MseLossBackward0>), 2: tensor(1.0447, grad_fn=<MseLossBackward0>)}
0.9997909333333334
episode:  0 training step:  64 loss of agent  1 :  {0: tensor(2.4511, grad_fn=<MseLossBackward0>), 1: tensor(0.7133, grad_fn=<MseLossBackward0>), 2: tensor(1.0447, grad_fn=<MseLossBackward0>)}
0.9997909333333334
episode:  0 training step:  64 loss of agent  2 :  {0: tensor(2.4511, grad_fn=<MseLossBackward0>), 1: tensor(0.7133, grad_fn=<MseLossBackward0>), 2: tensor(0.8621, grad_fn=<MseLossBackward0>)}
0.9997876666666666
episode:  0 training step:  65 loss of agent  0 :  {0: tensor(2.1090, grad_fn=<MseLossBackward0>), 1: tensor(0.7133, grad_fn=<MseLossBackward0>), 2: tensor(0.8621, grad_fn=<MseLossBackward0>)}
0.9997876666666666
episode:  0 training step:  65 loss of agent  1 :  {0: tensor(2.1090, grad_fn=<MseLossBackward0>), 1: tensor(0.5707, grad_fn=<MseLossBackward0>), 2: tensor(0.8621, grad_fn=<MseLossBackward0>)}
0.9997876666666666
episode:  0 training step:  65 loss of agent  2 :  {0: tensor(2.1090, grad_fn=<MseLossBackward0>), 1: tensor(0.5707, grad_fn=<MseLossBackward0>), 2: tensor(0.7573, grad_fn=<MseLossBackward0>)}
0.9997844
episode:  0 training step:  66 loss of agent  0 :  {0: tensor(2.1933, grad_fn=<MseLossBackward0>), 1: tensor(0.5707, grad_fn=<MseLossBackward0>), 2: tensor(0.7573, grad_fn=<MseLossBackward0>)}
0.9997844
episode:  0 training step:  66 loss of agent  1 :  {0: tensor(2.1933, grad_fn=<MseLossBackward0>), 1: tensor(0.5108, grad_fn=<MseLossBackward0>), 2: tensor(0.7573, grad_fn=<MseLossBackward0>)}
0.9997844
episode:  0 training step:  66 loss of agent  2 :  {0: tensor(2.1933, grad_fn=<MseLossBackward0>), 1: tensor(0.5108, grad_fn=<MseLossBackward0>), 2: tensor(0.8669, grad_fn=<MseLossBackward0>)}
0.9997811333333333
episode:  0 training step:  67 loss of agent  0 :  {0: tensor(1.9945, grad_fn=<MseLossBackward0>), 1: tensor(0.5108, grad_fn=<MseLossBackward0>), 2: tensor(0.8669, grad_fn=<MseLossBackward0>)}
0.9997811333333333
episode:  0 training step:  67 loss of agent  1 :  {0: tensor(1.9945, grad_fn=<MseLossBackward0>), 1: tensor(0.4663, grad_fn=<MseLossBackward0>), 2: tensor(0.8669, grad_fn=<MseLossBackward0>)}
0.9997811333333333
episode:  0 training step:  67 loss of agent  2 :  {0: tensor(1.9945, grad_fn=<MseLossBackward0>), 1: tensor(0.4663, grad_fn=<MseLossBackward0>), 2: tensor(0.6112, grad_fn=<MseLossBackward0>)}
0.9997778666666667
episode:  0 training step:  68 loss of agent  0 :  {0: tensor(1.7134, grad_fn=<MseLossBackward0>), 1: tensor(0.4663, grad_fn=<MseLossBackward0>), 2: tensor(0.6112, grad_fn=<MseLossBackward0>)}
0.9997778666666667
episode:  0 training step:  68 loss of agent  1 :  {0: tensor(1.7134, grad_fn=<MseLossBackward0>), 1: tensor(0.4679, grad_fn=<MseLossBackward0>), 2: tensor(0.6112, grad_fn=<MseLossBackward0>)}
0.9997778666666667
episode:  0 training step:  68 loss of agent  2 :  {0: tensor(1.7134, grad_fn=<MseLossBackward0>), 1: tensor(0.4679, grad_fn=<MseLossBackward0>), 2: tensor(0.6407, grad_fn=<MseLossBackward0>)}
0.9997746
episode:  0 training step:  69 loss of agent  0 :  {0: tensor(1.8070, grad_fn=<MseLossBackward0>), 1: tensor(0.4679, grad_fn=<MseLossBackward0>), 2: tensor(0.6407, grad_fn=<MseLossBackward0>)}
0.9997746
episode:  0 training step:  69 loss of agent  1 :  {0: tensor(1.8070, grad_fn=<MseLossBackward0>), 1: tensor(0.4169, grad_fn=<MseLossBackward0>), 2: tensor(0.6407, grad_fn=<MseLossBackward0>)}
0.9997746
episode:  0 training step:  69 loss of agent  2 :  {0: tensor(1.8070, grad_fn=<MseLossBackward0>), 1: tensor(0.4169, grad_fn=<MseLossBackward0>), 2: tensor(0.5040, grad_fn=<MseLossBackward0>)}
0.9997713333333333
episode:  0 training step:  70 loss of agent  0 :  {0: tensor(1.4589, grad_fn=<MseLossBackward0>), 1: tensor(0.4169, grad_fn=<MseLossBackward0>), 2: tensor(0.5040, grad_fn=<MseLossBackward0>)}
0.9997713333333333
episode:  0 training step:  70 loss of agent  1 :  {0: tensor(1.4589, grad_fn=<MseLossBackward0>), 1: tensor(0.2826, grad_fn=<MseLossBackward0>), 2: tensor(0.5040, grad_fn=<MseLossBackward0>)}
0.9997713333333333
episode:  0 training step:  70 loss of agent  2 :  {0: tensor(1.4589, grad_fn=<MseLossBackward0>), 1: tensor(0.2826, grad_fn=<MseLossBackward0>), 2: tensor(0.4545, grad_fn=<MseLossBackward0>)}
0.9997680666666666
episode:  0 training step:  71 loss of agent  0 :  {0: tensor(2.3712, grad_fn=<MseLossBackward0>), 1: tensor(0.2826, grad_fn=<MseLossBackward0>), 2: tensor(0.4545, grad_fn=<MseLossBackward0>)}
0.9997680666666666
episode:  0 training step:  71 loss of agent  1 :  {0: tensor(2.3712, grad_fn=<MseLossBackward0>), 1: tensor(0.9441, grad_fn=<MseLossBackward0>), 2: tensor(0.4545, grad_fn=<MseLossBackward0>)}
0.9997680666666666
episode:  0 training step:  71 loss of agent  2 :  {0: tensor(2.3712, grad_fn=<MseLossBackward0>), 1: tensor(0.9441, grad_fn=<MseLossBackward0>), 2: tensor(1.4945, grad_fn=<MseLossBackward0>)}
0.9997648
episode:  0 training step:  72 loss of agent  0 :  {0: tensor(2.3745, grad_fn=<MseLossBackward0>), 1: tensor(0.9441, grad_fn=<MseLossBackward0>), 2: tensor(1.4945, grad_fn=<MseLossBackward0>)}
0.9997648
episode:  0 training step:  72 loss of agent  1 :  {0: tensor(2.3745, grad_fn=<MseLossBackward0>), 1: tensor(0.8682, grad_fn=<MseLossBackward0>), 2: tensor(1.4945, grad_fn=<MseLossBackward0>)}
0.9997648
episode:  0 training step:  72 loss of agent  2 :  {0: tensor(2.3745, grad_fn=<MseLossBackward0>), 1: tensor(0.8682, grad_fn=<MseLossBackward0>), 2: tensor(1.3746, grad_fn=<MseLossBackward0>)}
0.9997615333333333
episode:  0 training step:  73 loss of agent  0 :  {0: tensor(1.9122, grad_fn=<MseLossBackward0>), 1: tensor(0.8682, grad_fn=<MseLossBackward0>), 2: tensor(1.3746, grad_fn=<MseLossBackward0>)}
0.9997615333333333
episode:  0 training step:  73 loss of agent  1 :  {0: tensor(1.9122, grad_fn=<MseLossBackward0>), 1: tensor(0.7447, grad_fn=<MseLossBackward0>), 2: tensor(1.3746, grad_fn=<MseLossBackward0>)}
0.9997615333333333
episode:  0 training step:  73 loss of agent  2 :  {0: tensor(1.9122, grad_fn=<MseLossBackward0>), 1: tensor(0.7447, grad_fn=<MseLossBackward0>), 2: tensor(1.0831, grad_fn=<MseLossBackward0>)}
0.9997582666666667
episode:  0 training step:  74 loss of agent  0 :  {0: tensor(1.9537, grad_fn=<MseLossBackward0>), 1: tensor(0.7447, grad_fn=<MseLossBackward0>), 2: tensor(1.0831, grad_fn=<MseLossBackward0>)}
0.9997582666666667
episode:  0 training step:  74 loss of agent  1 :  {0: tensor(1.9537, grad_fn=<MseLossBackward0>), 1: tensor(0.6323, grad_fn=<MseLossBackward0>), 2: tensor(1.0831, grad_fn=<MseLossBackward0>)}
0.9997582666666667
episode:  0 training step:  74 loss of agent  2 :  {0: tensor(1.9537, grad_fn=<MseLossBackward0>), 1: tensor(0.6323, grad_fn=<MseLossBackward0>), 2: tensor(1.0749, grad_fn=<MseLossBackward0>)}
0.999755
episode:  0 training step:  75 loss of agent  0 :  {0: tensor(1.6344, grad_fn=<MseLossBackward0>), 1: tensor(0.6323, grad_fn=<MseLossBackward0>), 2: tensor(1.0749, grad_fn=<MseLossBackward0>)}
0.999755
episode:  0 training step:  75 loss of agent  1 :  {0: tensor(1.6344, grad_fn=<MseLossBackward0>), 1: tensor(0.6564, grad_fn=<MseLossBackward0>), 2: tensor(1.0749, grad_fn=<MseLossBackward0>)}
0.999755
episode:  0 training step:  75 loss of agent  2 :  {0: tensor(1.6344, grad_fn=<MseLossBackward0>), 1: tensor(0.6564, grad_fn=<MseLossBackward0>), 2: tensor(0.9770, grad_fn=<MseLossBackward0>)}
0.9997517333333333
episode:  0 training step:  76 loss of agent  0 :  {0: tensor(1.4112, grad_fn=<MseLossBackward0>), 1: tensor(0.6564, grad_fn=<MseLossBackward0>), 2: tensor(0.9770, grad_fn=<MseLossBackward0>)}
0.9997517333333333
episode:  0 training step:  76 loss of agent  1 :  {0: tensor(1.4112, grad_fn=<MseLossBackward0>), 1: tensor(0.4668, grad_fn=<MseLossBackward0>), 2: tensor(0.9770, grad_fn=<MseLossBackward0>)}
0.9997517333333333
episode:  0 training step:  76 loss of agent  2 :  {0: tensor(1.4112, grad_fn=<MseLossBackward0>), 1: tensor(0.4668, grad_fn=<MseLossBackward0>), 2: tensor(0.8358, grad_fn=<MseLossBackward0>)}
0.9997484666666666
episode:  0 training step:  77 loss of agent  0 :  {0: tensor(1.5483, grad_fn=<MseLossBackward0>), 1: tensor(0.4668, grad_fn=<MseLossBackward0>), 2: tensor(0.8358, grad_fn=<MseLossBackward0>)}
0.9997484666666666
episode:  0 training step:  77 loss of agent  1 :  {0: tensor(1.5483, grad_fn=<MseLossBackward0>), 1: tensor(0.4803, grad_fn=<MseLossBackward0>), 2: tensor(0.8358, grad_fn=<MseLossBackward0>)}
0.9997484666666666
episode:  0 training step:  77 loss of agent  2 :  {0: tensor(1.5483, grad_fn=<MseLossBackward0>), 1: tensor(0.4803, grad_fn=<MseLossBackward0>), 2: tensor(0.6926, grad_fn=<MseLossBackward0>)}
0.9997452
episode:  0 training step:  78 loss of agent  0 :  {0: tensor(1.3361, grad_fn=<MseLossBackward0>), 1: tensor(0.4803, grad_fn=<MseLossBackward0>), 2: tensor(0.6926, grad_fn=<MseLossBackward0>)}
0.9997452
episode:  0 training step:  78 loss of agent  1 :  {0: tensor(1.3361, grad_fn=<MseLossBackward0>), 1: tensor(0.3261, grad_fn=<MseLossBackward0>), 2: tensor(0.6926, grad_fn=<MseLossBackward0>)}
0.9997452
episode:  0 training step:  78 loss of agent  2 :  {0: tensor(1.3361, grad_fn=<MseLossBackward0>), 1: tensor(0.3261, grad_fn=<MseLossBackward0>), 2: tensor(0.5066, grad_fn=<MseLossBackward0>)}
0.9997419333333334
episode:  0 training step:  79 loss of agent  0 :  {0: tensor(1.2432, grad_fn=<MseLossBackward0>), 1: tensor(0.3261, grad_fn=<MseLossBackward0>), 2: tensor(0.5066, grad_fn=<MseLossBackward0>)}
0.9997419333333334
episode:  0 training step:  79 loss of agent  1 :  {0: tensor(1.2432, grad_fn=<MseLossBackward0>), 1: tensor(0.2381, grad_fn=<MseLossBackward0>), 2: tensor(0.5066, grad_fn=<MseLossBackward0>)}
0.9997419333333334
episode:  0 training step:  79 loss of agent  2 :  {0: tensor(1.2432, grad_fn=<MseLossBackward0>), 1: tensor(0.2381, grad_fn=<MseLossBackward0>), 2: tensor(0.3433, grad_fn=<MseLossBackward0>)}
0.9997386666666667
episode:  0 training step:  80 loss of agent  0 :  {0: tensor(1.0921, grad_fn=<MseLossBackward0>), 1: tensor(0.2381, grad_fn=<MseLossBackward0>), 2: tensor(0.3433, grad_fn=<MseLossBackward0>)}
0.9997386666666667
episode:  0 training step:  80 loss of agent  1 :  {0: tensor(1.0921, grad_fn=<MseLossBackward0>), 1: tensor(0.2376, grad_fn=<MseLossBackward0>), 2: tensor(0.3433, grad_fn=<MseLossBackward0>)}
0.9997386666666667
episode:  0 training step:  80 loss of agent  2 :  {0: tensor(1.0921, grad_fn=<MseLossBackward0>), 1: tensor(0.2376, grad_fn=<MseLossBackward0>), 2: tensor(0.2585, grad_fn=<MseLossBackward0>)}
0.9997354
episode:  0 training step:  81 loss of agent  0 :  {0: tensor(1.9195, grad_fn=<MseLossBackward0>), 1: tensor(0.2376, grad_fn=<MseLossBackward0>), 2: tensor(0.2585, grad_fn=<MseLossBackward0>)}
0.9997354
episode:  0 training step:  81 loss of agent  1 :  {0: tensor(1.9195, grad_fn=<MseLossBackward0>), 1: tensor(1.0280, grad_fn=<MseLossBackward0>), 2: tensor(0.2585, grad_fn=<MseLossBackward0>)}
0.9997354
episode:  0 training step:  81 loss of agent  2 :  {0: tensor(1.9195, grad_fn=<MseLossBackward0>), 1: tensor(1.0280, grad_fn=<MseLossBackward0>), 2: tensor(1.5112, grad_fn=<MseLossBackward0>)}
0.9997321333333333
episode:  0 training step:  82 loss of agent  0 :  {0: tensor(1.5332, grad_fn=<MseLossBackward0>), 1: tensor(1.0280, grad_fn=<MseLossBackward0>), 2: tensor(1.5112, grad_fn=<MseLossBackward0>)}
0.9997321333333333
episode:  0 training step:  82 loss of agent  1 :  {0: tensor(1.5332, grad_fn=<MseLossBackward0>), 1: tensor(0.9377, grad_fn=<MseLossBackward0>), 2: tensor(1.5112, grad_fn=<MseLossBackward0>)}
0.9997321333333333
episode:  0 training step:  82 loss of agent  2 :  {0: tensor(1.5332, grad_fn=<MseLossBackward0>), 1: tensor(0.9377, grad_fn=<MseLossBackward0>), 2: tensor(1.1937, grad_fn=<MseLossBackward0>)}
0.9997288666666667
episode:  0 training step:  83 loss of agent  0 :  {0: tensor(1.6509, grad_fn=<MseLossBackward0>), 1: tensor(0.9377, grad_fn=<MseLossBackward0>), 2: tensor(1.1937, grad_fn=<MseLossBackward0>)}
0.9997288666666667
episode:  0 training step:  83 loss of agent  1 :  {0: tensor(1.6509, grad_fn=<MseLossBackward0>), 1: tensor(0.7057, grad_fn=<MseLossBackward0>), 2: tensor(1.1937, grad_fn=<MseLossBackward0>)}
0.9997288666666667
episode:  0 training step:  83 loss of agent  2 :  {0: tensor(1.6509, grad_fn=<MseLossBackward0>), 1: tensor(0.7057, grad_fn=<MseLossBackward0>), 2: tensor(0.9420, grad_fn=<MseLossBackward0>)}
0.9997256
episode:  0 training step:  84 loss of agent  0 :  {0: tensor(1.1107, grad_fn=<MseLossBackward0>), 1: tensor(0.7057, grad_fn=<MseLossBackward0>), 2: tensor(0.9420, grad_fn=<MseLossBackward0>)}
0.9997256
episode:  0 training step:  84 loss of agent  1 :  {0: tensor(1.1107, grad_fn=<MseLossBackward0>), 1: tensor(0.5762, grad_fn=<MseLossBackward0>), 2: tensor(0.9420, grad_fn=<MseLossBackward0>)}
0.9997256
episode:  0 training step:  84 loss of agent  2 :  {0: tensor(1.1107, grad_fn=<MseLossBackward0>), 1: tensor(0.5762, grad_fn=<MseLossBackward0>), 2: tensor(0.8220, grad_fn=<MseLossBackward0>)}
0.9997223333333334
episode:  0 training step:  85 loss of agent  0 :  {0: tensor(1.0704, grad_fn=<MseLossBackward0>), 1: tensor(0.5762, grad_fn=<MseLossBackward0>), 2: tensor(0.8220, grad_fn=<MseLossBackward0>)}
0.9997223333333334
episode:  0 training step:  85 loss of agent  1 :  {0: tensor(1.0704, grad_fn=<MseLossBackward0>), 1: tensor(0.3748, grad_fn=<MseLossBackward0>), 2: tensor(0.8220, grad_fn=<MseLossBackward0>)}
0.9997223333333334
episode:  0 training step:  85 loss of agent  2 :  {0: tensor(1.0704, grad_fn=<MseLossBackward0>), 1: tensor(0.3748, grad_fn=<MseLossBackward0>), 2: tensor(0.7796, grad_fn=<MseLossBackward0>)}
0.9997190666666667
episode:  0 training step:  86 loss of agent  0 :  {0: tensor(1.0544, grad_fn=<MseLossBackward0>), 1: tensor(0.3748, grad_fn=<MseLossBackward0>), 2: tensor(0.7796, grad_fn=<MseLossBackward0>)}
0.9997190666666667
episode:  0 training step:  86 loss of agent  1 :  {0: tensor(1.0544, grad_fn=<MseLossBackward0>), 1: tensor(0.3364, grad_fn=<MseLossBackward0>), 2: tensor(0.7796, grad_fn=<MseLossBackward0>)}
0.9997190666666667
episode:  0 training step:  86 loss of agent  2 :  {0: tensor(1.0544, grad_fn=<MseLossBackward0>), 1: tensor(0.3364, grad_fn=<MseLossBackward0>), 2: tensor(0.6568, grad_fn=<MseLossBackward0>)}
0.9997158
episode:  0 training step:  87 loss of agent  0 :  {0: tensor(0.9563, grad_fn=<MseLossBackward0>), 1: tensor(0.3364, grad_fn=<MseLossBackward0>), 2: tensor(0.6568, grad_fn=<MseLossBackward0>)}
0.9997158
episode:  0 training step:  87 loss of agent  1 :  {0: tensor(0.9563, grad_fn=<MseLossBackward0>), 1: tensor(0.3409, grad_fn=<MseLossBackward0>), 2: tensor(0.6568, grad_fn=<MseLossBackward0>)}
0.9997158
episode:  0 training step:  87 loss of agent  2 :  {0: tensor(0.9563, grad_fn=<MseLossBackward0>), 1: tensor(0.3409, grad_fn=<MseLossBackward0>), 2: tensor(0.4875, grad_fn=<MseLossBackward0>)}
0.9997125333333333
episode:  0 training step:  88 loss of agent  0 :  {0: tensor(0.5723, grad_fn=<MseLossBackward0>), 1: tensor(0.3409, grad_fn=<MseLossBackward0>), 2: tensor(0.4875, grad_fn=<MseLossBackward0>)}
0.9997125333333333
episode:  0 training step:  88 loss of agent  1 :  {0: tensor(0.5723, grad_fn=<MseLossBackward0>), 1: tensor(0.2348, grad_fn=<MseLossBackward0>), 2: tensor(0.4875, grad_fn=<MseLossBackward0>)}
0.9997125333333333
episode:  0 training step:  88 loss of agent  2 :  {0: tensor(0.5723, grad_fn=<MseLossBackward0>), 1: tensor(0.2348, grad_fn=<MseLossBackward0>), 2: tensor(0.3736, grad_fn=<MseLossBackward0>)}
0.9997092666666667
episode:  0 training step:  89 loss of agent  0 :  {0: tensor(0.6224, grad_fn=<MseLossBackward0>), 1: tensor(0.2348, grad_fn=<MseLossBackward0>), 2: tensor(0.3736, grad_fn=<MseLossBackward0>)}
0.9997092666666667
episode:  0 training step:  89 loss of agent  1 :  {0: tensor(0.6224, grad_fn=<MseLossBackward0>), 1: tensor(0.1998, grad_fn=<MseLossBackward0>), 2: tensor(0.3736, grad_fn=<MseLossBackward0>)}
0.9997092666666667
episode:  0 training step:  89 loss of agent  2 :  {0: tensor(0.6224, grad_fn=<MseLossBackward0>), 1: tensor(0.1998, grad_fn=<MseLossBackward0>), 2: tensor(0.2229, grad_fn=<MseLossBackward0>)}
0.999706
episode:  0 training step:  90 loss of agent  0 :  {0: tensor(0.5023, grad_fn=<MseLossBackward0>), 1: tensor(0.1998, grad_fn=<MseLossBackward0>), 2: tensor(0.2229, grad_fn=<MseLossBackward0>)}
0.999706
episode:  0 training step:  90 loss of agent  1 :  {0: tensor(0.5023, grad_fn=<MseLossBackward0>), 1: tensor(0.1671, grad_fn=<MseLossBackward0>), 2: tensor(0.2229, grad_fn=<MseLossBackward0>)}
0.999706
episode:  0 training step:  90 loss of agent  2 :  {0: tensor(0.5023, grad_fn=<MseLossBackward0>), 1: tensor(0.1671, grad_fn=<MseLossBackward0>), 2: tensor(0.1810, grad_fn=<MseLossBackward0>)}
0.9997027333333334
episode:  0 training step:  91 loss of agent  0 :  {0: tensor(1.2250, grad_fn=<MseLossBackward0>), 1: tensor(0.1671, grad_fn=<MseLossBackward0>), 2: tensor(0.1810, grad_fn=<MseLossBackward0>)}
0.9997027333333334
episode:  0 training step:  91 loss of agent  1 :  {0: tensor(1.2250, grad_fn=<MseLossBackward0>), 1: tensor(0.9938, grad_fn=<MseLossBackward0>), 2: tensor(0.1810, grad_fn=<MseLossBackward0>)}
0.9997027333333334
episode:  0 training step:  91 loss of agent  2 :  {0: tensor(1.2250, grad_fn=<MseLossBackward0>), 1: tensor(0.9938, grad_fn=<MseLossBackward0>), 2: tensor(1.4841, grad_fn=<MseLossBackward0>)}
0.9996994666666666
episode:  0 training step:  92 loss of agent  0 :  {0: tensor(1.3697, grad_fn=<MseLossBackward0>), 1: tensor(0.9938, grad_fn=<MseLossBackward0>), 2: tensor(1.4841, grad_fn=<MseLossBackward0>)}
0.9996994666666666
episode:  0 training step:  92 loss of agent  1 :  {0: tensor(1.3697, grad_fn=<MseLossBackward0>), 1: tensor(0.7581, grad_fn=<MseLossBackward0>), 2: tensor(1.4841, grad_fn=<MseLossBackward0>)}
0.9996994666666666
episode:  0 training step:  92 loss of agent  2 :  {0: tensor(1.3697, grad_fn=<MseLossBackward0>), 1: tensor(0.7581, grad_fn=<MseLossBackward0>), 2: tensor(1.4018, grad_fn=<MseLossBackward0>)}
0.9996962
episode:  0 training step:  93 loss of agent  0 :  {0: tensor(0.9439, grad_fn=<MseLossBackward0>), 1: tensor(0.7581, grad_fn=<MseLossBackward0>), 2: tensor(1.4018, grad_fn=<MseLossBackward0>)}
0.9996962
episode:  0 training step:  93 loss of agent  1 :  {0: tensor(0.9439, grad_fn=<MseLossBackward0>), 1: tensor(0.7765, grad_fn=<MseLossBackward0>), 2: tensor(1.4018, grad_fn=<MseLossBackward0>)}
0.9996962
episode:  0 training step:  93 loss of agent  2 :  {0: tensor(0.9439, grad_fn=<MseLossBackward0>), 1: tensor(0.7765, grad_fn=<MseLossBackward0>), 2: tensor(0.7546, grad_fn=<MseLossBackward0>)}
0.9996929333333333
episode:  0 training step:  94 loss of agent  0 :  {0: tensor(0.8503, grad_fn=<MseLossBackward0>), 1: tensor(0.7765, grad_fn=<MseLossBackward0>), 2: tensor(0.7546, grad_fn=<MseLossBackward0>)}
0.9996929333333333
episode:  0 training step:  94 loss of agent  1 :  {0: tensor(0.8503, grad_fn=<MseLossBackward0>), 1: tensor(0.5187, grad_fn=<MseLossBackward0>), 2: tensor(0.7546, grad_fn=<MseLossBackward0>)}
0.9996929333333333
episode:  0 training step:  94 loss of agent  2 :  {0: tensor(0.8503, grad_fn=<MseLossBackward0>), 1: tensor(0.5187, grad_fn=<MseLossBackward0>), 2: tensor(0.9075, grad_fn=<MseLossBackward0>)}
0.9996896666666667
episode:  0 training step:  95 loss of agent  0 :  {0: tensor(0.6278, grad_fn=<MseLossBackward0>), 1: tensor(0.5187, grad_fn=<MseLossBackward0>), 2: tensor(0.9075, grad_fn=<MseLossBackward0>)}
0.9996896666666667
episode:  0 training step:  95 loss of agent  1 :  {0: tensor(0.6278, grad_fn=<MseLossBackward0>), 1: tensor(0.4818, grad_fn=<MseLossBackward0>), 2: tensor(0.9075, grad_fn=<MseLossBackward0>)}
0.9996896666666667
episode:  0 training step:  95 loss of agent  2 :  {0: tensor(0.6278, grad_fn=<MseLossBackward0>), 1: tensor(0.4818, grad_fn=<MseLossBackward0>), 2: tensor(0.6329, grad_fn=<MseLossBackward0>)}
0.9996864
episode:  0 training step:  96 loss of agent  0 :  {0: tensor(0.7846, grad_fn=<MseLossBackward0>), 1: tensor(0.4818, grad_fn=<MseLossBackward0>), 2: tensor(0.6329, grad_fn=<MseLossBackward0>)}
0.9996864
episode:  0 training step:  96 loss of agent  1 :  {0: tensor(0.7846, grad_fn=<MseLossBackward0>), 1: tensor(0.3265, grad_fn=<MseLossBackward0>), 2: tensor(0.6329, grad_fn=<MseLossBackward0>)}
0.9996864
episode:  0 training step:  96 loss of agent  2 :  {0: tensor(0.7846, grad_fn=<MseLossBackward0>), 1: tensor(0.3265, grad_fn=<MseLossBackward0>), 2: tensor(0.4428, grad_fn=<MseLossBackward0>)}
0.9996831333333334
episode:  0 training step:  97 loss of agent  0 :  {0: tensor(0.5577, grad_fn=<MseLossBackward0>), 1: tensor(0.3265, grad_fn=<MseLossBackward0>), 2: tensor(0.4428, grad_fn=<MseLossBackward0>)}
0.9996831333333334
episode:  0 training step:  97 loss of agent  1 :  {0: tensor(0.5577, grad_fn=<MseLossBackward0>), 1: tensor(0.2830, grad_fn=<MseLossBackward0>), 2: tensor(0.4428, grad_fn=<MseLossBackward0>)}
0.9996831333333334
episode:  0 training step:  97 loss of agent  2 :  {0: tensor(0.5577, grad_fn=<MseLossBackward0>), 1: tensor(0.2830, grad_fn=<MseLossBackward0>), 2: tensor(0.3199, grad_fn=<MseLossBackward0>)}
0.9996798666666666
episode:  0 training step:  98 loss of agent  0 :  {0: tensor(0.3911, grad_fn=<MseLossBackward0>), 1: tensor(0.2830, grad_fn=<MseLossBackward0>), 2: tensor(0.3199, grad_fn=<MseLossBackward0>)}
0.9996798666666666
episode:  0 training step:  98 loss of agent  1 :  {0: tensor(0.3911, grad_fn=<MseLossBackward0>), 1: tensor(0.1978, grad_fn=<MseLossBackward0>), 2: tensor(0.3199, grad_fn=<MseLossBackward0>)}
0.9996798666666666
episode:  0 training step:  98 loss of agent  2 :  {0: tensor(0.3911, grad_fn=<MseLossBackward0>), 1: tensor(0.1978, grad_fn=<MseLossBackward0>), 2: tensor(0.2779, grad_fn=<MseLossBackward0>)}
0.9996766
episode:  0 training step:  99 loss of agent  0 :  {0: tensor(0.3888, grad_fn=<MseLossBackward0>), 1: tensor(0.1978, grad_fn=<MseLossBackward0>), 2: tensor(0.2779, grad_fn=<MseLossBackward0>)}
0.9996766
episode:  0 training step:  99 loss of agent  1 :  {0: tensor(0.3888, grad_fn=<MseLossBackward0>), 1: tensor(0.1922, grad_fn=<MseLossBackward0>), 2: tensor(0.2779, grad_fn=<MseLossBackward0>)}
0.9996766
episode:  0 training step:  99 loss of agent  2 :  {0: tensor(0.3888, grad_fn=<MseLossBackward0>), 1: tensor(0.1922, grad_fn=<MseLossBackward0>), 2: tensor(0.1710, grad_fn=<MseLossBackward0>)}
0.9996733333333333
episode:  0 training step:  100 loss of agent  0 :  {0: tensor(0.4292, grad_fn=<MseLossBackward0>), 1: tensor(0.1922, grad_fn=<MseLossBackward0>), 2: tensor(0.1710, grad_fn=<MseLossBackward0>)}
0.9996733333333333
episode:  0 training step:  100 loss of agent  1 :  {0: tensor(0.4292, grad_fn=<MseLossBackward0>), 1: tensor(0.1060, grad_fn=<MseLossBackward0>), 2: tensor(0.1710, grad_fn=<MseLossBackward0>)}
0.9996733333333333
episode:  0 training step:  100 loss of agent  2 :  {0: tensor(0.4292, grad_fn=<MseLossBackward0>), 1: tensor(0.1060, grad_fn=<MseLossBackward0>), 2: tensor(0.1341, grad_fn=<MseLossBackward0>)}
seed: 5 steps: 0k score: {'adversary_0': -920.3636341974859, 'agent_0': 841.687466902539, 'agent_1': 841.687466902539}
0.9996700666666667
episode:  0 training step:  101 loss of agent  0 :  {0: tensor(0.9130, grad_fn=<MseLossBackward0>), 1: tensor(0.1060, grad_fn=<MseLossBackward0>), 2: tensor(0.1341, grad_fn=<MseLossBackward0>)}
0.9996700666666667
episode:  0 training step:  101 loss of agent  1 :  {0: tensor(0.9130, grad_fn=<MseLossBackward0>), 1: tensor(1.2509, grad_fn=<MseLossBackward0>), 2: tensor(0.1341, grad_fn=<MseLossBackward0>)}
0.9996700666666667
episode:  0 training step:  101 loss of agent  2 :  {0: tensor(0.9130, grad_fn=<MseLossBackward0>), 1: tensor(1.2509, grad_fn=<MseLossBackward0>), 2: tensor(1.3617, grad_fn=<MseLossBackward0>)}
0.9996668
episode:  0 training step:  102 loss of agent  0 :  {0: tensor(0.8148, grad_fn=<MseLossBackward0>), 1: tensor(1.2509, grad_fn=<MseLossBackward0>), 2: tensor(1.3617, grad_fn=<MseLossBackward0>)}
0.9996668
episode:  0 training step:  102 loss of agent  1 :  {0: tensor(0.8148, grad_fn=<MseLossBackward0>), 1: tensor(1.0502, grad_fn=<MseLossBackward0>), 2: tensor(1.3617, grad_fn=<MseLossBackward0>)}
0.9996668
episode:  0 training step:  102 loss of agent  2 :  {0: tensor(0.8148, grad_fn=<MseLossBackward0>), 1: tensor(1.0502, grad_fn=<MseLossBackward0>), 2: tensor(1.1335, grad_fn=<MseLossBackward0>)}
0.9996635333333334
episode:  0 training step:  103 loss of agent  0 :  {0: tensor(0.6241, grad_fn=<MseLossBackward0>), 1: tensor(1.0502, grad_fn=<MseLossBackward0>), 2: tensor(1.1335, grad_fn=<MseLossBackward0>)}
0.9996635333333334
episode:  0 training step:  103 loss of agent  1 :  {0: tensor(0.6241, grad_fn=<MseLossBackward0>), 1: tensor(0.6920, grad_fn=<MseLossBackward0>), 2: tensor(1.1335, grad_fn=<MseLossBackward0>)}
0.9996635333333334
episode:  0 training step:  103 loss of agent  2 :  {0: tensor(0.6241, grad_fn=<MseLossBackward0>), 1: tensor(0.6920, grad_fn=<MseLossBackward0>), 2: tensor(0.7605, grad_fn=<MseLossBackward0>)}
0.9996602666666666
episode:  0 training step:  104 loss of agent  0 :  {0: tensor(0.5169, grad_fn=<MseLossBackward0>), 1: tensor(0.6920, grad_fn=<MseLossBackward0>), 2: tensor(0.7605, grad_fn=<MseLossBackward0>)}
0.9996602666666666
episode:  0 training step:  104 loss of agent  1 :  {0: tensor(0.5169, grad_fn=<MseLossBackward0>), 1: tensor(0.6417, grad_fn=<MseLossBackward0>), 2: tensor(0.7605, grad_fn=<MseLossBackward0>)}
0.9996602666666666
episode:  0 training step:  104 loss of agent  2 :  {0: tensor(0.5169, grad_fn=<MseLossBackward0>), 1: tensor(0.6417, grad_fn=<MseLossBackward0>), 2: tensor(0.6172, grad_fn=<MseLossBackward0>)}
0.999657
episode:  0 training step:  105 loss of agent  0 :  {0: tensor(0.3934, grad_fn=<MseLossBackward0>), 1: tensor(0.6417, grad_fn=<MseLossBackward0>), 2: tensor(0.6172, grad_fn=<MseLossBackward0>)}
0.999657
episode:  0 training step:  105 loss of agent  1 :  {0: tensor(0.3934, grad_fn=<MseLossBackward0>), 1: tensor(0.3985, grad_fn=<MseLossBackward0>), 2: tensor(0.6172, grad_fn=<MseLossBackward0>)}
0.999657
episode:  0 training step:  105 loss of agent  2 :  {0: tensor(0.3934, grad_fn=<MseLossBackward0>), 1: tensor(0.3985, grad_fn=<MseLossBackward0>), 2: tensor(0.4863, grad_fn=<MseLossBackward0>)}
0.9996537333333333
episode:  0 training step:  106 loss of agent  0 :  {0: tensor(0.4026, grad_fn=<MseLossBackward0>), 1: tensor(0.3985, grad_fn=<MseLossBackward0>), 2: tensor(0.4863, grad_fn=<MseLossBackward0>)}
0.9996537333333333
episode:  0 training step:  106 loss of agent  1 :  {0: tensor(0.4026, grad_fn=<MseLossBackward0>), 1: tensor(0.2957, grad_fn=<MseLossBackward0>), 2: tensor(0.4863, grad_fn=<MseLossBackward0>)}
0.9996537333333333
episode:  0 training step:  106 loss of agent  2 :  {0: tensor(0.4026, grad_fn=<MseLossBackward0>), 1: tensor(0.2957, grad_fn=<MseLossBackward0>), 2: tensor(0.3881, grad_fn=<MseLossBackward0>)}
0.9996504666666667
episode:  0 training step:  107 loss of agent  0 :  {0: tensor(0.4947, grad_fn=<MseLossBackward0>), 1: tensor(0.2957, grad_fn=<MseLossBackward0>), 2: tensor(0.3881, grad_fn=<MseLossBackward0>)}
0.9996504666666667
episode:  0 training step:  107 loss of agent  1 :  {0: tensor(0.4947, grad_fn=<MseLossBackward0>), 1: tensor(0.2339, grad_fn=<MseLossBackward0>), 2: tensor(0.3881, grad_fn=<MseLossBackward0>)}
0.9996504666666667
episode:  0 training step:  107 loss of agent  2 :  {0: tensor(0.4947, grad_fn=<MseLossBackward0>), 1: tensor(0.2339, grad_fn=<MseLossBackward0>), 2: tensor(0.2142, grad_fn=<MseLossBackward0>)}
0.9996472
episode:  0 training step:  108 loss of agent  0 :  {0: tensor(0.2049, grad_fn=<MseLossBackward0>), 1: tensor(0.2339, grad_fn=<MseLossBackward0>), 2: tensor(0.2142, grad_fn=<MseLossBackward0>)}
0.9996472
episode:  0 training step:  108 loss of agent  1 :  {0: tensor(0.2049, grad_fn=<MseLossBackward0>), 1: tensor(0.1558, grad_fn=<MseLossBackward0>), 2: tensor(0.2142, grad_fn=<MseLossBackward0>)}
0.9996472
episode:  0 training step:  108 loss of agent  2 :  {0: tensor(0.2049, grad_fn=<MseLossBackward0>), 1: tensor(0.1558, grad_fn=<MseLossBackward0>), 2: tensor(0.1770, grad_fn=<MseLossBackward0>)}
0.9996439333333333
episode:  0 training step:  109 loss of agent  0 :  {0: tensor(0.2087, grad_fn=<MseLossBackward0>), 1: tensor(0.1558, grad_fn=<MseLossBackward0>), 2: tensor(0.1770, grad_fn=<MseLossBackward0>)}
0.9996439333333333
episode:  0 training step:  109 loss of agent  1 :  {0: tensor(0.2087, grad_fn=<MseLossBackward0>), 1: tensor(0.1424, grad_fn=<MseLossBackward0>), 2: tensor(0.1770, grad_fn=<MseLossBackward0>)}
0.9996439333333333
episode:  0 training step:  109 loss of agent  2 :  {0: tensor(0.2087, grad_fn=<MseLossBackward0>), 1: tensor(0.1424, grad_fn=<MseLossBackward0>), 2: tensor(0.1416, grad_fn=<MseLossBackward0>)}
0.9996406666666666
episode:  0 training step:  110 loss of agent  0 :  {0: tensor(0.1324, grad_fn=<MseLossBackward0>), 1: tensor(0.1424, grad_fn=<MseLossBackward0>), 2: tensor(0.1416, grad_fn=<MseLossBackward0>)}
0.9996406666666666
episode:  0 training step:  110 loss of agent  1 :  {0: tensor(0.1324, grad_fn=<MseLossBackward0>), 1: tensor(0.1586, grad_fn=<MseLossBackward0>), 2: tensor(0.1416, grad_fn=<MseLossBackward0>)}
0.9996406666666666
episode:  0 training step:  110 loss of agent  2 :  {0: tensor(0.1324, grad_fn=<MseLossBackward0>), 1: tensor(0.1586, grad_fn=<MseLossBackward0>), 2: tensor(0.1277, grad_fn=<MseLossBackward0>)}
0.9996374
episode:  0 training step:  111 loss of agent  0 :  {0: tensor(1.0148, grad_fn=<MseLossBackward0>), 1: tensor(0.1586, grad_fn=<MseLossBackward0>), 2: tensor(0.1277, grad_fn=<MseLossBackward0>)}
0.9996374
episode:  0 training step:  111 loss of agent  1 :  {0: tensor(1.0148, grad_fn=<MseLossBackward0>), 1: tensor(1.3025, grad_fn=<MseLossBackward0>), 2: tensor(0.1277, grad_fn=<MseLossBackward0>)}
0.9996374
episode:  0 training step:  111 loss of agent  2 :  {0: tensor(1.0148, grad_fn=<MseLossBackward0>), 1: tensor(1.3025, grad_fn=<MseLossBackward0>), 2: tensor(1.1825, grad_fn=<MseLossBackward0>)}
0.9996341333333333
episode:  0 training step:  112 loss of agent  0 :  {0: tensor(0.6962, grad_fn=<MseLossBackward0>), 1: tensor(1.3025, grad_fn=<MseLossBackward0>), 2: tensor(1.1825, grad_fn=<MseLossBackward0>)}
0.9996341333333333
episode:  0 training step:  112 loss of agent  1 :  {0: tensor(0.6962, grad_fn=<MseLossBackward0>), 1: tensor(0.8380, grad_fn=<MseLossBackward0>), 2: tensor(1.1825, grad_fn=<MseLossBackward0>)}
0.9996341333333333
episode:  0 training step:  112 loss of agent  2 :  {0: tensor(0.6962, grad_fn=<MseLossBackward0>), 1: tensor(0.8380, grad_fn=<MseLossBackward0>), 2: tensor(1.3191, grad_fn=<MseLossBackward0>)}
0.9996308666666667
episode:  0 training step:  113 loss of agent  0 :  {0: tensor(0.5711, grad_fn=<MseLossBackward0>), 1: tensor(0.8380, grad_fn=<MseLossBackward0>), 2: tensor(1.3191, grad_fn=<MseLossBackward0>)}
0.9996308666666667
episode:  0 training step:  113 loss of agent  1 :  {0: tensor(0.5711, grad_fn=<MseLossBackward0>), 1: tensor(0.6107, grad_fn=<MseLossBackward0>), 2: tensor(1.3191, grad_fn=<MseLossBackward0>)}
0.9996308666666667
episode:  0 training step:  113 loss of agent  2 :  {0: tensor(0.5711, grad_fn=<MseLossBackward0>), 1: tensor(0.6107, grad_fn=<MseLossBackward0>), 2: tensor(0.8932, grad_fn=<MseLossBackward0>)}
0.9996276
episode:  0 training step:  114 loss of agent  0 :  {0: tensor(0.4694, grad_fn=<MseLossBackward0>), 1: tensor(0.6107, grad_fn=<MseLossBackward0>), 2: tensor(0.8932, grad_fn=<MseLossBackward0>)}
0.9996276
episode:  0 training step:  114 loss of agent  1 :  {0: tensor(0.4694, grad_fn=<MseLossBackward0>), 1: tensor(0.5711, grad_fn=<MseLossBackward0>), 2: tensor(0.8932, grad_fn=<MseLossBackward0>)}
0.9996276
episode:  0 training step:  114 loss of agent  2 :  {0: tensor(0.4694, grad_fn=<MseLossBackward0>), 1: tensor(0.5711, grad_fn=<MseLossBackward0>), 2: tensor(0.5717, grad_fn=<MseLossBackward0>)}
0.9996243333333333
episode:  0 training step:  115 loss of agent  0 :  {0: tensor(0.2958, grad_fn=<MseLossBackward0>), 1: tensor(0.5711, grad_fn=<MseLossBackward0>), 2: tensor(0.5717, grad_fn=<MseLossBackward0>)}
0.9996243333333333
episode:  0 training step:  115 loss of agent  1 :  {0: tensor(0.2958, grad_fn=<MseLossBackward0>), 1: tensor(0.3516, grad_fn=<MseLossBackward0>), 2: tensor(0.5717, grad_fn=<MseLossBackward0>)}
0.9996243333333333
episode:  0 training step:  115 loss of agent  2 :  {0: tensor(0.2958, grad_fn=<MseLossBackward0>), 1: tensor(0.3516, grad_fn=<MseLossBackward0>), 2: tensor(0.3143, grad_fn=<MseLossBackward0>)}
0.9996210666666666
episode:  0 training step:  116 loss of agent  0 :  {0: tensor(0.3173, grad_fn=<MseLossBackward0>), 1: tensor(0.3516, grad_fn=<MseLossBackward0>), 2: tensor(0.3143, grad_fn=<MseLossBackward0>)}
0.9996210666666666
episode:  0 training step:  116 loss of agent  1 :  {0: tensor(0.3173, grad_fn=<MseLossBackward0>), 1: tensor(0.2131, grad_fn=<MseLossBackward0>), 2: tensor(0.3143, grad_fn=<MseLossBackward0>)}
0.9996210666666666
episode:  0 training step:  116 loss of agent  2 :  {0: tensor(0.3173, grad_fn=<MseLossBackward0>), 1: tensor(0.2131, grad_fn=<MseLossBackward0>), 2: tensor(0.3016, grad_fn=<MseLossBackward0>)}
0.9996178
episode:  0 training step:  117 loss of agent  0 :  {0: tensor(0.4279, grad_fn=<MseLossBackward0>), 1: tensor(0.2131, grad_fn=<MseLossBackward0>), 2: tensor(0.3016, grad_fn=<MseLossBackward0>)}
0.9996178
episode:  0 training step:  117 loss of agent  1 :  {0: tensor(0.4279, grad_fn=<MseLossBackward0>), 1: tensor(0.1538, grad_fn=<MseLossBackward0>), 2: tensor(0.3016, grad_fn=<MseLossBackward0>)}
0.9996178
episode:  0 training step:  117 loss of agent  2 :  {0: tensor(0.4279, grad_fn=<MseLossBackward0>), 1: tensor(0.1538, grad_fn=<MseLossBackward0>), 2: tensor(0.2081, grad_fn=<MseLossBackward0>)}
0.9996145333333334
episode:  0 training step:  118 loss of agent  0 :  {0: tensor(0.1809, grad_fn=<MseLossBackward0>), 1: tensor(0.1538, grad_fn=<MseLossBackward0>), 2: tensor(0.2081, grad_fn=<MseLossBackward0>)}
0.9996145333333334
episode:  0 training step:  118 loss of agent  1 :  {0: tensor(0.1809, grad_fn=<MseLossBackward0>), 1: tensor(0.1737, grad_fn=<MseLossBackward0>), 2: tensor(0.2081, grad_fn=<MseLossBackward0>)}
0.9996145333333334
episode:  0 training step:  118 loss of agent  2 :  {0: tensor(0.1809, grad_fn=<MseLossBackward0>), 1: tensor(0.1737, grad_fn=<MseLossBackward0>), 2: tensor(0.0807, grad_fn=<MseLossBackward0>)}
0.9996112666666667
episode:  0 training step:  119 loss of agent  0 :  {0: tensor(0.1676, grad_fn=<MseLossBackward0>), 1: tensor(0.1737, grad_fn=<MseLossBackward0>), 2: tensor(0.0807, grad_fn=<MseLossBackward0>)}
0.9996112666666667
episode:  0 training step:  119 loss of agent  1 :  {0: tensor(0.1676, grad_fn=<MseLossBackward0>), 1: tensor(0.1096, grad_fn=<MseLossBackward0>), 2: tensor(0.0807, grad_fn=<MseLossBackward0>)}
0.9996112666666667
episode:  0 training step:  119 loss of agent  2 :  {0: tensor(0.1676, grad_fn=<MseLossBackward0>), 1: tensor(0.1096, grad_fn=<MseLossBackward0>), 2: tensor(0.1067, grad_fn=<MseLossBackward0>)}
0.999608
episode:  0 training step:  120 loss of agent  0 :  {0: tensor(0.1204, grad_fn=<MseLossBackward0>), 1: tensor(0.1096, grad_fn=<MseLossBackward0>), 2: tensor(0.1067, grad_fn=<MseLossBackward0>)}
0.999608
episode:  0 training step:  120 loss of agent  1 :  {0: tensor(0.1204, grad_fn=<MseLossBackward0>), 1: tensor(0.2613, grad_fn=<MseLossBackward0>), 2: tensor(0.1067, grad_fn=<MseLossBackward0>)}
0.999608
episode:  0 training step:  120 loss of agent  2 :  {0: tensor(0.1204, grad_fn=<MseLossBackward0>), 1: tensor(0.2613, grad_fn=<MseLossBackward0>), 2: tensor(0.1089, grad_fn=<MseLossBackward0>)}
0.9996047333333333
episode:  0 training step:  121 loss of agent  0 :  {0: tensor(0.9468, grad_fn=<MseLossBackward0>), 1: tensor(0.2613, grad_fn=<MseLossBackward0>), 2: tensor(0.1089, grad_fn=<MseLossBackward0>)}
0.9996047333333333
episode:  0 training step:  121 loss of agent  1 :  {0: tensor(0.9468, grad_fn=<MseLossBackward0>), 1: tensor(1.2572, grad_fn=<MseLossBackward0>), 2: tensor(0.1089, grad_fn=<MseLossBackward0>)}
0.9996047333333333
episode:  0 training step:  121 loss of agent  2 :  {0: tensor(0.9468, grad_fn=<MseLossBackward0>), 1: tensor(1.2572, grad_fn=<MseLossBackward0>), 2: tensor(1.2608, grad_fn=<MseLossBackward0>)}
0.9996014666666667
episode:  0 training step:  122 loss of agent  0 :  {0: tensor(0.9049, grad_fn=<MseLossBackward0>), 1: tensor(1.2572, grad_fn=<MseLossBackward0>), 2: tensor(1.2608, grad_fn=<MseLossBackward0>)}
0.9996014666666667
episode:  0 training step:  122 loss of agent  1 :  {0: tensor(0.9049, grad_fn=<MseLossBackward0>), 1: tensor(1.0088, grad_fn=<MseLossBackward0>), 2: tensor(1.2608, grad_fn=<MseLossBackward0>)}
0.9996014666666667
episode:  0 training step:  122 loss of agent  2 :  {0: tensor(0.9049, grad_fn=<MseLossBackward0>), 1: tensor(1.0088, grad_fn=<MseLossBackward0>), 2: tensor(0.8243, grad_fn=<MseLossBackward0>)}
0.9995982
episode:  0 training step:  123 loss of agent  0 :  {0: tensor(0.5754, grad_fn=<MseLossBackward0>), 1: tensor(1.0088, grad_fn=<MseLossBackward0>), 2: tensor(0.8243, grad_fn=<MseLossBackward0>)}
0.9995982
episode:  0 training step:  123 loss of agent  1 :  {0: tensor(0.5754, grad_fn=<MseLossBackward0>), 1: tensor(0.6881, grad_fn=<MseLossBackward0>), 2: tensor(0.8243, grad_fn=<MseLossBackward0>)}
0.9995982
episode:  0 training step:  123 loss of agent  2 :  {0: tensor(0.5754, grad_fn=<MseLossBackward0>), 1: tensor(0.6881, grad_fn=<MseLossBackward0>), 2: tensor(0.6596, grad_fn=<MseLossBackward0>)}
0.9995949333333334
episode:  0 training step:  124 loss of agent  0 :  {0: tensor(0.6181, grad_fn=<MseLossBackward0>), 1: tensor(0.6881, grad_fn=<MseLossBackward0>), 2: tensor(0.6596, grad_fn=<MseLossBackward0>)}
0.9995949333333334
episode:  0 training step:  124 loss of agent  1 :  {0: tensor(0.6181, grad_fn=<MseLossBackward0>), 1: tensor(0.4582, grad_fn=<MseLossBackward0>), 2: tensor(0.6596, grad_fn=<MseLossBackward0>)}
0.9995949333333334
episode:  0 training step:  124 loss of agent  2 :  {0: tensor(0.6181, grad_fn=<MseLossBackward0>), 1: tensor(0.4582, grad_fn=<MseLossBackward0>), 2: tensor(0.5512, grad_fn=<MseLossBackward0>)}
0.9995916666666667
episode:  0 training step:  125 loss of agent  0 :  {0: tensor(0.3994, grad_fn=<MseLossBackward0>), 1: tensor(0.4582, grad_fn=<MseLossBackward0>), 2: tensor(0.5512, grad_fn=<MseLossBackward0>)}
0.9995916666666667
episode:  0 training step:  125 loss of agent  1 :  {0: tensor(0.3994, grad_fn=<MseLossBackward0>), 1: tensor(0.3661, grad_fn=<MseLossBackward0>), 2: tensor(0.5512, grad_fn=<MseLossBackward0>)}
0.9995916666666667
episode:  0 training step:  125 loss of agent  2 :  {0: tensor(0.3994, grad_fn=<MseLossBackward0>), 1: tensor(0.3661, grad_fn=<MseLossBackward0>), 2: tensor(0.2235, grad_fn=<MseLossBackward0>)}
0.9995884
episode:  0 training step:  126 loss of agent  0 :  {0: tensor(0.4224, grad_fn=<MseLossBackward0>), 1: tensor(0.3661, grad_fn=<MseLossBackward0>), 2: tensor(0.2235, grad_fn=<MseLossBackward0>)}
0.9995884
episode:  0 training step:  126 loss of agent  1 :  {0: tensor(0.4224, grad_fn=<MseLossBackward0>), 1: tensor(0.2832, grad_fn=<MseLossBackward0>), 2: tensor(0.2235, grad_fn=<MseLossBackward0>)}
0.9995884
episode:  0 training step:  126 loss of agent  2 :  {0: tensor(0.4224, grad_fn=<MseLossBackward0>), 1: tensor(0.2832, grad_fn=<MseLossBackward0>), 2: tensor(0.1473, grad_fn=<MseLossBackward0>)}
0.9995851333333333
episode:  0 training step:  127 loss of agent  0 :  {0: tensor(0.3210, grad_fn=<MseLossBackward0>), 1: tensor(0.2832, grad_fn=<MseLossBackward0>), 2: tensor(0.1473, grad_fn=<MseLossBackward0>)}
0.9995851333333333
episode:  0 training step:  127 loss of agent  1 :  {0: tensor(0.3210, grad_fn=<MseLossBackward0>), 1: tensor(0.1625, grad_fn=<MseLossBackward0>), 2: tensor(0.1473, grad_fn=<MseLossBackward0>)}
0.9995851333333333
episode:  0 training step:  127 loss of agent  2 :  {0: tensor(0.3210, grad_fn=<MseLossBackward0>), 1: tensor(0.1625, grad_fn=<MseLossBackward0>), 2: tensor(0.1399, grad_fn=<MseLossBackward0>)}
0.9995818666666667
episode:  0 training step:  128 loss of agent  0 :  {0: tensor(0.2399, grad_fn=<MseLossBackward0>), 1: tensor(0.1625, grad_fn=<MseLossBackward0>), 2: tensor(0.1399, grad_fn=<MseLossBackward0>)}
0.9995818666666667
episode:  0 training step:  128 loss of agent  1 :  {0: tensor(0.2399, grad_fn=<MseLossBackward0>), 1: tensor(0.1794, grad_fn=<MseLossBackward0>), 2: tensor(0.1399, grad_fn=<MseLossBackward0>)}
0.9995818666666667
episode:  0 training step:  128 loss of agent  2 :  {0: tensor(0.2399, grad_fn=<MseLossBackward0>), 1: tensor(0.1794, grad_fn=<MseLossBackward0>), 2: tensor(0.1188, grad_fn=<MseLossBackward0>)}
0.9995786
episode:  0 training step:  129 loss of agent  0 :  {0: tensor(0.1646, grad_fn=<MseLossBackward0>), 1: tensor(0.1794, grad_fn=<MseLossBackward0>), 2: tensor(0.1188, grad_fn=<MseLossBackward0>)}
0.9995786
episode:  0 training step:  129 loss of agent  1 :  {0: tensor(0.1646, grad_fn=<MseLossBackward0>), 1: tensor(0.1210, grad_fn=<MseLossBackward0>), 2: tensor(0.1188, grad_fn=<MseLossBackward0>)}
0.9995786
episode:  0 training step:  129 loss of agent  2 :  {0: tensor(0.1646, grad_fn=<MseLossBackward0>), 1: tensor(0.1210, grad_fn=<MseLossBackward0>), 2: tensor(0.1467, grad_fn=<MseLossBackward0>)}
0.9995753333333334
episode:  0 training step:  130 loss of agent  0 :  {0: tensor(0.2369, grad_fn=<MseLossBackward0>), 1: tensor(0.1210, grad_fn=<MseLossBackward0>), 2: tensor(0.1467, grad_fn=<MseLossBackward0>)}
0.9995753333333334
episode:  0 training step:  130 loss of agent  1 :  {0: tensor(0.2369, grad_fn=<MseLossBackward0>), 1: tensor(0.1904, grad_fn=<MseLossBackward0>), 2: tensor(0.1467, grad_fn=<MseLossBackward0>)}
0.9995753333333334
episode:  0 training step:  130 loss of agent  2 :  {0: tensor(0.2369, grad_fn=<MseLossBackward0>), 1: tensor(0.1904, grad_fn=<MseLossBackward0>), 2: tensor(0.1883, grad_fn=<MseLossBackward0>)}
0.9995720666666666
episode:  0 training step:  131 loss of agent  0 :  {0: tensor(1.3706, grad_fn=<MseLossBackward0>), 1: tensor(0.1904, grad_fn=<MseLossBackward0>), 2: tensor(0.1883, grad_fn=<MseLossBackward0>)}
0.9995720666666666
episode:  0 training step:  131 loss of agent  1 :  {0: tensor(1.3706, grad_fn=<MseLossBackward0>), 1: tensor(1.1492, grad_fn=<MseLossBackward0>), 2: tensor(0.1883, grad_fn=<MseLossBackward0>)}
0.9995720666666666
episode:  0 training step:  131 loss of agent  2 :  {0: tensor(1.3706, grad_fn=<MseLossBackward0>), 1: tensor(1.1492, grad_fn=<MseLossBackward0>), 2: tensor(1.0920, grad_fn=<MseLossBackward0>)}
0.9995688
episode:  0 training step:  132 loss of agent  0 :  {0: tensor(1.2046, grad_fn=<MseLossBackward0>), 1: tensor(1.1492, grad_fn=<MseLossBackward0>), 2: tensor(1.0920, grad_fn=<MseLossBackward0>)}
0.9995688
episode:  0 training step:  132 loss of agent  1 :  {0: tensor(1.2046, grad_fn=<MseLossBackward0>), 1: tensor(0.9249, grad_fn=<MseLossBackward0>), 2: tensor(1.0920, grad_fn=<MseLossBackward0>)}
0.9995688
episode:  0 training step:  132 loss of agent  2 :  {0: tensor(1.2046, grad_fn=<MseLossBackward0>), 1: tensor(0.9249, grad_fn=<MseLossBackward0>), 2: tensor(0.8289, grad_fn=<MseLossBackward0>)}
0.9995655333333333
episode:  0 training step:  133 loss of agent  0 :  {0: tensor(0.7565, grad_fn=<MseLossBackward0>), 1: tensor(0.9249, grad_fn=<MseLossBackward0>), 2: tensor(0.8289, grad_fn=<MseLossBackward0>)}
0.9995655333333333
episode:  0 training step:  133 loss of agent  1 :  {0: tensor(0.7565, grad_fn=<MseLossBackward0>), 1: tensor(0.6077, grad_fn=<MseLossBackward0>), 2: tensor(0.8289, grad_fn=<MseLossBackward0>)}
0.9995655333333333
episode:  0 training step:  133 loss of agent  2 :  {0: tensor(0.7565, grad_fn=<MseLossBackward0>), 1: tensor(0.6077, grad_fn=<MseLossBackward0>), 2: tensor(0.4581, grad_fn=<MseLossBackward0>)}
0.9995622666666667
episode:  0 training step:  134 loss of agent  0 :  {0: tensor(0.5179, grad_fn=<MseLossBackward0>), 1: tensor(0.6077, grad_fn=<MseLossBackward0>), 2: tensor(0.4581, grad_fn=<MseLossBackward0>)}
0.9995622666666667
episode:  0 training step:  134 loss of agent  1 :  {0: tensor(0.5179, grad_fn=<MseLossBackward0>), 1: tensor(0.4671, grad_fn=<MseLossBackward0>), 2: tensor(0.4581, grad_fn=<MseLossBackward0>)}
0.9995622666666667
episode:  0 training step:  134 loss of agent  2 :  {0: tensor(0.5179, grad_fn=<MseLossBackward0>), 1: tensor(0.4671, grad_fn=<MseLossBackward0>), 2: tensor(0.4053, grad_fn=<MseLossBackward0>)}
0.999559
episode:  0 training step:  135 loss of agent  0 :  {0: tensor(0.4193, grad_fn=<MseLossBackward0>), 1: tensor(0.4671, grad_fn=<MseLossBackward0>), 2: tensor(0.4053, grad_fn=<MseLossBackward0>)}
0.999559
episode:  0 training step:  135 loss of agent  1 :  {0: tensor(0.4193, grad_fn=<MseLossBackward0>), 1: tensor(0.3136, grad_fn=<MseLossBackward0>), 2: tensor(0.4053, grad_fn=<MseLossBackward0>)}
0.999559
episode:  0 training step:  135 loss of agent  2 :  {0: tensor(0.4193, grad_fn=<MseLossBackward0>), 1: tensor(0.3136, grad_fn=<MseLossBackward0>), 2: tensor(0.2316, grad_fn=<MseLossBackward0>)}
0.9995557333333334
episode:  0 training step:  136 loss of agent  0 :  {0: tensor(0.3339, grad_fn=<MseLossBackward0>), 1: tensor(0.3136, grad_fn=<MseLossBackward0>), 2: tensor(0.2316, grad_fn=<MseLossBackward0>)}
0.9995557333333334
episode:  0 training step:  136 loss of agent  1 :  {0: tensor(0.3339, grad_fn=<MseLossBackward0>), 1: tensor(0.2028, grad_fn=<MseLossBackward0>), 2: tensor(0.2316, grad_fn=<MseLossBackward0>)}
0.9995557333333334
episode:  0 training step:  136 loss of agent  2 :  {0: tensor(0.3339, grad_fn=<MseLossBackward0>), 1: tensor(0.2028, grad_fn=<MseLossBackward0>), 2: tensor(0.1349, grad_fn=<MseLossBackward0>)}
0.9995524666666666
episode:  0 training step:  137 loss of agent  0 :  {0: tensor(0.2526, grad_fn=<MseLossBackward0>), 1: tensor(0.2028, grad_fn=<MseLossBackward0>), 2: tensor(0.1349, grad_fn=<MseLossBackward0>)}
0.9995524666666666
episode:  0 training step:  137 loss of agent  1 :  {0: tensor(0.2526, grad_fn=<MseLossBackward0>), 1: tensor(0.1768, grad_fn=<MseLossBackward0>), 2: tensor(0.1349, grad_fn=<MseLossBackward0>)}
0.9995524666666666
episode:  0 training step:  137 loss of agent  2 :  {0: tensor(0.2526, grad_fn=<MseLossBackward0>), 1: tensor(0.1768, grad_fn=<MseLossBackward0>), 2: tensor(0.1212, grad_fn=<MseLossBackward0>)}
0.9995492
episode:  0 training step:  138 loss of agent  0 :  {0: tensor(0.2097, grad_fn=<MseLossBackward0>), 1: tensor(0.1768, grad_fn=<MseLossBackward0>), 2: tensor(0.1212, grad_fn=<MseLossBackward0>)}
0.9995492
episode:  0 training step:  138 loss of agent  1 :  {0: tensor(0.2097, grad_fn=<MseLossBackward0>), 1: tensor(0.1820, grad_fn=<MseLossBackward0>), 2: tensor(0.1212, grad_fn=<MseLossBackward0>)}
0.9995492
episode:  0 training step:  138 loss of agent  2 :  {0: tensor(0.2097, grad_fn=<MseLossBackward0>), 1: tensor(0.1820, grad_fn=<MseLossBackward0>), 2: tensor(0.1195, grad_fn=<MseLossBackward0>)}
0.9995459333333333
episode:  0 training step:  139 loss of agent  0 :  {0: tensor(0.1302, grad_fn=<MseLossBackward0>), 1: tensor(0.1820, grad_fn=<MseLossBackward0>), 2: tensor(0.1195, grad_fn=<MseLossBackward0>)}
0.9995459333333333
episode:  0 training step:  139 loss of agent  1 :  {0: tensor(0.1302, grad_fn=<MseLossBackward0>), 1: tensor(0.1731, grad_fn=<MseLossBackward0>), 2: tensor(0.1195, grad_fn=<MseLossBackward0>)}
0.9995459333333333
episode:  0 training step:  139 loss of agent  2 :  {0: tensor(0.1302, grad_fn=<MseLossBackward0>), 1: tensor(0.1731, grad_fn=<MseLossBackward0>), 2: tensor(0.1389, grad_fn=<MseLossBackward0>)}
0.9995426666666667
episode:  0 training step:  140 loss of agent  0 :  {0: tensor(0.1473, grad_fn=<MseLossBackward0>), 1: tensor(0.1731, grad_fn=<MseLossBackward0>), 2: tensor(0.1389, grad_fn=<MseLossBackward0>)}
0.9995426666666667
episode:  0 training step:  140 loss of agent  1 :  {0: tensor(0.1473, grad_fn=<MseLossBackward0>), 1: tensor(0.3013, grad_fn=<MseLossBackward0>), 2: tensor(0.1389, grad_fn=<MseLossBackward0>)}
0.9995426666666667
episode:  0 training step:  140 loss of agent  2 :  {0: tensor(0.1473, grad_fn=<MseLossBackward0>), 1: tensor(0.3013, grad_fn=<MseLossBackward0>), 2: tensor(0.1660, grad_fn=<MseLossBackward0>)}
0.9995394
episode:  0 training step:  141 loss of agent  0 :  {0: tensor(1.4964, grad_fn=<MseLossBackward0>), 1: tensor(0.3013, grad_fn=<MseLossBackward0>), 2: tensor(0.1660, grad_fn=<MseLossBackward0>)}
0.9995394
episode:  0 training step:  141 loss of agent  1 :  {0: tensor(1.4964, grad_fn=<MseLossBackward0>), 1: tensor(0.9575, grad_fn=<MseLossBackward0>), 2: tensor(0.1660, grad_fn=<MseLossBackward0>)}
0.9995394
episode:  0 training step:  141 loss of agent  2 :  {0: tensor(1.4964, grad_fn=<MseLossBackward0>), 1: tensor(0.9575, grad_fn=<MseLossBackward0>), 2: tensor(0.7737, grad_fn=<MseLossBackward0>)}
0.9995361333333334
episode:  0 training step:  142 loss of agent  0 :  {0: tensor(1.3948, grad_fn=<MseLossBackward0>), 1: tensor(0.9575, grad_fn=<MseLossBackward0>), 2: tensor(0.7737, grad_fn=<MseLossBackward0>)}
0.9995361333333334
episode:  0 training step:  142 loss of agent  1 :  {0: tensor(1.3948, grad_fn=<MseLossBackward0>), 1: tensor(0.7793, grad_fn=<MseLossBackward0>), 2: tensor(0.7737, grad_fn=<MseLossBackward0>)}
0.9995361333333334
episode:  0 training step:  142 loss of agent  2 :  {0: tensor(1.3948, grad_fn=<MseLossBackward0>), 1: tensor(0.7793, grad_fn=<MseLossBackward0>), 2: tensor(0.6893, grad_fn=<MseLossBackward0>)}
0.9995328666666666
episode:  0 training step:  143 loss of agent  0 :  {0: tensor(0.8796, grad_fn=<MseLossBackward0>), 1: tensor(0.7793, grad_fn=<MseLossBackward0>), 2: tensor(0.6893, grad_fn=<MseLossBackward0>)}
0.9995328666666666
episode:  0 training step:  143 loss of agent  1 :  {0: tensor(0.8796, grad_fn=<MseLossBackward0>), 1: tensor(0.6519, grad_fn=<MseLossBackward0>), 2: tensor(0.6893, grad_fn=<MseLossBackward0>)}
0.9995328666666666
episode:  0 training step:  143 loss of agent  2 :  {0: tensor(0.8796, grad_fn=<MseLossBackward0>), 1: tensor(0.6519, grad_fn=<MseLossBackward0>), 2: tensor(0.5186, grad_fn=<MseLossBackward0>)}
0.9995296
episode:  0 training step:  144 loss of agent  0 :  {0: tensor(0.8615, grad_fn=<MseLossBackward0>), 1: tensor(0.6519, grad_fn=<MseLossBackward0>), 2: tensor(0.5186, grad_fn=<MseLossBackward0>)}
0.9995296
episode:  0 training step:  144 loss of agent  1 :  {0: tensor(0.8615, grad_fn=<MseLossBackward0>), 1: tensor(0.4372, grad_fn=<MseLossBackward0>), 2: tensor(0.5186, grad_fn=<MseLossBackward0>)}
0.9995296
episode:  0 training step:  144 loss of agent  2 :  {0: tensor(0.8615, grad_fn=<MseLossBackward0>), 1: tensor(0.4372, grad_fn=<MseLossBackward0>), 2: tensor(0.3588, grad_fn=<MseLossBackward0>)}
0.9995263333333333
episode:  0 training step:  145 loss of agent  0 :  {0: tensor(0.6146, grad_fn=<MseLossBackward0>), 1: tensor(0.4372, grad_fn=<MseLossBackward0>), 2: tensor(0.3588, grad_fn=<MseLossBackward0>)}
0.9995263333333333
episode:  0 training step:  145 loss of agent  1 :  {0: tensor(0.6146, grad_fn=<MseLossBackward0>), 1: tensor(0.2041, grad_fn=<MseLossBackward0>), 2: tensor(0.3588, grad_fn=<MseLossBackward0>)}
0.9995263333333333
episode:  0 training step:  145 loss of agent  2 :  {0: tensor(0.6146, grad_fn=<MseLossBackward0>), 1: tensor(0.2041, grad_fn=<MseLossBackward0>), 2: tensor(0.2008, grad_fn=<MseLossBackward0>)}
0.9995230666666667
episode:  0 training step:  146 loss of agent  0 :  {0: tensor(0.4561, grad_fn=<MseLossBackward0>), 1: tensor(0.2041, grad_fn=<MseLossBackward0>), 2: tensor(0.2008, grad_fn=<MseLossBackward0>)}
0.9995230666666667
episode:  0 training step:  146 loss of agent  1 :  {0: tensor(0.4561, grad_fn=<MseLossBackward0>), 1: tensor(0.1839, grad_fn=<MseLossBackward0>), 2: tensor(0.2008, grad_fn=<MseLossBackward0>)}
0.9995230666666667
episode:  0 training step:  146 loss of agent  2 :  {0: tensor(0.4561, grad_fn=<MseLossBackward0>), 1: tensor(0.1839, grad_fn=<MseLossBackward0>), 2: tensor(0.1389, grad_fn=<MseLossBackward0>)}
0.9995198
episode:  0 training step:  147 loss of agent  0 :  {0: tensor(0.1532, grad_fn=<MseLossBackward0>), 1: tensor(0.1839, grad_fn=<MseLossBackward0>), 2: tensor(0.1389, grad_fn=<MseLossBackward0>)}
0.9995198
episode:  0 training step:  147 loss of agent  1 :  {0: tensor(0.1532, grad_fn=<MseLossBackward0>), 1: tensor(0.1582, grad_fn=<MseLossBackward0>), 2: tensor(0.1389, grad_fn=<MseLossBackward0>)}
0.9995198
episode:  0 training step:  147 loss of agent  2 :  {0: tensor(0.1532, grad_fn=<MseLossBackward0>), 1: tensor(0.1582, grad_fn=<MseLossBackward0>), 2: tensor(0.1869, grad_fn=<MseLossBackward0>)}
0.9995165333333333
episode:  0 training step:  148 loss of agent  0 :  {0: tensor(0.1531, grad_fn=<MseLossBackward0>), 1: tensor(0.1582, grad_fn=<MseLossBackward0>), 2: tensor(0.1869, grad_fn=<MseLossBackward0>)}
0.9995165333333333
episode:  0 training step:  148 loss of agent  1 :  {0: tensor(0.1531, grad_fn=<MseLossBackward0>), 1: tensor(0.1616, grad_fn=<MseLossBackward0>), 2: tensor(0.1869, grad_fn=<MseLossBackward0>)}
0.9995165333333333
episode:  0 training step:  148 loss of agent  2 :  {0: tensor(0.1531, grad_fn=<MseLossBackward0>), 1: tensor(0.1616, grad_fn=<MseLossBackward0>), 2: tensor(0.1838, grad_fn=<MseLossBackward0>)}
0.9995132666666666
episode:  0 training step:  149 loss of agent  0 :  {0: tensor(0.1765, grad_fn=<MseLossBackward0>), 1: tensor(0.1616, grad_fn=<MseLossBackward0>), 2: tensor(0.1838, grad_fn=<MseLossBackward0>)}
0.9995132666666666
episode:  0 training step:  149 loss of agent  1 :  {0: tensor(0.1765, grad_fn=<MseLossBackward0>), 1: tensor(0.1761, grad_fn=<MseLossBackward0>), 2: tensor(0.1838, grad_fn=<MseLossBackward0>)}
0.9995132666666666
episode:  0 training step:  149 loss of agent  2 :  {0: tensor(0.1765, grad_fn=<MseLossBackward0>), 1: tensor(0.1761, grad_fn=<MseLossBackward0>), 2: tensor(0.2106, grad_fn=<MseLossBackward0>)}
0.99951
episode:  0 training step:  150 loss of agent  0 :  {0: tensor(0.1939, grad_fn=<MseLossBackward0>), 1: tensor(0.1761, grad_fn=<MseLossBackward0>), 2: tensor(0.2106, grad_fn=<MseLossBackward0>)}
0.99951
episode:  0 training step:  150 loss of agent  1 :  {0: tensor(0.1939, grad_fn=<MseLossBackward0>), 1: tensor(0.2474, grad_fn=<MseLossBackward0>), 2: tensor(0.2106, grad_fn=<MseLossBackward0>)}
0.99951
episode:  0 training step:  150 loss of agent  2 :  {0: tensor(0.1939, grad_fn=<MseLossBackward0>), 1: tensor(0.2474, grad_fn=<MseLossBackward0>), 2: tensor(0.2627, grad_fn=<MseLossBackward0>)}
episode 0 terminated at 100
episode: 1
0.9995067333333333
episode:  1 training step:  151 loss of agent  0 :  {0: tensor(1.7432, grad_fn=<MseLossBackward0>), 1: tensor(0.2474, grad_fn=<MseLossBackward0>), 2: tensor(0.2627, grad_fn=<MseLossBackward0>)}
0.9995067333333333
episode:  1 training step:  151 loss of agent  1 :  {0: tensor(1.7432, grad_fn=<MseLossBackward0>), 1: tensor(1.0357, grad_fn=<MseLossBackward0>), 2: tensor(0.2627, grad_fn=<MseLossBackward0>)}
0.9995067333333333
episode:  1 training step:  151 loss of agent  2 :  {0: tensor(1.7432, grad_fn=<MseLossBackward0>), 1: tensor(1.0357, grad_fn=<MseLossBackward0>), 2: tensor(1.1997, grad_fn=<MseLossBackward0>)}
0.9995034666666667
episode:  1 training step:  152 loss of agent  0 :  {0: tensor(1.4128, grad_fn=<MseLossBackward0>), 1: tensor(1.0357, grad_fn=<MseLossBackward0>), 2: tensor(1.1997, grad_fn=<MseLossBackward0>)}
0.9995034666666667
episode:  1 training step:  152 loss of agent  1 :  {0: tensor(1.4128, grad_fn=<MseLossBackward0>), 1: tensor(0.7694, grad_fn=<MseLossBackward0>), 2: tensor(1.1997, grad_fn=<MseLossBackward0>)}
0.9995034666666667
episode:  1 training step:  152 loss of agent  2 :  {0: tensor(1.4128, grad_fn=<MseLossBackward0>), 1: tensor(0.7694, grad_fn=<MseLossBackward0>), 2: tensor(1.0579, grad_fn=<MseLossBackward0>)}
0.9995002
episode:  1 training step:  153 loss of agent  0 :  {0: tensor(1.1328, grad_fn=<MseLossBackward0>), 1: tensor(0.7694, grad_fn=<MseLossBackward0>), 2: tensor(1.0579, grad_fn=<MseLossBackward0>)}
0.9995002
episode:  1 training step:  153 loss of agent  1 :  {0: tensor(1.1328, grad_fn=<MseLossBackward0>), 1: tensor(0.6161, grad_fn=<MseLossBackward0>), 2: tensor(1.0579, grad_fn=<MseLossBackward0>)}
0.9995002
episode:  1 training step:  153 loss of agent  2 :  {0: tensor(1.1328, grad_fn=<MseLossBackward0>), 1: tensor(0.6161, grad_fn=<MseLossBackward0>), 2: tensor(0.7277, grad_fn=<MseLossBackward0>)}
0.9994969333333333
episode:  1 training step:  154 loss of agent  0 :  {0: tensor(0.8057, grad_fn=<MseLossBackward0>), 1: tensor(0.6161, grad_fn=<MseLossBackward0>), 2: tensor(0.7277, grad_fn=<MseLossBackward0>)}
0.9994969333333333
episode:  1 training step:  154 loss of agent  1 :  {0: tensor(0.8057, grad_fn=<MseLossBackward0>), 1: tensor(0.3817, grad_fn=<MseLossBackward0>), 2: tensor(0.7277, grad_fn=<MseLossBackward0>)}
0.9994969333333333
episode:  1 training step:  154 loss of agent  2 :  {0: tensor(0.8057, grad_fn=<MseLossBackward0>), 1: tensor(0.3817, grad_fn=<MseLossBackward0>), 2: tensor(0.4400, grad_fn=<MseLossBackward0>)}
0.9994936666666666
episode:  1 training step:  155 loss of agent  0 :  {0: tensor(0.4952, grad_fn=<MseLossBackward0>), 1: tensor(0.3817, grad_fn=<MseLossBackward0>), 2: tensor(0.4400, grad_fn=<MseLossBackward0>)}
0.9994936666666666
episode:  1 training step:  155 loss of agent  1 :  {0: tensor(0.4952, grad_fn=<MseLossBackward0>), 1: tensor(0.2398, grad_fn=<MseLossBackward0>), 2: tensor(0.4400, grad_fn=<MseLossBackward0>)}
0.9994936666666666
episode:  1 training step:  155 loss of agent  2 :  {0: tensor(0.4952, grad_fn=<MseLossBackward0>), 1: tensor(0.2398, grad_fn=<MseLossBackward0>), 2: tensor(0.2561, grad_fn=<MseLossBackward0>)}
0.9994904
episode:  1 training step:  156 loss of agent  0 :  {0: tensor(0.2720, grad_fn=<MseLossBackward0>), 1: tensor(0.2398, grad_fn=<MseLossBackward0>), 2: tensor(0.2561, grad_fn=<MseLossBackward0>)}
0.9994904
episode:  1 training step:  156 loss of agent  1 :  {0: tensor(0.2720, grad_fn=<MseLossBackward0>), 1: tensor(0.1567, grad_fn=<MseLossBackward0>), 2: tensor(0.2561, grad_fn=<MseLossBackward0>)}
0.9994904
episode:  1 training step:  156 loss of agent  2 :  {0: tensor(0.2720, grad_fn=<MseLossBackward0>), 1: tensor(0.1567, grad_fn=<MseLossBackward0>), 2: tensor(0.1959, grad_fn=<MseLossBackward0>)}
0.9994871333333334
episode:  1 training step:  157 loss of agent  0 :  {0: tensor(0.1751, grad_fn=<MseLossBackward0>), 1: tensor(0.1567, grad_fn=<MseLossBackward0>), 2: tensor(0.1959, grad_fn=<MseLossBackward0>)}
0.9994871333333334
episode:  1 training step:  157 loss of agent  1 :  {0: tensor(0.1751, grad_fn=<MseLossBackward0>), 1: tensor(0.1984, grad_fn=<MseLossBackward0>), 2: tensor(0.1959, grad_fn=<MseLossBackward0>)}
0.9994871333333334
episode:  1 training step:  157 loss of agent  2 :  {0: tensor(0.1751, grad_fn=<MseLossBackward0>), 1: tensor(0.1984, grad_fn=<MseLossBackward0>), 2: tensor(0.2115, grad_fn=<MseLossBackward0>)}
0.9994838666666667
episode:  1 training step:  158 loss of agent  0 :  {0: tensor(0.1593, grad_fn=<MseLossBackward0>), 1: tensor(0.1984, grad_fn=<MseLossBackward0>), 2: tensor(0.2115, grad_fn=<MseLossBackward0>)}
0.9994838666666667
episode:  1 training step:  158 loss of agent  1 :  {0: tensor(0.1593, grad_fn=<MseLossBackward0>), 1: tensor(0.1252, grad_fn=<MseLossBackward0>), 2: tensor(0.2115, grad_fn=<MseLossBackward0>)}
0.9994838666666667
episode:  1 training step:  158 loss of agent  2 :  {0: tensor(0.1593, grad_fn=<MseLossBackward0>), 1: tensor(0.1252, grad_fn=<MseLossBackward0>), 2: tensor(0.3607, grad_fn=<MseLossBackward0>)}
0.9994806
episode:  1 training step:  159 loss of agent  0 :  {0: tensor(0.1450, grad_fn=<MseLossBackward0>), 1: tensor(0.1252, grad_fn=<MseLossBackward0>), 2: tensor(0.3607, grad_fn=<MseLossBackward0>)}
0.9994806
episode:  1 training step:  159 loss of agent  1 :  {0: tensor(0.1450, grad_fn=<MseLossBackward0>), 1: tensor(0.2109, grad_fn=<MseLossBackward0>), 2: tensor(0.3607, grad_fn=<MseLossBackward0>)}
0.9994806
episode:  1 training step:  159 loss of agent  2 :  {0: tensor(0.1450, grad_fn=<MseLossBackward0>), 1: tensor(0.2109, grad_fn=<MseLossBackward0>), 2: tensor(0.3445, grad_fn=<MseLossBackward0>)}
0.9994773333333333
episode:  1 training step:  160 loss of agent  0 :  {0: tensor(0.2287, grad_fn=<MseLossBackward0>), 1: tensor(0.2109, grad_fn=<MseLossBackward0>), 2: tensor(0.3445, grad_fn=<MseLossBackward0>)}
0.9994773333333333
episode:  1 training step:  160 loss of agent  1 :  {0: tensor(0.2287, grad_fn=<MseLossBackward0>), 1: tensor(0.2526, grad_fn=<MseLossBackward0>), 2: tensor(0.3445, grad_fn=<MseLossBackward0>)}
0.9994773333333333
episode:  1 training step:  160 loss of agent  2 :  {0: tensor(0.2287, grad_fn=<MseLossBackward0>), 1: tensor(0.2526, grad_fn=<MseLossBackward0>), 2: tensor(0.5262, grad_fn=<MseLossBackward0>)}
0.9994740666666667
episode:  1 training step:  161 loss of agent  0 :  {0: tensor(1.9710, grad_fn=<MseLossBackward0>), 1: tensor(0.2526, grad_fn=<MseLossBackward0>), 2: tensor(0.5262, grad_fn=<MseLossBackward0>)}
0.9994740666666667
episode:  1 training step:  161 loss of agent  1 :  {0: tensor(1.9710, grad_fn=<MseLossBackward0>), 1: tensor(1.0272, grad_fn=<MseLossBackward0>), 2: tensor(0.5262, grad_fn=<MseLossBackward0>)}
0.9994740666666667
episode:  1 training step:  161 loss of agent  2 :  {0: tensor(1.9710, grad_fn=<MseLossBackward0>), 1: tensor(1.0272, grad_fn=<MseLossBackward0>), 2: tensor(1.6513, grad_fn=<MseLossBackward0>)}
0.9994708
episode:  1 training step:  162 loss of agent  0 :  {0: tensor(1.5821, grad_fn=<MseLossBackward0>), 1: tensor(1.0272, grad_fn=<MseLossBackward0>), 2: tensor(1.6513, grad_fn=<MseLossBackward0>)}
0.9994708
episode:  1 training step:  162 loss of agent  1 :  {0: tensor(1.5821, grad_fn=<MseLossBackward0>), 1: tensor(0.9292, grad_fn=<MseLossBackward0>), 2: tensor(1.6513, grad_fn=<MseLossBackward0>)}
0.9994708
episode:  1 training step:  162 loss of agent  2 :  {0: tensor(1.5821, grad_fn=<MseLossBackward0>), 1: tensor(0.9292, grad_fn=<MseLossBackward0>), 2: tensor(1.1272, grad_fn=<MseLossBackward0>)}
0.9994675333333334
episode:  1 training step:  163 loss of agent  0 :  {0: tensor(1.2380, grad_fn=<MseLossBackward0>), 1: tensor(0.9292, grad_fn=<MseLossBackward0>), 2: tensor(1.1272, grad_fn=<MseLossBackward0>)}
0.9994675333333334
episode:  1 training step:  163 loss of agent  1 :  {0: tensor(1.2380, grad_fn=<MseLossBackward0>), 1: tensor(0.5574, grad_fn=<MseLossBackward0>), 2: tensor(1.1272, grad_fn=<MseLossBackward0>)}
0.9994675333333334
episode:  1 training step:  163 loss of agent  2 :  {0: tensor(1.2380, grad_fn=<MseLossBackward0>), 1: tensor(0.5574, grad_fn=<MseLossBackward0>), 2: tensor(0.8004, grad_fn=<MseLossBackward0>)}
0.9994642666666667
episode:  1 training step:  164 loss of agent  0 :  {0: tensor(0.7205, grad_fn=<MseLossBackward0>), 1: tensor(0.5574, grad_fn=<MseLossBackward0>), 2: tensor(0.8004, grad_fn=<MseLossBackward0>)}
0.9994642666666667
episode:  1 training step:  164 loss of agent  1 :  {0: tensor(0.7205, grad_fn=<MseLossBackward0>), 1: tensor(0.4840, grad_fn=<MseLossBackward0>), 2: tensor(0.8004, grad_fn=<MseLossBackward0>)}
0.9994642666666667
episode:  1 training step:  164 loss of agent  2 :  {0: tensor(0.7205, grad_fn=<MseLossBackward0>), 1: tensor(0.4840, grad_fn=<MseLossBackward0>), 2: tensor(0.5273, grad_fn=<MseLossBackward0>)}
0.999461
episode:  1 training step:  165 loss of agent  0 :  {0: tensor(0.5011, grad_fn=<MseLossBackward0>), 1: tensor(0.4840, grad_fn=<MseLossBackward0>), 2: tensor(0.5273, grad_fn=<MseLossBackward0>)}
0.999461
episode:  1 training step:  165 loss of agent  1 :  {0: tensor(0.5011, grad_fn=<MseLossBackward0>), 1: tensor(0.3387, grad_fn=<MseLossBackward0>), 2: tensor(0.5273, grad_fn=<MseLossBackward0>)}
0.999461
episode:  1 training step:  165 loss of agent  2 :  {0: tensor(0.5011, grad_fn=<MseLossBackward0>), 1: tensor(0.3387, grad_fn=<MseLossBackward0>), 2: tensor(0.3611, grad_fn=<MseLossBackward0>)}
0.9994577333333333
episode:  1 training step:  166 loss of agent  0 :  {0: tensor(0.2233, grad_fn=<MseLossBackward0>), 1: tensor(0.3387, grad_fn=<MseLossBackward0>), 2: tensor(0.3611, grad_fn=<MseLossBackward0>)}
0.9994577333333333
episode:  1 training step:  166 loss of agent  1 :  {0: tensor(0.2233, grad_fn=<MseLossBackward0>), 1: tensor(0.2454, grad_fn=<MseLossBackward0>), 2: tensor(0.3611, grad_fn=<MseLossBackward0>)}
0.9994577333333333
episode:  1 training step:  166 loss of agent  2 :  {0: tensor(0.2233, grad_fn=<MseLossBackward0>), 1: tensor(0.2454, grad_fn=<MseLossBackward0>), 2: tensor(0.2925, grad_fn=<MseLossBackward0>)}
0.9994544666666667
episode:  1 training step:  167 loss of agent  0 :  {0: tensor(0.2168, grad_fn=<MseLossBackward0>), 1: tensor(0.2454, grad_fn=<MseLossBackward0>), 2: tensor(0.2925, grad_fn=<MseLossBackward0>)}
0.9994544666666667
episode:  1 training step:  167 loss of agent  1 :  {0: tensor(0.2168, grad_fn=<MseLossBackward0>), 1: tensor(0.1927, grad_fn=<MseLossBackward0>), 2: tensor(0.2925, grad_fn=<MseLossBackward0>)}
0.9994544666666667
episode:  1 training step:  167 loss of agent  2 :  {0: tensor(0.2168, grad_fn=<MseLossBackward0>), 1: tensor(0.1927, grad_fn=<MseLossBackward0>), 2: tensor(0.1624, grad_fn=<MseLossBackward0>)}
0.9994512
episode:  1 training step:  168 loss of agent  0 :  {0: tensor(0.1551, grad_fn=<MseLossBackward0>), 1: tensor(0.1927, grad_fn=<MseLossBackward0>), 2: tensor(0.1624, grad_fn=<MseLossBackward0>)}
0.9994512
episode:  1 training step:  168 loss of agent  1 :  {0: tensor(0.1551, grad_fn=<MseLossBackward0>), 1: tensor(0.2102, grad_fn=<MseLossBackward0>), 2: tensor(0.1624, grad_fn=<MseLossBackward0>)}
0.9994512
episode:  1 training step:  168 loss of agent  2 :  {0: tensor(0.1551, grad_fn=<MseLossBackward0>), 1: tensor(0.2102, grad_fn=<MseLossBackward0>), 2: tensor(0.2432, grad_fn=<MseLossBackward0>)}
0.9994479333333334
episode:  1 training step:  169 loss of agent  0 :  {0: tensor(0.2085, grad_fn=<MseLossBackward0>), 1: tensor(0.2102, grad_fn=<MseLossBackward0>), 2: tensor(0.2432, grad_fn=<MseLossBackward0>)}
0.9994479333333334
episode:  1 training step:  169 loss of agent  1 :  {0: tensor(0.2085, grad_fn=<MseLossBackward0>), 1: tensor(0.2087, grad_fn=<MseLossBackward0>), 2: tensor(0.2432, grad_fn=<MseLossBackward0>)}
0.9994479333333334
episode:  1 training step:  169 loss of agent  2 :  {0: tensor(0.2085, grad_fn=<MseLossBackward0>), 1: tensor(0.2087, grad_fn=<MseLossBackward0>), 2: tensor(0.3131, grad_fn=<MseLossBackward0>)}
0.9994446666666666
episode:  1 training step:  170 loss of agent  0 :  {0: tensor(0.3030, grad_fn=<MseLossBackward0>), 1: tensor(0.2087, grad_fn=<MseLossBackward0>), 2: tensor(0.3131, grad_fn=<MseLossBackward0>)}
0.9994446666666666
episode:  1 training step:  170 loss of agent  1 :  {0: tensor(0.3030, grad_fn=<MseLossBackward0>), 1: tensor(0.2928, grad_fn=<MseLossBackward0>), 2: tensor(0.3131, grad_fn=<MseLossBackward0>)}
0.9994446666666666
episode:  1 training step:  170 loss of agent  2 :  {0: tensor(0.3030, grad_fn=<MseLossBackward0>), 1: tensor(0.2928, grad_fn=<MseLossBackward0>), 2: tensor(0.3194, grad_fn=<MseLossBackward0>)}
0.9994414
episode:  1 training step:  171 loss of agent  0 :  {0: tensor(1.8216, grad_fn=<MseLossBackward0>), 1: tensor(0.2928, grad_fn=<MseLossBackward0>), 2: tensor(0.3194, grad_fn=<MseLossBackward0>)}
0.9994414
episode:  1 training step:  171 loss of agent  1 :  {0: tensor(1.8216, grad_fn=<MseLossBackward0>), 1: tensor(0.9573, grad_fn=<MseLossBackward0>), 2: tensor(0.3194, grad_fn=<MseLossBackward0>)}
0.9994414
episode:  1 training step:  171 loss of agent  2 :  {0: tensor(1.8216, grad_fn=<MseLossBackward0>), 1: tensor(0.9573, grad_fn=<MseLossBackward0>), 2: tensor(1.3407, grad_fn=<MseLossBackward0>)}
0.9994381333333333
episode:  1 training step:  172 loss of agent  0 :  {0: tensor(1.2364, grad_fn=<MseLossBackward0>), 1: tensor(0.9573, grad_fn=<MseLossBackward0>), 2: tensor(1.3407, grad_fn=<MseLossBackward0>)}
0.9994381333333333
episode:  1 training step:  172 loss of agent  1 :  {0: tensor(1.2364, grad_fn=<MseLossBackward0>), 1: tensor(0.6452, grad_fn=<MseLossBackward0>), 2: tensor(1.3407, grad_fn=<MseLossBackward0>)}
0.9994381333333333
episode:  1 training step:  172 loss of agent  2 :  {0: tensor(1.2364, grad_fn=<MseLossBackward0>), 1: tensor(0.6452, grad_fn=<MseLossBackward0>), 2: tensor(1.1762, grad_fn=<MseLossBackward0>)}
0.9994348666666667
episode:  1 training step:  173 loss of agent  0 :  {0: tensor(0.9911, grad_fn=<MseLossBackward0>), 1: tensor(0.6452, grad_fn=<MseLossBackward0>), 2: tensor(1.1762, grad_fn=<MseLossBackward0>)}
0.9994348666666667
episode:  1 training step:  173 loss of agent  1 :  {0: tensor(0.9911, grad_fn=<MseLossBackward0>), 1: tensor(0.6349, grad_fn=<MseLossBackward0>), 2: tensor(1.1762, grad_fn=<MseLossBackward0>)}
0.9994348666666667
episode:  1 training step:  173 loss of agent  2 :  {0: tensor(0.9911, grad_fn=<MseLossBackward0>), 1: tensor(0.6349, grad_fn=<MseLossBackward0>), 2: tensor(0.8431, grad_fn=<MseLossBackward0>)}
0.9994316
episode:  1 training step:  174 loss of agent  0 :  {0: tensor(0.7444, grad_fn=<MseLossBackward0>), 1: tensor(0.6349, grad_fn=<MseLossBackward0>), 2: tensor(0.8431, grad_fn=<MseLossBackward0>)}
0.9994316
episode:  1 training step:  174 loss of agent  1 :  {0: tensor(0.7444, grad_fn=<MseLossBackward0>), 1: tensor(0.3758, grad_fn=<MseLossBackward0>), 2: tensor(0.8431, grad_fn=<MseLossBackward0>)}
0.9994316
episode:  1 training step:  174 loss of agent  2 :  {0: tensor(0.7444, grad_fn=<MseLossBackward0>), 1: tensor(0.3758, grad_fn=<MseLossBackward0>), 2: tensor(0.5057, grad_fn=<MseLossBackward0>)}
0.9994283333333334
episode:  1 training step:  175 loss of agent  0 :  {0: tensor(0.3223, grad_fn=<MseLossBackward0>), 1: tensor(0.3758, grad_fn=<MseLossBackward0>), 2: tensor(0.5057, grad_fn=<MseLossBackward0>)}
0.9994283333333334
episode:  1 training step:  175 loss of agent  1 :  {0: tensor(0.3223, grad_fn=<MseLossBackward0>), 1: tensor(0.2586, grad_fn=<MseLossBackward0>), 2: tensor(0.5057, grad_fn=<MseLossBackward0>)}
0.9994283333333334
episode:  1 training step:  175 loss of agent  2 :  {0: tensor(0.3223, grad_fn=<MseLossBackward0>), 1: tensor(0.2586, grad_fn=<MseLossBackward0>), 2: tensor(0.3715, grad_fn=<MseLossBackward0>)}
0.9994250666666666
episode:  1 training step:  176 loss of agent  0 :  {0: tensor(0.3019, grad_fn=<MseLossBackward0>), 1: tensor(0.2586, grad_fn=<MseLossBackward0>), 2: tensor(0.3715, grad_fn=<MseLossBackward0>)}
0.9994250666666666
episode:  1 training step:  176 loss of agent  1 :  {0: tensor(0.3019, grad_fn=<MseLossBackward0>), 1: tensor(0.1740, grad_fn=<MseLossBackward0>), 2: tensor(0.3715, grad_fn=<MseLossBackward0>)}
0.9994250666666666
episode:  1 training step:  176 loss of agent  2 :  {0: tensor(0.3019, grad_fn=<MseLossBackward0>), 1: tensor(0.1740, grad_fn=<MseLossBackward0>), 2: tensor(0.1708, grad_fn=<MseLossBackward0>)}
0.9994218
episode:  1 training step:  177 loss of agent  0 :  {0: tensor(0.2071, grad_fn=<MseLossBackward0>), 1: tensor(0.1740, grad_fn=<MseLossBackward0>), 2: tensor(0.1708, grad_fn=<MseLossBackward0>)}
0.9994218
episode:  1 training step:  177 loss of agent  1 :  {0: tensor(0.2071, grad_fn=<MseLossBackward0>), 1: tensor(0.1369, grad_fn=<MseLossBackward0>), 2: tensor(0.1708, grad_fn=<MseLossBackward0>)}
0.9994218
episode:  1 training step:  177 loss of agent  2 :  {0: tensor(0.2071, grad_fn=<MseLossBackward0>), 1: tensor(0.1369, grad_fn=<MseLossBackward0>), 2: tensor(0.2052, grad_fn=<MseLossBackward0>)}
0.9994185333333333
episode:  1 training step:  178 loss of agent  0 :  {0: tensor(0.2324, grad_fn=<MseLossBackward0>), 1: tensor(0.1369, grad_fn=<MseLossBackward0>), 2: tensor(0.2052, grad_fn=<MseLossBackward0>)}
0.9994185333333333
episode:  1 training step:  178 loss of agent  1 :  {0: tensor(0.2324, grad_fn=<MseLossBackward0>), 1: tensor(0.2292, grad_fn=<MseLossBackward0>), 2: tensor(0.2052, grad_fn=<MseLossBackward0>)}
0.9994185333333333
episode:  1 training step:  178 loss of agent  2 :  {0: tensor(0.2324, grad_fn=<MseLossBackward0>), 1: tensor(0.2292, grad_fn=<MseLossBackward0>), 2: tensor(0.3424, grad_fn=<MseLossBackward0>)}
0.9994152666666667
episode:  1 training step:  179 loss of agent  0 :  {0: tensor(0.2785, grad_fn=<MseLossBackward0>), 1: tensor(0.2292, grad_fn=<MseLossBackward0>), 2: tensor(0.3424, grad_fn=<MseLossBackward0>)}
0.9994152666666667
episode:  1 training step:  179 loss of agent  1 :  {0: tensor(0.2785, grad_fn=<MseLossBackward0>), 1: tensor(0.1617, grad_fn=<MseLossBackward0>), 2: tensor(0.3424, grad_fn=<MseLossBackward0>)}
0.9994152666666667
episode:  1 training step:  179 loss of agent  2 :  {0: tensor(0.2785, grad_fn=<MseLossBackward0>), 1: tensor(0.1617, grad_fn=<MseLossBackward0>), 2: tensor(0.5356, grad_fn=<MseLossBackward0>)}
0.999412
episode:  1 training step:  180 loss of agent  0 :  {0: tensor(0.3335, grad_fn=<MseLossBackward0>), 1: tensor(0.1617, grad_fn=<MseLossBackward0>), 2: tensor(0.5356, grad_fn=<MseLossBackward0>)}
0.999412
episode:  1 training step:  180 loss of agent  1 :  {0: tensor(0.3335, grad_fn=<MseLossBackward0>), 1: tensor(0.2481, grad_fn=<MseLossBackward0>), 2: tensor(0.5356, grad_fn=<MseLossBackward0>)}
0.999412
episode:  1 training step:  180 loss of agent  2 :  {0: tensor(0.3335, grad_fn=<MseLossBackward0>), 1: tensor(0.2481, grad_fn=<MseLossBackward0>), 2: tensor(0.3747, grad_fn=<MseLossBackward0>)}
0.9994087333333334
episode:  1 training step:  181 loss of agent  0 :  {0: tensor(1.5762, grad_fn=<MseLossBackward0>), 1: tensor(0.2481, grad_fn=<MseLossBackward0>), 2: tensor(0.3747, grad_fn=<MseLossBackward0>)}
0.9994087333333334
episode:  1 training step:  181 loss of agent  1 :  {0: tensor(1.5762, grad_fn=<MseLossBackward0>), 1: tensor(1.1043, grad_fn=<MseLossBackward0>), 2: tensor(0.3747, grad_fn=<MseLossBackward0>)}
0.9994087333333334
episode:  1 training step:  181 loss of agent  2 :  {0: tensor(1.5762, grad_fn=<MseLossBackward0>), 1: tensor(1.1043, grad_fn=<MseLossBackward0>), 2: tensor(1.3021, grad_fn=<MseLossBackward0>)}
0.9994054666666666
episode:  1 training step:  182 loss of agent  0 :  {0: tensor(1.4307, grad_fn=<MseLossBackward0>), 1: tensor(1.1043, grad_fn=<MseLossBackward0>), 2: tensor(1.3021, grad_fn=<MseLossBackward0>)}
0.9994054666666666
episode:  1 training step:  182 loss of agent  1 :  {0: tensor(1.4307, grad_fn=<MseLossBackward0>), 1: tensor(0.7819, grad_fn=<MseLossBackward0>), 2: tensor(1.3021, grad_fn=<MseLossBackward0>)}
0.9994054666666666
episode:  1 training step:  182 loss of agent  2 :  {0: tensor(1.4307, grad_fn=<MseLossBackward0>), 1: tensor(0.7819, grad_fn=<MseLossBackward0>), 2: tensor(0.8258, grad_fn=<MseLossBackward0>)}
0.9994022
episode:  1 training step:  183 loss of agent  0 :  {0: tensor(0.9948, grad_fn=<MseLossBackward0>), 1: tensor(0.7819, grad_fn=<MseLossBackward0>), 2: tensor(0.8258, grad_fn=<MseLossBackward0>)}
0.9994022
episode:  1 training step:  183 loss of agent  1 :  {0: tensor(0.9948, grad_fn=<MseLossBackward0>), 1: tensor(0.6050, grad_fn=<MseLossBackward0>), 2: tensor(0.8258, grad_fn=<MseLossBackward0>)}
0.9994022
episode:  1 training step:  183 loss of agent  2 :  {0: tensor(0.9948, grad_fn=<MseLossBackward0>), 1: tensor(0.6050, grad_fn=<MseLossBackward0>), 2: tensor(1.0651, grad_fn=<MseLossBackward0>)}
0.9993989333333333
episode:  1 training step:  184 loss of agent  0 :  {0: tensor(0.7680, grad_fn=<MseLossBackward0>), 1: tensor(0.6050, grad_fn=<MseLossBackward0>), 2: tensor(1.0651, grad_fn=<MseLossBackward0>)}
0.9993989333333333
episode:  1 training step:  184 loss of agent  1 :  {0: tensor(0.7680, grad_fn=<MseLossBackward0>), 1: tensor(0.5232, grad_fn=<MseLossBackward0>), 2: tensor(1.0651, grad_fn=<MseLossBackward0>)}
0.9993989333333333
episode:  1 training step:  184 loss of agent  2 :  {0: tensor(0.7680, grad_fn=<MseLossBackward0>), 1: tensor(0.5232, grad_fn=<MseLossBackward0>), 2: tensor(0.3730, grad_fn=<MseLossBackward0>)}
0.9993956666666667
episode:  1 training step:  185 loss of agent  0 :  {0: tensor(0.5823, grad_fn=<MseLossBackward0>), 1: tensor(0.5232, grad_fn=<MseLossBackward0>), 2: tensor(0.3730, grad_fn=<MseLossBackward0>)}
0.9993956666666667
episode:  1 training step:  185 loss of agent  1 :  {0: tensor(0.5823, grad_fn=<MseLossBackward0>), 1: tensor(0.3152, grad_fn=<MseLossBackward0>), 2: tensor(0.3730, grad_fn=<MseLossBackward0>)}
0.9993956666666667
episode:  1 training step:  185 loss of agent  2 :  {0: tensor(0.5823, grad_fn=<MseLossBackward0>), 1: tensor(0.3152, grad_fn=<MseLossBackward0>), 2: tensor(0.2762, grad_fn=<MseLossBackward0>)}
0.9993924
episode:  1 training step:  186 loss of agent  0 :  {0: tensor(0.2053, grad_fn=<MseLossBackward0>), 1: tensor(0.3152, grad_fn=<MseLossBackward0>), 2: tensor(0.2762, grad_fn=<MseLossBackward0>)}
0.9993924
episode:  1 training step:  186 loss of agent  1 :  {0: tensor(0.2053, grad_fn=<MseLossBackward0>), 1: tensor(0.1447, grad_fn=<MseLossBackward0>), 2: tensor(0.2762, grad_fn=<MseLossBackward0>)}
0.9993924
episode:  1 training step:  186 loss of agent  2 :  {0: tensor(0.2053, grad_fn=<MseLossBackward0>), 1: tensor(0.1447, grad_fn=<MseLossBackward0>), 2: tensor(0.3360, grad_fn=<MseLossBackward0>)}
0.9993891333333333
episode:  1 training step:  187 loss of agent  0 :  {0: tensor(0.2145, grad_fn=<MseLossBackward0>), 1: tensor(0.1447, grad_fn=<MseLossBackward0>), 2: tensor(0.3360, grad_fn=<MseLossBackward0>)}
0.9993891333333333
episode:  1 training step:  187 loss of agent  1 :  {0: tensor(0.2145, grad_fn=<MseLossBackward0>), 1: tensor(0.1826, grad_fn=<MseLossBackward0>), 2: tensor(0.3360, grad_fn=<MseLossBackward0>)}
0.9993891333333333
episode:  1 training step:  187 loss of agent  2 :  {0: tensor(0.2145, grad_fn=<MseLossBackward0>), 1: tensor(0.1826, grad_fn=<MseLossBackward0>), 2: tensor(0.1880, grad_fn=<MseLossBackward0>)}
0.9993858666666666
episode:  1 training step:  188 loss of agent  0 :  {0: tensor(0.1634, grad_fn=<MseLossBackward0>), 1: tensor(0.1826, grad_fn=<MseLossBackward0>), 2: tensor(0.1880, grad_fn=<MseLossBackward0>)}
0.9993858666666666
episode:  1 training step:  188 loss of agent  1 :  {0: tensor(0.1634, grad_fn=<MseLossBackward0>), 1: tensor(0.2628, grad_fn=<MseLossBackward0>), 2: tensor(0.1880, grad_fn=<MseLossBackward0>)}
0.9993858666666666
episode:  1 training step:  188 loss of agent  2 :  {0: tensor(0.1634, grad_fn=<MseLossBackward0>), 1: tensor(0.2628, grad_fn=<MseLossBackward0>), 2: tensor(0.2214, grad_fn=<MseLossBackward0>)}
0.9993826
episode:  1 training step:  189 loss of agent  0 :  {0: tensor(0.3360, grad_fn=<MseLossBackward0>), 1: tensor(0.2628, grad_fn=<MseLossBackward0>), 2: tensor(0.2214, grad_fn=<MseLossBackward0>)}
0.9993826
episode:  1 training step:  189 loss of agent  1 :  {0: tensor(0.3360, grad_fn=<MseLossBackward0>), 1: tensor(0.3071, grad_fn=<MseLossBackward0>), 2: tensor(0.2214, grad_fn=<MseLossBackward0>)}
0.9993826
episode:  1 training step:  189 loss of agent  2 :  {0: tensor(0.3360, grad_fn=<MseLossBackward0>), 1: tensor(0.3071, grad_fn=<MseLossBackward0>), 2: tensor(0.3272, grad_fn=<MseLossBackward0>)}
0.9993793333333333
episode:  1 training step:  190 loss of agent  0 :  {0: tensor(0.4419, grad_fn=<MseLossBackward0>), 1: tensor(0.3071, grad_fn=<MseLossBackward0>), 2: tensor(0.3272, grad_fn=<MseLossBackward0>)}
0.9993793333333333
episode:  1 training step:  190 loss of agent  1 :  {0: tensor(0.4419, grad_fn=<MseLossBackward0>), 1: tensor(0.2946, grad_fn=<MseLossBackward0>), 2: tensor(0.3272, grad_fn=<MseLossBackward0>)}
0.9993793333333333
episode:  1 training step:  190 loss of agent  2 :  {0: tensor(0.4419, grad_fn=<MseLossBackward0>), 1: tensor(0.2946, grad_fn=<MseLossBackward0>), 2: tensor(0.4500, grad_fn=<MseLossBackward0>)}
0.9993760666666667
episode:  1 training step:  191 loss of agent  0 :  {0: tensor(2.0231, grad_fn=<MseLossBackward0>), 1: tensor(0.2946, grad_fn=<MseLossBackward0>), 2: tensor(0.4500, grad_fn=<MseLossBackward0>)}
0.9993760666666667
episode:  1 training step:  191 loss of agent  1 :  {0: tensor(2.0231, grad_fn=<MseLossBackward0>), 1: tensor(0.9247, grad_fn=<MseLossBackward0>), 2: tensor(0.4500, grad_fn=<MseLossBackward0>)}
0.9993760666666667
episode:  1 training step:  191 loss of agent  2 :  {0: tensor(2.0231, grad_fn=<MseLossBackward0>), 1: tensor(0.9247, grad_fn=<MseLossBackward0>), 2: tensor(0.9459, grad_fn=<MseLossBackward0>)}
0.9993728
episode:  1 training step:  192 loss of agent  0 :  {0: tensor(1.7275, grad_fn=<MseLossBackward0>), 1: tensor(0.9247, grad_fn=<MseLossBackward0>), 2: tensor(0.9459, grad_fn=<MseLossBackward0>)}
0.9993728
episode:  1 training step:  192 loss of agent  1 :  {0: tensor(1.7275, grad_fn=<MseLossBackward0>), 1: tensor(0.5845, grad_fn=<MseLossBackward0>), 2: tensor(0.9459, grad_fn=<MseLossBackward0>)}
0.9993728
episode:  1 training step:  192 loss of agent  2 :  {0: tensor(1.7275, grad_fn=<MseLossBackward0>), 1: tensor(0.5845, grad_fn=<MseLossBackward0>), 2: tensor(0.7754, grad_fn=<MseLossBackward0>)}
0.9993695333333333
episode:  1 training step:  193 loss of agent  0 :  {0: tensor(1.3734, grad_fn=<MseLossBackward0>), 1: tensor(0.5845, grad_fn=<MseLossBackward0>), 2: tensor(0.7754, grad_fn=<MseLossBackward0>)}
0.9993695333333333
episode:  1 training step:  193 loss of agent  1 :  {0: tensor(1.3734, grad_fn=<MseLossBackward0>), 1: tensor(0.5728, grad_fn=<MseLossBackward0>), 2: tensor(0.7754, grad_fn=<MseLossBackward0>)}
0.9993695333333333
episode:  1 training step:  193 loss of agent  2 :  {0: tensor(1.3734, grad_fn=<MseLossBackward0>), 1: tensor(0.5728, grad_fn=<MseLossBackward0>), 2: tensor(0.5320, grad_fn=<MseLossBackward0>)}
0.9993662666666666
episode:  1 training step:  194 loss of agent  0 :  {0: tensor(0.8519, grad_fn=<MseLossBackward0>), 1: tensor(0.5728, grad_fn=<MseLossBackward0>), 2: tensor(0.5320, grad_fn=<MseLossBackward0>)}
0.9993662666666666
episode:  1 training step:  194 loss of agent  1 :  {0: tensor(0.8519, grad_fn=<MseLossBackward0>), 1: tensor(0.4134, grad_fn=<MseLossBackward0>), 2: tensor(0.5320, grad_fn=<MseLossBackward0>)}
0.9993662666666666
episode:  1 training step:  194 loss of agent  2 :  {0: tensor(0.8519, grad_fn=<MseLossBackward0>), 1: tensor(0.4134, grad_fn=<MseLossBackward0>), 2: tensor(0.3040, grad_fn=<MseLossBackward0>)}
0.999363
episode:  1 training step:  195 loss of agent  0 :  {0: tensor(0.5938, grad_fn=<MseLossBackward0>), 1: tensor(0.4134, grad_fn=<MseLossBackward0>), 2: tensor(0.3040, grad_fn=<MseLossBackward0>)}
0.999363
episode:  1 training step:  195 loss of agent  1 :  {0: tensor(0.5938, grad_fn=<MseLossBackward0>), 1: tensor(0.2908, grad_fn=<MseLossBackward0>), 2: tensor(0.3040, grad_fn=<MseLossBackward0>)}
0.999363
episode:  1 training step:  195 loss of agent  2 :  {0: tensor(0.5938, grad_fn=<MseLossBackward0>), 1: tensor(0.2908, grad_fn=<MseLossBackward0>), 2: tensor(0.2521, grad_fn=<MseLossBackward0>)}
0.9993597333333334
episode:  1 training step:  196 loss of agent  0 :  {0: tensor(0.4603, grad_fn=<MseLossBackward0>), 1: tensor(0.2908, grad_fn=<MseLossBackward0>), 2: tensor(0.2521, grad_fn=<MseLossBackward0>)}
0.9993597333333334
episode:  1 training step:  196 loss of agent  1 :  {0: tensor(0.4603, grad_fn=<MseLossBackward0>), 1: tensor(0.1824, grad_fn=<MseLossBackward0>), 2: tensor(0.2521, grad_fn=<MseLossBackward0>)}
0.9993597333333334
episode:  1 training step:  196 loss of agent  2 :  {0: tensor(0.4603, grad_fn=<MseLossBackward0>), 1: tensor(0.1824, grad_fn=<MseLossBackward0>), 2: tensor(0.2331, grad_fn=<MseLossBackward0>)}
0.9993564666666667
episode:  1 training step:  197 loss of agent  0 :  {0: tensor(0.2217, grad_fn=<MseLossBackward0>), 1: tensor(0.1824, grad_fn=<MseLossBackward0>), 2: tensor(0.2331, grad_fn=<MseLossBackward0>)}
0.9993564666666667
episode:  1 training step:  197 loss of agent  1 :  {0: tensor(0.2217, grad_fn=<MseLossBackward0>), 1: tensor(0.2612, grad_fn=<MseLossBackward0>), 2: tensor(0.2331, grad_fn=<MseLossBackward0>)}
0.9993564666666667
episode:  1 training step:  197 loss of agent  2 :  {0: tensor(0.2217, grad_fn=<MseLossBackward0>), 1: tensor(0.2612, grad_fn=<MseLossBackward0>), 2: tensor(0.2353, grad_fn=<MseLossBackward0>)}
0.9993532
episode:  1 training step:  198 loss of agent  0 :  {0: tensor(0.2020, grad_fn=<MseLossBackward0>), 1: tensor(0.2612, grad_fn=<MseLossBackward0>), 2: tensor(0.2353, grad_fn=<MseLossBackward0>)}
0.9993532
episode:  1 training step:  198 loss of agent  1 :  {0: tensor(0.2020, grad_fn=<MseLossBackward0>), 1: tensor(0.3834, grad_fn=<MseLossBackward0>), 2: tensor(0.2353, grad_fn=<MseLossBackward0>)}
0.9993532
episode:  1 training step:  198 loss of agent  2 :  {0: tensor(0.2020, grad_fn=<MseLossBackward0>), 1: tensor(0.3834, grad_fn=<MseLossBackward0>), 2: tensor(0.2924, grad_fn=<MseLossBackward0>)}
0.9993499333333333
episode:  1 training step:  199 loss of agent  0 :  {0: tensor(0.2722, grad_fn=<MseLossBackward0>), 1: tensor(0.3834, grad_fn=<MseLossBackward0>), 2: tensor(0.2924, grad_fn=<MseLossBackward0>)}
0.9993499333333333
episode:  1 training step:  199 loss of agent  1 :  {0: tensor(0.2722, grad_fn=<MseLossBackward0>), 1: tensor(0.3688, grad_fn=<MseLossBackward0>), 2: tensor(0.2924, grad_fn=<MseLossBackward0>)}
0.9993499333333333
episode:  1 training step:  199 loss of agent  2 :  {0: tensor(0.2722, grad_fn=<MseLossBackward0>), 1: tensor(0.3688, grad_fn=<MseLossBackward0>), 2: tensor(0.2903, grad_fn=<MseLossBackward0>)}
0.9993466666666667
episode:  1 training step:  200 loss of agent  0 :  {0: tensor(0.3761, grad_fn=<MseLossBackward0>), 1: tensor(0.3688, grad_fn=<MseLossBackward0>), 2: tensor(0.2903, grad_fn=<MseLossBackward0>)}
0.9993466666666667
episode:  1 training step:  200 loss of agent  1 :  {0: tensor(0.3761, grad_fn=<MseLossBackward0>), 1: tensor(0.3713, grad_fn=<MseLossBackward0>), 2: tensor(0.2903, grad_fn=<MseLossBackward0>)}
0.9993466666666667
episode:  1 training step:  200 loss of agent  2 :  {0: tensor(0.3761, grad_fn=<MseLossBackward0>), 1: tensor(0.3713, grad_fn=<MseLossBackward0>), 2: tensor(0.3436, grad_fn=<MseLossBackward0>)}
seed: 5 steps: 0k score: {'adversary_0': -696.6618486871035, 'agent_0': 383.8588146266909, 'agent_1': 383.8588146266909}
0.9993434
episode:  1 training step:  201 loss of agent  0 :  {0: tensor(2.6245, grad_fn=<MseLossBackward0>), 1: tensor(0.3713, grad_fn=<MseLossBackward0>), 2: tensor(0.3436, grad_fn=<MseLossBackward0>)}
0.9993434
episode:  1 training step:  201 loss of agent  1 :  {0: tensor(2.6245, grad_fn=<MseLossBackward0>), 1: tensor(0.7377, grad_fn=<MseLossBackward0>), 2: tensor(0.3436, grad_fn=<MseLossBackward0>)}
0.9993434
episode:  1 training step:  201 loss of agent  2 :  {0: tensor(2.6245, grad_fn=<MseLossBackward0>), 1: tensor(0.7377, grad_fn=<MseLossBackward0>), 2: tensor(0.9931, grad_fn=<MseLossBackward0>)}
0.9993401333333334
episode:  1 training step:  202 loss of agent  0 :  {0: tensor(1.6668, grad_fn=<MseLossBackward0>), 1: tensor(0.7377, grad_fn=<MseLossBackward0>), 2: tensor(0.9931, grad_fn=<MseLossBackward0>)}
0.9993401333333334
episode:  1 training step:  202 loss of agent  1 :  {0: tensor(1.6668, grad_fn=<MseLossBackward0>), 1: tensor(0.6875, grad_fn=<MseLossBackward0>), 2: tensor(0.9931, grad_fn=<MseLossBackward0>)}
0.9993401333333334
episode:  1 training step:  202 loss of agent  2 :  {0: tensor(1.6668, grad_fn=<MseLossBackward0>), 1: tensor(0.6875, grad_fn=<MseLossBackward0>), 2: tensor(0.5440, grad_fn=<MseLossBackward0>)}
0.9993368666666667
episode:  1 training step:  203 loss of agent  0 :  {0: tensor(1.4757, grad_fn=<MseLossBackward0>), 1: tensor(0.6875, grad_fn=<MseLossBackward0>), 2: tensor(0.5440, grad_fn=<MseLossBackward0>)}
0.9993368666666667
episode:  1 training step:  203 loss of agent  1 :  {0: tensor(1.4757, grad_fn=<MseLossBackward0>), 1: tensor(0.4187, grad_fn=<MseLossBackward0>), 2: tensor(0.5440, grad_fn=<MseLossBackward0>)}
0.9993368666666667
episode:  1 training step:  203 loss of agent  2 :  {0: tensor(1.4757, grad_fn=<MseLossBackward0>), 1: tensor(0.4187, grad_fn=<MseLossBackward0>), 2: tensor(0.6021, grad_fn=<MseLossBackward0>)}
0.9993336
episode:  1 training step:  204 loss of agent  0 :  {0: tensor(0.9104, grad_fn=<MseLossBackward0>), 1: tensor(0.4187, grad_fn=<MseLossBackward0>), 2: tensor(0.6021, grad_fn=<MseLossBackward0>)}
0.9993336
episode:  1 training step:  204 loss of agent  1 :  {0: tensor(0.9104, grad_fn=<MseLossBackward0>), 1: tensor(0.4122, grad_fn=<MseLossBackward0>), 2: tensor(0.6021, grad_fn=<MseLossBackward0>)}
0.9993336
episode:  1 training step:  204 loss of agent  2 :  {0: tensor(0.9104, grad_fn=<MseLossBackward0>), 1: tensor(0.4122, grad_fn=<MseLossBackward0>), 2: tensor(0.3814, grad_fn=<MseLossBackward0>)}
0.9993303333333333
episode:  1 training step:  205 loss of agent  0 :  {0: tensor(0.4229, grad_fn=<MseLossBackward0>), 1: tensor(0.4122, grad_fn=<MseLossBackward0>), 2: tensor(0.3814, grad_fn=<MseLossBackward0>)}
0.9993303333333333
episode:  1 training step:  205 loss of agent  1 :  {0: tensor(0.4229, grad_fn=<MseLossBackward0>), 1: tensor(0.2595, grad_fn=<MseLossBackward0>), 2: tensor(0.3814, grad_fn=<MseLossBackward0>)}
0.9993303333333333
episode:  1 training step:  205 loss of agent  2 :  {0: tensor(0.4229, grad_fn=<MseLossBackward0>), 1: tensor(0.2595, grad_fn=<MseLossBackward0>), 2: tensor(0.2772, grad_fn=<MseLossBackward0>)}
0.9993270666666667
episode:  1 training step:  206 loss of agent  0 :  {0: tensor(0.2042, grad_fn=<MseLossBackward0>), 1: tensor(0.2595, grad_fn=<MseLossBackward0>), 2: tensor(0.2772, grad_fn=<MseLossBackward0>)}
0.9993270666666667
episode:  1 training step:  206 loss of agent  1 :  {0: tensor(0.2042, grad_fn=<MseLossBackward0>), 1: tensor(0.2578, grad_fn=<MseLossBackward0>), 2: tensor(0.2772, grad_fn=<MseLossBackward0>)}
0.9993270666666667
episode:  1 training step:  206 loss of agent  2 :  {0: tensor(0.2042, grad_fn=<MseLossBackward0>), 1: tensor(0.2578, grad_fn=<MseLossBackward0>), 2: tensor(0.2470, grad_fn=<MseLossBackward0>)}
0.9993238
episode:  1 training step:  207 loss of agent  0 :  {0: tensor(0.2549, grad_fn=<MseLossBackward0>), 1: tensor(0.2578, grad_fn=<MseLossBackward0>), 2: tensor(0.2470, grad_fn=<MseLossBackward0>)}
0.9993238
episode:  1 training step:  207 loss of agent  1 :  {0: tensor(0.2549, grad_fn=<MseLossBackward0>), 1: tensor(0.1702, grad_fn=<MseLossBackward0>), 2: tensor(0.2470, grad_fn=<MseLossBackward0>)}
0.9993238
episode:  1 training step:  207 loss of agent  2 :  {0: tensor(0.2549, grad_fn=<MseLossBackward0>), 1: tensor(0.1702, grad_fn=<MseLossBackward0>), 2: tensor(0.2378, grad_fn=<MseLossBackward0>)}
0.9993205333333334
episode:  1 training step:  208 loss of agent  0 :  {0: tensor(0.3577, grad_fn=<MseLossBackward0>), 1: tensor(0.1702, grad_fn=<MseLossBackward0>), 2: tensor(0.2378, grad_fn=<MseLossBackward0>)}
0.9993205333333334
episode:  1 training step:  208 loss of agent  1 :  {0: tensor(0.3577, grad_fn=<MseLossBackward0>), 1: tensor(0.2291, grad_fn=<MseLossBackward0>), 2: tensor(0.2378, grad_fn=<MseLossBackward0>)}
0.9993205333333334
episode:  1 training step:  208 loss of agent  2 :  {0: tensor(0.3577, grad_fn=<MseLossBackward0>), 1: tensor(0.2291, grad_fn=<MseLossBackward0>), 2: tensor(0.3436, grad_fn=<MseLossBackward0>)}
0.9993172666666666
episode:  1 training step:  209 loss of agent  0 :  {0: tensor(0.4303, grad_fn=<MseLossBackward0>), 1: tensor(0.2291, grad_fn=<MseLossBackward0>), 2: tensor(0.3436, grad_fn=<MseLossBackward0>)}
0.9993172666666666
episode:  1 training step:  209 loss of agent  1 :  {0: tensor(0.4303, grad_fn=<MseLossBackward0>), 1: tensor(0.0987, grad_fn=<MseLossBackward0>), 2: tensor(0.3436, grad_fn=<MseLossBackward0>)}
0.9993172666666666
episode:  1 training step:  209 loss of agent  2 :  {0: tensor(0.4303, grad_fn=<MseLossBackward0>), 1: tensor(0.0987, grad_fn=<MseLossBackward0>), 2: tensor(0.4312, grad_fn=<MseLossBackward0>)}
0.999314
episode:  1 training step:  210 loss of agent  0 :  {0: tensor(0.6143, grad_fn=<MseLossBackward0>), 1: tensor(0.0987, grad_fn=<MseLossBackward0>), 2: tensor(0.4312, grad_fn=<MseLossBackward0>)}
0.999314
episode:  1 training step:  210 loss of agent  1 :  {0: tensor(0.6143, grad_fn=<MseLossBackward0>), 1: tensor(0.2469, grad_fn=<MseLossBackward0>), 2: tensor(0.4312, grad_fn=<MseLossBackward0>)}
0.999314
episode:  1 training step:  210 loss of agent  2 :  {0: tensor(0.6143, grad_fn=<MseLossBackward0>), 1: tensor(0.2469, grad_fn=<MseLossBackward0>), 2: tensor(0.4815, grad_fn=<MseLossBackward0>)}
0.9993107333333333
episode:  1 training step:  211 loss of agent  0 :  {0: tensor(1.3730, grad_fn=<MseLossBackward0>), 1: tensor(0.2469, grad_fn=<MseLossBackward0>), 2: tensor(0.4815, grad_fn=<MseLossBackward0>)}
0.9993107333333333
episode:  1 training step:  211 loss of agent  1 :  {0: tensor(1.3730, grad_fn=<MseLossBackward0>), 1: tensor(0.8187, grad_fn=<MseLossBackward0>), 2: tensor(0.4815, grad_fn=<MseLossBackward0>)}
0.9993107333333333
episode:  1 training step:  211 loss of agent  2 :  {0: tensor(1.3730, grad_fn=<MseLossBackward0>), 1: tensor(0.8187, grad_fn=<MseLossBackward0>), 2: tensor(0.7247, grad_fn=<MseLossBackward0>)}
0.9993074666666667
episode:  1 training step:  212 loss of agent  0 :  {0: tensor(1.2420, grad_fn=<MseLossBackward0>), 1: tensor(0.8187, grad_fn=<MseLossBackward0>), 2: tensor(0.7247, grad_fn=<MseLossBackward0>)}
0.9993074666666667
episode:  1 training step:  212 loss of agent  1 :  {0: tensor(1.2420, grad_fn=<MseLossBackward0>), 1: tensor(0.6249, grad_fn=<MseLossBackward0>), 2: tensor(0.7247, grad_fn=<MseLossBackward0>)}
0.9993074666666667
episode:  1 training step:  212 loss of agent  2 :  {0: tensor(1.2420, grad_fn=<MseLossBackward0>), 1: tensor(0.6249, grad_fn=<MseLossBackward0>), 2: tensor(0.8822, grad_fn=<MseLossBackward0>)}
0.9993042
episode:  1 training step:  213 loss of agent  0 :  {0: tensor(0.8829, grad_fn=<MseLossBackward0>), 1: tensor(0.6249, grad_fn=<MseLossBackward0>), 2: tensor(0.8822, grad_fn=<MseLossBackward0>)}
0.9993042
episode:  1 training step:  213 loss of agent  1 :  {0: tensor(0.8829, grad_fn=<MseLossBackward0>), 1: tensor(0.3645, grad_fn=<MseLossBackward0>), 2: tensor(0.8822, grad_fn=<MseLossBackward0>)}
0.9993042
episode:  1 training step:  213 loss of agent  2 :  {0: tensor(0.8829, grad_fn=<MseLossBackward0>), 1: tensor(0.3645, grad_fn=<MseLossBackward0>), 2: tensor(0.6510, grad_fn=<MseLossBackward0>)}
0.9993009333333334
episode:  1 training step:  214 loss of agent  0 :  {0: tensor(0.8373, grad_fn=<MseLossBackward0>), 1: tensor(0.3645, grad_fn=<MseLossBackward0>), 2: tensor(0.6510, grad_fn=<MseLossBackward0>)}
0.9993009333333334
episode:  1 training step:  214 loss of agent  1 :  {0: tensor(0.8373, grad_fn=<MseLossBackward0>), 1: tensor(0.2785, grad_fn=<MseLossBackward0>), 2: tensor(0.6510, grad_fn=<MseLossBackward0>)}
0.9993009333333334
episode:  1 training step:  214 loss of agent  2 :  {0: tensor(0.8373, grad_fn=<MseLossBackward0>), 1: tensor(0.2785, grad_fn=<MseLossBackward0>), 2: tensor(0.4708, grad_fn=<MseLossBackward0>)}
0.9992976666666666
episode:  1 training step:  215 loss of agent  0 :  {0: tensor(0.3767, grad_fn=<MseLossBackward0>), 1: tensor(0.2785, grad_fn=<MseLossBackward0>), 2: tensor(0.4708, grad_fn=<MseLossBackward0>)}
0.9992976666666666
episode:  1 training step:  215 loss of agent  1 :  {0: tensor(0.3767, grad_fn=<MseLossBackward0>), 1: tensor(0.2402, grad_fn=<MseLossBackward0>), 2: tensor(0.4708, grad_fn=<MseLossBackward0>)}
0.9992976666666666
episode:  1 training step:  215 loss of agent  2 :  {0: tensor(0.3767, grad_fn=<MseLossBackward0>), 1: tensor(0.2402, grad_fn=<MseLossBackward0>), 2: tensor(0.3779, grad_fn=<MseLossBackward0>)}
0.9992944
episode:  1 training step:  216 loss of agent  0 :  {0: tensor(0.1977, grad_fn=<MseLossBackward0>), 1: tensor(0.2402, grad_fn=<MseLossBackward0>), 2: tensor(0.3779, grad_fn=<MseLossBackward0>)}
0.9992944
episode:  1 training step:  216 loss of agent  1 :  {0: tensor(0.1977, grad_fn=<MseLossBackward0>), 1: tensor(0.1579, grad_fn=<MseLossBackward0>), 2: tensor(0.3779, grad_fn=<MseLossBackward0>)}
0.9992944
episode:  1 training step:  216 loss of agent  2 :  {0: tensor(0.1977, grad_fn=<MseLossBackward0>), 1: tensor(0.1579, grad_fn=<MseLossBackward0>), 2: tensor(0.1676, grad_fn=<MseLossBackward0>)}
0.9992911333333333
episode:  1 training step:  217 loss of agent  0 :  {0: tensor(0.3469, grad_fn=<MseLossBackward0>), 1: tensor(0.1579, grad_fn=<MseLossBackward0>), 2: tensor(0.1676, grad_fn=<MseLossBackward0>)}
0.9992911333333333
episode:  1 training step:  217 loss of agent  1 :  {0: tensor(0.3469, grad_fn=<MseLossBackward0>), 1: tensor(0.2021, grad_fn=<MseLossBackward0>), 2: tensor(0.1676, grad_fn=<MseLossBackward0>)}
0.9992911333333333
episode:  1 training step:  217 loss of agent  2 :  {0: tensor(0.3469, grad_fn=<MseLossBackward0>), 1: tensor(0.2021, grad_fn=<MseLossBackward0>), 2: tensor(0.2775, grad_fn=<MseLossBackward0>)}
0.9992878666666667
episode:  1 training step:  218 loss of agent  0 :  {0: tensor(0.3095, grad_fn=<MseLossBackward0>), 1: tensor(0.2021, grad_fn=<MseLossBackward0>), 2: tensor(0.2775, grad_fn=<MseLossBackward0>)}
0.9992878666666667
episode:  1 training step:  218 loss of agent  1 :  {0: tensor(0.3095, grad_fn=<MseLossBackward0>), 1: tensor(0.2859, grad_fn=<MseLossBackward0>), 2: tensor(0.2775, grad_fn=<MseLossBackward0>)}
0.9992878666666667
episode:  1 training step:  218 loss of agent  2 :  {0: tensor(0.3095, grad_fn=<MseLossBackward0>), 1: tensor(0.2859, grad_fn=<MseLossBackward0>), 2: tensor(0.1932, grad_fn=<MseLossBackward0>)}
0.9992846
episode:  1 training step:  219 loss of agent  0 :  {0: tensor(0.3496, grad_fn=<MseLossBackward0>), 1: tensor(0.2859, grad_fn=<MseLossBackward0>), 2: tensor(0.1932, grad_fn=<MseLossBackward0>)}
0.9992846
episode:  1 training step:  219 loss of agent  1 :  {0: tensor(0.3496, grad_fn=<MseLossBackward0>), 1: tensor(0.2524, grad_fn=<MseLossBackward0>), 2: tensor(0.1932, grad_fn=<MseLossBackward0>)}
0.9992846
episode:  1 training step:  219 loss of agent  2 :  {0: tensor(0.3496, grad_fn=<MseLossBackward0>), 1: tensor(0.2524, grad_fn=<MseLossBackward0>), 2: tensor(0.2969, grad_fn=<MseLossBackward0>)}
0.9992813333333334
episode:  1 training step:  220 loss of agent  0 :  {0: tensor(0.6710, grad_fn=<MseLossBackward0>), 1: tensor(0.2524, grad_fn=<MseLossBackward0>), 2: tensor(0.2969, grad_fn=<MseLossBackward0>)}
0.9992813333333334
episode:  1 training step:  220 loss of agent  1 :  {0: tensor(0.6710, grad_fn=<MseLossBackward0>), 1: tensor(0.2268, grad_fn=<MseLossBackward0>), 2: tensor(0.2969, grad_fn=<MseLossBackward0>)}
0.9992813333333334
episode:  1 training step:  220 loss of agent  2 :  {0: tensor(0.6710, grad_fn=<MseLossBackward0>), 1: tensor(0.2268, grad_fn=<MseLossBackward0>), 2: tensor(0.3826, grad_fn=<MseLossBackward0>)}
0.9992780666666666
episode:  1 training step:  221 loss of agent  0 :  {0: tensor(1.9297, grad_fn=<MseLossBackward0>), 1: tensor(0.2268, grad_fn=<MseLossBackward0>), 2: tensor(0.3826, grad_fn=<MseLossBackward0>)}
0.9992780666666666
episode:  1 training step:  221 loss of agent  1 :  {0: tensor(1.9297, grad_fn=<MseLossBackward0>), 1: tensor(0.8310, grad_fn=<MseLossBackward0>), 2: tensor(0.3826, grad_fn=<MseLossBackward0>)}
0.9992780666666666
episode:  1 training step:  221 loss of agent  2 :  {0: tensor(1.9297, grad_fn=<MseLossBackward0>), 1: tensor(0.8310, grad_fn=<MseLossBackward0>), 2: tensor(0.8603, grad_fn=<MseLossBackward0>)}
0.9992748
episode:  1 training step:  222 loss of agent  0 :  {0: tensor(1.2518, grad_fn=<MseLossBackward0>), 1: tensor(0.8310, grad_fn=<MseLossBackward0>), 2: tensor(0.8603, grad_fn=<MseLossBackward0>)}
0.9992748
episode:  1 training step:  222 loss of agent  1 :  {0: tensor(1.2518, grad_fn=<MseLossBackward0>), 1: tensor(0.8492, grad_fn=<MseLossBackward0>), 2: tensor(0.8603, grad_fn=<MseLossBackward0>)}
0.9992748
episode:  1 training step:  222 loss of agent  2 :  {0: tensor(1.2518, grad_fn=<MseLossBackward0>), 1: tensor(0.8492, grad_fn=<MseLossBackward0>), 2: tensor(0.7222, grad_fn=<MseLossBackward0>)}
0.9992715333333333
episode:  1 training step:  223 loss of agent  0 :  {0: tensor(1.5391, grad_fn=<MseLossBackward0>), 1: tensor(0.8492, grad_fn=<MseLossBackward0>), 2: tensor(0.7222, grad_fn=<MseLossBackward0>)}
0.9992715333333333
episode:  1 training step:  223 loss of agent  1 :  {0: tensor(1.5391, grad_fn=<MseLossBackward0>), 1: tensor(0.6196, grad_fn=<MseLossBackward0>), 2: tensor(0.7222, grad_fn=<MseLossBackward0>)}
0.9992715333333333
episode:  1 training step:  223 loss of agent  2 :  {0: tensor(1.5391, grad_fn=<MseLossBackward0>), 1: tensor(0.6196, grad_fn=<MseLossBackward0>), 2: tensor(0.5614, grad_fn=<MseLossBackward0>)}
0.9992682666666667
episode:  1 training step:  224 loss of agent  0 :  {0: tensor(0.9006, grad_fn=<MseLossBackward0>), 1: tensor(0.6196, grad_fn=<MseLossBackward0>), 2: tensor(0.5614, grad_fn=<MseLossBackward0>)}
0.9992682666666667
episode:  1 training step:  224 loss of agent  1 :  {0: tensor(0.9006, grad_fn=<MseLossBackward0>), 1: tensor(0.4760, grad_fn=<MseLossBackward0>), 2: tensor(0.5614, grad_fn=<MseLossBackward0>)}
0.9992682666666667
episode:  1 training step:  224 loss of agent  2 :  {0: tensor(0.9006, grad_fn=<MseLossBackward0>), 1: tensor(0.4760, grad_fn=<MseLossBackward0>), 2: tensor(0.3907, grad_fn=<MseLossBackward0>)}
0.999265
episode:  1 training step:  225 loss of agent  0 :  {0: tensor(0.7220, grad_fn=<MseLossBackward0>), 1: tensor(0.4760, grad_fn=<MseLossBackward0>), 2: tensor(0.3907, grad_fn=<MseLossBackward0>)}
0.999265
episode:  1 training step:  225 loss of agent  1 :  {0: tensor(0.7220, grad_fn=<MseLossBackward0>), 1: tensor(0.2878, grad_fn=<MseLossBackward0>), 2: tensor(0.3907, grad_fn=<MseLossBackward0>)}
0.999265
episode:  1 training step:  225 loss of agent  2 :  {0: tensor(0.7220, grad_fn=<MseLossBackward0>), 1: tensor(0.2878, grad_fn=<MseLossBackward0>), 2: tensor(0.3050, grad_fn=<MseLossBackward0>)}
0.9992617333333333
episode:  1 training step:  226 loss of agent  0 :  {0: tensor(0.2925, grad_fn=<MseLossBackward0>), 1: tensor(0.2878, grad_fn=<MseLossBackward0>), 2: tensor(0.3050, grad_fn=<MseLossBackward0>)}
0.9992617333333333
episode:  1 training step:  226 loss of agent  1 :  {0: tensor(0.2925, grad_fn=<MseLossBackward0>), 1: tensor(0.2167, grad_fn=<MseLossBackward0>), 2: tensor(0.3050, grad_fn=<MseLossBackward0>)}
0.9992617333333333
episode:  1 training step:  226 loss of agent  2 :  {0: tensor(0.2925, grad_fn=<MseLossBackward0>), 1: tensor(0.2167, grad_fn=<MseLossBackward0>), 2: tensor(0.1676, grad_fn=<MseLossBackward0>)}
0.9992584666666666
episode:  1 training step:  227 loss of agent  0 :  {0: tensor(0.3564, grad_fn=<MseLossBackward0>), 1: tensor(0.2167, grad_fn=<MseLossBackward0>), 2: tensor(0.1676, grad_fn=<MseLossBackward0>)}
0.9992584666666666
episode:  1 training step:  227 loss of agent  1 :  {0: tensor(0.3564, grad_fn=<MseLossBackward0>), 1: tensor(0.1783, grad_fn=<MseLossBackward0>), 2: tensor(0.1676, grad_fn=<MseLossBackward0>)}
0.9992584666666666
episode:  1 training step:  227 loss of agent  2 :  {0: tensor(0.3564, grad_fn=<MseLossBackward0>), 1: tensor(0.1783, grad_fn=<MseLossBackward0>), 2: tensor(0.3402, grad_fn=<MseLossBackward0>)}
0.9992552
episode:  1 training step:  228 loss of agent  0 :  {0: tensor(0.2811, grad_fn=<MseLossBackward0>), 1: tensor(0.1783, grad_fn=<MseLossBackward0>), 2: tensor(0.3402, grad_fn=<MseLossBackward0>)}
0.9992552
episode:  1 training step:  228 loss of agent  1 :  {0: tensor(0.2811, grad_fn=<MseLossBackward0>), 1: tensor(0.1681, grad_fn=<MseLossBackward0>), 2: tensor(0.3402, grad_fn=<MseLossBackward0>)}
0.9992552
episode:  1 training step:  228 loss of agent  2 :  {0: tensor(0.2811, grad_fn=<MseLossBackward0>), 1: tensor(0.1681, grad_fn=<MseLossBackward0>), 2: tensor(0.3367, grad_fn=<MseLossBackward0>)}
0.9992519333333333
episode:  1 training step:  229 loss of agent  0 :  {0: tensor(0.3773, grad_fn=<MseLossBackward0>), 1: tensor(0.1681, grad_fn=<MseLossBackward0>), 2: tensor(0.3367, grad_fn=<MseLossBackward0>)}
0.9992519333333333
episode:  1 training step:  229 loss of agent  1 :  {0: tensor(0.3773, grad_fn=<MseLossBackward0>), 1: tensor(0.2538, grad_fn=<MseLossBackward0>), 2: tensor(0.3367, grad_fn=<MseLossBackward0>)}
0.9992519333333333
episode:  1 training step:  229 loss of agent  2 :  {0: tensor(0.3773, grad_fn=<MseLossBackward0>), 1: tensor(0.2538, grad_fn=<MseLossBackward0>), 2: tensor(0.5276, grad_fn=<MseLossBackward0>)}
0.9992486666666667
episode:  1 training step:  230 loss of agent  0 :  {0: tensor(0.6708, grad_fn=<MseLossBackward0>), 1: tensor(0.2538, grad_fn=<MseLossBackward0>), 2: tensor(0.5276, grad_fn=<MseLossBackward0>)}
0.9992486666666667
episode:  1 training step:  230 loss of agent  1 :  {0: tensor(0.6708, grad_fn=<MseLossBackward0>), 1: tensor(0.3565, grad_fn=<MseLossBackward0>), 2: tensor(0.5276, grad_fn=<MseLossBackward0>)}
0.9992486666666667
episode:  1 training step:  230 loss of agent  2 :  {0: tensor(0.6708, grad_fn=<MseLossBackward0>), 1: tensor(0.3565, grad_fn=<MseLossBackward0>), 2: tensor(0.5353, grad_fn=<MseLossBackward0>)}
0.9992454
episode:  1 training step:  231 loss of agent  0 :  {0: tensor(2.2368, grad_fn=<MseLossBackward0>), 1: tensor(0.3565, grad_fn=<MseLossBackward0>), 2: tensor(0.5353, grad_fn=<MseLossBackward0>)}
0.9992454
episode:  1 training step:  231 loss of agent  1 :  {0: tensor(2.2368, grad_fn=<MseLossBackward0>), 1: tensor(0.8160, grad_fn=<MseLossBackward0>), 2: tensor(0.5353, grad_fn=<MseLossBackward0>)}
0.9992454
episode:  1 training step:  231 loss of agent  2 :  {0: tensor(2.2368, grad_fn=<MseLossBackward0>), 1: tensor(0.8160, grad_fn=<MseLossBackward0>), 2: tensor(0.8099, grad_fn=<MseLossBackward0>)}
0.9992421333333333
episode:  1 training step:  232 loss of agent  0 :  {0: tensor(1.5976, grad_fn=<MseLossBackward0>), 1: tensor(0.8160, grad_fn=<MseLossBackward0>), 2: tensor(0.8099, grad_fn=<MseLossBackward0>)}
0.9992421333333333
episode:  1 training step:  232 loss of agent  1 :  {0: tensor(1.5976, grad_fn=<MseLossBackward0>), 1: tensor(0.7633, grad_fn=<MseLossBackward0>), 2: tensor(0.8099, grad_fn=<MseLossBackward0>)}
0.9992421333333333
episode:  1 training step:  232 loss of agent  2 :  {0: tensor(1.5976, grad_fn=<MseLossBackward0>), 1: tensor(0.7633, grad_fn=<MseLossBackward0>), 2: tensor(0.8324, grad_fn=<MseLossBackward0>)}
0.9992388666666666
episode:  1 training step:  233 loss of agent  0 :  {0: tensor(1.6984, grad_fn=<MseLossBackward0>), 1: tensor(0.7633, grad_fn=<MseLossBackward0>), 2: tensor(0.8324, grad_fn=<MseLossBackward0>)}
0.9992388666666666
episode:  1 training step:  233 loss of agent  1 :  {0: tensor(1.6984, grad_fn=<MseLossBackward0>), 1: tensor(0.4265, grad_fn=<MseLossBackward0>), 2: tensor(0.8324, grad_fn=<MseLossBackward0>)}
0.9992388666666666
episode:  1 training step:  233 loss of agent  2 :  {0: tensor(1.6984, grad_fn=<MseLossBackward0>), 1: tensor(0.4265, grad_fn=<MseLossBackward0>), 2: tensor(0.6282, grad_fn=<MseLossBackward0>)}
0.9992356
episode:  1 training step:  234 loss of agent  0 :  {0: tensor(1.1379, grad_fn=<MseLossBackward0>), 1: tensor(0.4265, grad_fn=<MseLossBackward0>), 2: tensor(0.6282, grad_fn=<MseLossBackward0>)}
0.9992356
episode:  1 training step:  234 loss of agent  1 :  {0: tensor(1.1379, grad_fn=<MseLossBackward0>), 1: tensor(0.4172, grad_fn=<MseLossBackward0>), 2: tensor(0.6282, grad_fn=<MseLossBackward0>)}
0.9992356
episode:  1 training step:  234 loss of agent  2 :  {0: tensor(1.1379, grad_fn=<MseLossBackward0>), 1: tensor(0.4172, grad_fn=<MseLossBackward0>), 2: tensor(0.5110, grad_fn=<MseLossBackward0>)}
0.9992323333333333
episode:  1 training step:  235 loss of agent  0 :  {0: tensor(0.3895, grad_fn=<MseLossBackward0>), 1: tensor(0.4172, grad_fn=<MseLossBackward0>), 2: tensor(0.5110, grad_fn=<MseLossBackward0>)}
0.9992323333333333
episode:  1 training step:  235 loss of agent  1 :  {0: tensor(0.3895, grad_fn=<MseLossBackward0>), 1: tensor(0.2295, grad_fn=<MseLossBackward0>), 2: tensor(0.5110, grad_fn=<MseLossBackward0>)}
0.9992323333333333
episode:  1 training step:  235 loss of agent  2 :  {0: tensor(0.3895, grad_fn=<MseLossBackward0>), 1: tensor(0.2295, grad_fn=<MseLossBackward0>), 2: tensor(0.3354, grad_fn=<MseLossBackward0>)}
0.9992290666666667
episode:  1 training step:  236 loss of agent  0 :  {0: tensor(0.3138, grad_fn=<MseLossBackward0>), 1: tensor(0.2295, grad_fn=<MseLossBackward0>), 2: tensor(0.3354, grad_fn=<MseLossBackward0>)}
0.9992290666666667
episode:  1 training step:  236 loss of agent  1 :  {0: tensor(0.3138, grad_fn=<MseLossBackward0>), 1: tensor(0.2627, grad_fn=<MseLossBackward0>), 2: tensor(0.3354, grad_fn=<MseLossBackward0>)}
0.9992290666666667
episode:  1 training step:  236 loss of agent  2 :  {0: tensor(0.3138, grad_fn=<MseLossBackward0>), 1: tensor(0.2627, grad_fn=<MseLossBackward0>), 2: tensor(0.2570, grad_fn=<MseLossBackward0>)}
0.9992258
episode:  1 training step:  237 loss of agent  0 :  {0: tensor(0.3083, grad_fn=<MseLossBackward0>), 1: tensor(0.2627, grad_fn=<MseLossBackward0>), 2: tensor(0.2570, grad_fn=<MseLossBackward0>)}
0.9992258
episode:  1 training step:  237 loss of agent  1 :  {0: tensor(0.3083, grad_fn=<MseLossBackward0>), 1: tensor(0.2838, grad_fn=<MseLossBackward0>), 2: tensor(0.2570, grad_fn=<MseLossBackward0>)}
0.9992258
episode:  1 training step:  237 loss of agent  2 :  {0: tensor(0.3083, grad_fn=<MseLossBackward0>), 1: tensor(0.2838, grad_fn=<MseLossBackward0>), 2: tensor(0.2375, grad_fn=<MseLossBackward0>)}
0.9992225333333333
episode:  1 training step:  238 loss of agent  0 :  {0: tensor(0.3638, grad_fn=<MseLossBackward0>), 1: tensor(0.2838, grad_fn=<MseLossBackward0>), 2: tensor(0.2375, grad_fn=<MseLossBackward0>)}
0.9992225333333333
episode:  1 training step:  238 loss of agent  1 :  {0: tensor(0.3638, grad_fn=<MseLossBackward0>), 1: tensor(0.2956, grad_fn=<MseLossBackward0>), 2: tensor(0.2375, grad_fn=<MseLossBackward0>)}
0.9992225333333333
episode:  1 training step:  238 loss of agent  2 :  {0: tensor(0.3638, grad_fn=<MseLossBackward0>), 1: tensor(0.2956, grad_fn=<MseLossBackward0>), 2: tensor(0.2903, grad_fn=<MseLossBackward0>)}
0.9992192666666667
episode:  1 training step:  239 loss of agent  0 :  {0: tensor(0.4213, grad_fn=<MseLossBackward0>), 1: tensor(0.2956, grad_fn=<MseLossBackward0>), 2: tensor(0.2903, grad_fn=<MseLossBackward0>)}
0.9992192666666667
episode:  1 training step:  239 loss of agent  1 :  {0: tensor(0.4213, grad_fn=<MseLossBackward0>), 1: tensor(0.3483, grad_fn=<MseLossBackward0>), 2: tensor(0.2903, grad_fn=<MseLossBackward0>)}
0.9992192666666667
episode:  1 training step:  239 loss of agent  2 :  {0: tensor(0.4213, grad_fn=<MseLossBackward0>), 1: tensor(0.3483, grad_fn=<MseLossBackward0>), 2: tensor(0.3630, grad_fn=<MseLossBackward0>)}
0.999216
episode:  1 training step:  240 loss of agent  0 :  {0: tensor(0.3265, grad_fn=<MseLossBackward0>), 1: tensor(0.3483, grad_fn=<MseLossBackward0>), 2: tensor(0.3630, grad_fn=<MseLossBackward0>)}
0.999216
episode:  1 training step:  240 loss of agent  1 :  {0: tensor(0.3265, grad_fn=<MseLossBackward0>), 1: tensor(0.4940, grad_fn=<MseLossBackward0>), 2: tensor(0.3630, grad_fn=<MseLossBackward0>)}
0.999216
episode:  1 training step:  240 loss of agent  2 :  {0: tensor(0.3265, grad_fn=<MseLossBackward0>), 1: tensor(0.4940, grad_fn=<MseLossBackward0>), 2: tensor(0.3495, grad_fn=<MseLossBackward0>)}
0.9992127333333334
episode:  1 training step:  241 loss of agent  0 :  {0: tensor(1.9642, grad_fn=<MseLossBackward0>), 1: tensor(0.4940, grad_fn=<MseLossBackward0>), 2: tensor(0.3495, grad_fn=<MseLossBackward0>)}
0.9992127333333334
episode:  1 training step:  241 loss of agent  1 :  {0: tensor(1.9642, grad_fn=<MseLossBackward0>), 1: tensor(0.7064, grad_fn=<MseLossBackward0>), 2: tensor(0.3495, grad_fn=<MseLossBackward0>)}
0.9992127333333334
episode:  1 training step:  241 loss of agent  2 :  {0: tensor(1.9642, grad_fn=<MseLossBackward0>), 1: tensor(0.7064, grad_fn=<MseLossBackward0>), 2: tensor(0.8783, grad_fn=<MseLossBackward0>)}
0.9992094666666667
episode:  1 training step:  242 loss of agent  0 :  {0: tensor(2.1233, grad_fn=<MseLossBackward0>), 1: tensor(0.7064, grad_fn=<MseLossBackward0>), 2: tensor(0.8783, grad_fn=<MseLossBackward0>)}
0.9992094666666667
episode:  1 training step:  242 loss of agent  1 :  {0: tensor(2.1233, grad_fn=<MseLossBackward0>), 1: tensor(0.7252, grad_fn=<MseLossBackward0>), 2: tensor(0.8783, grad_fn=<MseLossBackward0>)}
0.9992094666666667
episode:  1 training step:  242 loss of agent  2 :  {0: tensor(2.1233, grad_fn=<MseLossBackward0>), 1: tensor(0.7252, grad_fn=<MseLossBackward0>), 2: tensor(1.1242, grad_fn=<MseLossBackward0>)}
0.9992062
episode:  1 training step:  243 loss of agent  0 :  {0: tensor(1.2501, grad_fn=<MseLossBackward0>), 1: tensor(0.7252, grad_fn=<MseLossBackward0>), 2: tensor(1.1242, grad_fn=<MseLossBackward0>)}
0.9992062
episode:  1 training step:  243 loss of agent  1 :  {0: tensor(1.2501, grad_fn=<MseLossBackward0>), 1: tensor(0.6456, grad_fn=<MseLossBackward0>), 2: tensor(1.1242, grad_fn=<MseLossBackward0>)}
0.9992062
episode:  1 training step:  243 loss of agent  2 :  {0: tensor(1.2501, grad_fn=<MseLossBackward0>), 1: tensor(0.6456, grad_fn=<MseLossBackward0>), 2: tensor(0.6798, grad_fn=<MseLossBackward0>)}
0.9992029333333333
episode:  1 training step:  244 loss of agent  0 :  {0: tensor(0.7843, grad_fn=<MseLossBackward0>), 1: tensor(0.6456, grad_fn=<MseLossBackward0>), 2: tensor(0.6798, grad_fn=<MseLossBackward0>)}
0.9992029333333333
episode:  1 training step:  244 loss of agent  1 :  {0: tensor(0.7843, grad_fn=<MseLossBackward0>), 1: tensor(0.4486, grad_fn=<MseLossBackward0>), 2: tensor(0.6798, grad_fn=<MseLossBackward0>)}
0.9992029333333333
episode:  1 training step:  244 loss of agent  2 :  {0: tensor(0.7843, grad_fn=<MseLossBackward0>), 1: tensor(0.4486, grad_fn=<MseLossBackward0>), 2: tensor(0.3099, grad_fn=<MseLossBackward0>)}
0.9991996666666667
episode:  1 training step:  245 loss of agent  0 :  {0: tensor(0.6660, grad_fn=<MseLossBackward0>), 1: tensor(0.4486, grad_fn=<MseLossBackward0>), 2: tensor(0.3099, grad_fn=<MseLossBackward0>)}
0.9991996666666667
episode:  1 training step:  245 loss of agent  1 :  {0: tensor(0.6660, grad_fn=<MseLossBackward0>), 1: tensor(0.1548, grad_fn=<MseLossBackward0>), 2: tensor(0.3099, grad_fn=<MseLossBackward0>)}
0.9991996666666667
episode:  1 training step:  245 loss of agent  2 :  {0: tensor(0.6660, grad_fn=<MseLossBackward0>), 1: tensor(0.1548, grad_fn=<MseLossBackward0>), 2: tensor(0.3590, grad_fn=<MseLossBackward0>)}
0.9991964
episode:  1 training step:  246 loss of agent  0 :  {0: tensor(0.4384, grad_fn=<MseLossBackward0>), 1: tensor(0.1548, grad_fn=<MseLossBackward0>), 2: tensor(0.3590, grad_fn=<MseLossBackward0>)}
0.9991964
episode:  1 training step:  246 loss of agent  1 :  {0: tensor(0.4384, grad_fn=<MseLossBackward0>), 1: tensor(0.1522, grad_fn=<MseLossBackward0>), 2: tensor(0.3590, grad_fn=<MseLossBackward0>)}
0.9991964
episode:  1 training step:  246 loss of agent  2 :  {0: tensor(0.4384, grad_fn=<MseLossBackward0>), 1: tensor(0.1522, grad_fn=<MseLossBackward0>), 2: tensor(0.3166, grad_fn=<MseLossBackward0>)}
0.9991931333333334
episode:  1 training step:  247 loss of agent  0 :  {0: tensor(0.3564, grad_fn=<MseLossBackward0>), 1: tensor(0.1522, grad_fn=<MseLossBackward0>), 2: tensor(0.3166, grad_fn=<MseLossBackward0>)}
0.9991931333333334
episode:  1 training step:  247 loss of agent  1 :  {0: tensor(0.3564, grad_fn=<MseLossBackward0>), 1: tensor(0.1868, grad_fn=<MseLossBackward0>), 2: tensor(0.3166, grad_fn=<MseLossBackward0>)}
0.9991931333333334
episode:  1 training step:  247 loss of agent  2 :  {0: tensor(0.3564, grad_fn=<MseLossBackward0>), 1: tensor(0.1868, grad_fn=<MseLossBackward0>), 2: tensor(0.3420, grad_fn=<MseLossBackward0>)}
0.9991898666666666
episode:  1 training step:  248 loss of agent  0 :  {0: tensor(0.6146, grad_fn=<MseLossBackward0>), 1: tensor(0.1868, grad_fn=<MseLossBackward0>), 2: tensor(0.3420, grad_fn=<MseLossBackward0>)}
0.9991898666666666
episode:  1 training step:  248 loss of agent  1 :  {0: tensor(0.6146, grad_fn=<MseLossBackward0>), 1: tensor(0.3234, grad_fn=<MseLossBackward0>), 2: tensor(0.3420, grad_fn=<MseLossBackward0>)}
0.9991898666666666
episode:  1 training step:  248 loss of agent  2 :  {0: tensor(0.6146, grad_fn=<MseLossBackward0>), 1: tensor(0.3234, grad_fn=<MseLossBackward0>), 2: tensor(0.2925, grad_fn=<MseLossBackward0>)}
0.9991866
episode:  1 training step:  249 loss of agent  0 :  {0: tensor(0.9076, grad_fn=<MseLossBackward0>), 1: tensor(0.3234, grad_fn=<MseLossBackward0>), 2: tensor(0.2925, grad_fn=<MseLossBackward0>)}
0.9991866
episode:  1 training step:  249 loss of agent  1 :  {0: tensor(0.9076, grad_fn=<MseLossBackward0>), 1: tensor(0.2995, grad_fn=<MseLossBackward0>), 2: tensor(0.2925, grad_fn=<MseLossBackward0>)}
0.9991866
episode:  1 training step:  249 loss of agent  2 :  {0: tensor(0.9076, grad_fn=<MseLossBackward0>), 1: tensor(0.2995, grad_fn=<MseLossBackward0>), 2: tensor(0.3825, grad_fn=<MseLossBackward0>)}
0.9991833333333333
episode:  1 training step:  250 loss of agent  0 :  {0: tensor(0.6025, grad_fn=<MseLossBackward0>), 1: tensor(0.2995, grad_fn=<MseLossBackward0>), 2: tensor(0.3825, grad_fn=<MseLossBackward0>)}
0.9991833333333333
episode:  1 training step:  250 loss of agent  1 :  {0: tensor(0.6025, grad_fn=<MseLossBackward0>), 1: tensor(0.3281, grad_fn=<MseLossBackward0>), 2: tensor(0.3825, grad_fn=<MseLossBackward0>)}
0.9991833333333333
episode:  1 training step:  250 loss of agent  2 :  {0: tensor(0.6025, grad_fn=<MseLossBackward0>), 1: tensor(0.3281, grad_fn=<MseLossBackward0>), 2: tensor(0.5865, grad_fn=<MseLossBackward0>)}
0.9991800666666667
episode:  1 training step:  251 loss of agent  0 :  {0: tensor(1.8749, grad_fn=<MseLossBackward0>), 1: tensor(0.3281, grad_fn=<MseLossBackward0>), 2: tensor(0.5865, grad_fn=<MseLossBackward0>)}
0.9991800666666667
episode:  1 training step:  251 loss of agent  1 :  {0: tensor(1.8749, grad_fn=<MseLossBackward0>), 1: tensor(1.0363, grad_fn=<MseLossBackward0>), 2: tensor(0.5865, grad_fn=<MseLossBackward0>)}
0.9991800666666667
episode:  1 training step:  251 loss of agent  2 :  {0: tensor(1.8749, grad_fn=<MseLossBackward0>), 1: tensor(1.0363, grad_fn=<MseLossBackward0>), 2: tensor(0.7886, grad_fn=<MseLossBackward0>)}
0.9991768
episode:  1 training step:  252 loss of agent  0 :  {0: tensor(1.4115, grad_fn=<MseLossBackward0>), 1: tensor(1.0363, grad_fn=<MseLossBackward0>), 2: tensor(0.7886, grad_fn=<MseLossBackward0>)}
0.9991768
episode:  1 training step:  252 loss of agent  1 :  {0: tensor(1.4115, grad_fn=<MseLossBackward0>), 1: tensor(0.6317, grad_fn=<MseLossBackward0>), 2: tensor(0.7886, grad_fn=<MseLossBackward0>)}
0.9991768
episode:  1 training step:  252 loss of agent  2 :  {0: tensor(1.4115, grad_fn=<MseLossBackward0>), 1: tensor(0.6317, grad_fn=<MseLossBackward0>), 2: tensor(0.6593, grad_fn=<MseLossBackward0>)}
0.9991735333333334
episode:  1 training step:  253 loss of agent  0 :  {0: tensor(1.5108, grad_fn=<MseLossBackward0>), 1: tensor(0.6317, grad_fn=<MseLossBackward0>), 2: tensor(0.6593, grad_fn=<MseLossBackward0>)}
0.9991735333333334
episode:  1 training step:  253 loss of agent  1 :  {0: tensor(1.5108, grad_fn=<MseLossBackward0>), 1: tensor(0.3785, grad_fn=<MseLossBackward0>), 2: tensor(0.6593, grad_fn=<MseLossBackward0>)}
0.9991735333333334
episode:  1 training step:  253 loss of agent  2 :  {0: tensor(1.5108, grad_fn=<MseLossBackward0>), 1: tensor(0.3785, grad_fn=<MseLossBackward0>), 2: tensor(0.7129, grad_fn=<MseLossBackward0>)}
0.9991702666666666
episode:  1 training step:  254 loss of agent  0 :  {0: tensor(0.6782, grad_fn=<MseLossBackward0>), 1: tensor(0.3785, grad_fn=<MseLossBackward0>), 2: tensor(0.7129, grad_fn=<MseLossBackward0>)}
0.9991702666666666
episode:  1 training step:  254 loss of agent  1 :  {0: tensor(0.6782, grad_fn=<MseLossBackward0>), 1: tensor(0.4518, grad_fn=<MseLossBackward0>), 2: tensor(0.7129, grad_fn=<MseLossBackward0>)}
0.9991702666666666
episode:  1 training step:  254 loss of agent  2 :  {0: tensor(0.6782, grad_fn=<MseLossBackward0>), 1: tensor(0.4518, grad_fn=<MseLossBackward0>), 2: tensor(0.5212, grad_fn=<MseLossBackward0>)}
0.999167
episode:  1 training step:  255 loss of agent  0 :  {0: tensor(0.6299, grad_fn=<MseLossBackward0>), 1: tensor(0.4518, grad_fn=<MseLossBackward0>), 2: tensor(0.5212, grad_fn=<MseLossBackward0>)}
0.999167
episode:  1 training step:  255 loss of agent  1 :  {0: tensor(0.6299, grad_fn=<MseLossBackward0>), 1: tensor(0.2946, grad_fn=<MseLossBackward0>), 2: tensor(0.5212, grad_fn=<MseLossBackward0>)}
0.999167
episode:  1 training step:  255 loss of agent  2 :  {0: tensor(0.6299, grad_fn=<MseLossBackward0>), 1: tensor(0.2946, grad_fn=<MseLossBackward0>), 2: tensor(0.3444, grad_fn=<MseLossBackward0>)}
0.9991637333333333
episode:  1 training step:  256 loss of agent  0 :  {0: tensor(0.5730, grad_fn=<MseLossBackward0>), 1: tensor(0.2946, grad_fn=<MseLossBackward0>), 2: tensor(0.3444, grad_fn=<MseLossBackward0>)}
0.9991637333333333
episode:  1 training step:  256 loss of agent  1 :  {0: tensor(0.5730, grad_fn=<MseLossBackward0>), 1: tensor(0.1975, grad_fn=<MseLossBackward0>), 2: tensor(0.3444, grad_fn=<MseLossBackward0>)}
0.9991637333333333
episode:  1 training step:  256 loss of agent  2 :  {0: tensor(0.5730, grad_fn=<MseLossBackward0>), 1: tensor(0.1975, grad_fn=<MseLossBackward0>), 2: tensor(0.3592, grad_fn=<MseLossBackward0>)}
0.9991604666666667
episode:  1 training step:  257 loss of agent  0 :  {0: tensor(0.3509, grad_fn=<MseLossBackward0>), 1: tensor(0.1975, grad_fn=<MseLossBackward0>), 2: tensor(0.3592, grad_fn=<MseLossBackward0>)}
0.9991604666666667
episode:  1 training step:  257 loss of agent  1 :  {0: tensor(0.3509, grad_fn=<MseLossBackward0>), 1: tensor(0.1603, grad_fn=<MseLossBackward0>), 2: tensor(0.3592, grad_fn=<MseLossBackward0>)}
0.9991604666666667
episode:  1 training step:  257 loss of agent  2 :  {0: tensor(0.3509, grad_fn=<MseLossBackward0>), 1: tensor(0.1603, grad_fn=<MseLossBackward0>), 2: tensor(0.3391, grad_fn=<MseLossBackward0>)}
0.9991572
episode:  1 training step:  258 loss of agent  0 :  {0: tensor(0.3986, grad_fn=<MseLossBackward0>), 1: tensor(0.1603, grad_fn=<MseLossBackward0>), 2: tensor(0.3391, grad_fn=<MseLossBackward0>)}
0.9991572
episode:  1 training step:  258 loss of agent  1 :  {0: tensor(0.3986, grad_fn=<MseLossBackward0>), 1: tensor(0.2151, grad_fn=<MseLossBackward0>), 2: tensor(0.3391, grad_fn=<MseLossBackward0>)}
0.9991572
episode:  1 training step:  258 loss of agent  2 :  {0: tensor(0.3986, grad_fn=<MseLossBackward0>), 1: tensor(0.2151, grad_fn=<MseLossBackward0>), 2: tensor(0.2973, grad_fn=<MseLossBackward0>)}
0.9991539333333334
episode:  1 training step:  259 loss of agent  0 :  {0: tensor(0.6378, grad_fn=<MseLossBackward0>), 1: tensor(0.2151, grad_fn=<MseLossBackward0>), 2: tensor(0.2973, grad_fn=<MseLossBackward0>)}
0.9991539333333334
episode:  1 training step:  259 loss of agent  1 :  {0: tensor(0.6378, grad_fn=<MseLossBackward0>), 1: tensor(0.4123, grad_fn=<MseLossBackward0>), 2: tensor(0.2973, grad_fn=<MseLossBackward0>)}
0.9991539333333334
episode:  1 training step:  259 loss of agent  2 :  {0: tensor(0.6378, grad_fn=<MseLossBackward0>), 1: tensor(0.4123, grad_fn=<MseLossBackward0>), 2: tensor(0.3219, grad_fn=<MseLossBackward0>)}
0.9991506666666666
episode:  1 training step:  260 loss of agent  0 :  {0: tensor(0.6477, grad_fn=<MseLossBackward0>), 1: tensor(0.4123, grad_fn=<MseLossBackward0>), 2: tensor(0.3219, grad_fn=<MseLossBackward0>)}
0.9991506666666666
episode:  1 training step:  260 loss of agent  1 :  {0: tensor(0.6477, grad_fn=<MseLossBackward0>), 1: tensor(0.3767, grad_fn=<MseLossBackward0>), 2: tensor(0.3219, grad_fn=<MseLossBackward0>)}
0.9991506666666666
episode:  1 training step:  260 loss of agent  2 :  {0: tensor(0.6477, grad_fn=<MseLossBackward0>), 1: tensor(0.3767, grad_fn=<MseLossBackward0>), 2: tensor(0.5350, grad_fn=<MseLossBackward0>)}
0.9991474
episode:  1 training step:  261 loss of agent  0 :  {0: tensor(1.3772, grad_fn=<MseLossBackward0>), 1: tensor(0.3767, grad_fn=<MseLossBackward0>), 2: tensor(0.5350, grad_fn=<MseLossBackward0>)}
0.9991474
episode:  1 training step:  261 loss of agent  1 :  {0: tensor(1.3772, grad_fn=<MseLossBackward0>), 1: tensor(0.8354, grad_fn=<MseLossBackward0>), 2: tensor(0.5350, grad_fn=<MseLossBackward0>)}
0.9991474
episode:  1 training step:  261 loss of agent  2 :  {0: tensor(1.3772, grad_fn=<MseLossBackward0>), 1: tensor(0.8354, grad_fn=<MseLossBackward0>), 2: tensor(0.8708, grad_fn=<MseLossBackward0>)}
0.9991441333333333
episode:  1 training step:  262 loss of agent  0 :  {0: tensor(1.3134, grad_fn=<MseLossBackward0>), 1: tensor(0.8354, grad_fn=<MseLossBackward0>), 2: tensor(0.8708, grad_fn=<MseLossBackward0>)}
0.9991441333333333
episode:  1 training step:  262 loss of agent  1 :  {0: tensor(1.3134, grad_fn=<MseLossBackward0>), 1: tensor(0.8075, grad_fn=<MseLossBackward0>), 2: tensor(0.8708, grad_fn=<MseLossBackward0>)}
0.9991441333333333
episode:  1 training step:  262 loss of agent  2 :  {0: tensor(1.3134, grad_fn=<MseLossBackward0>), 1: tensor(0.8075, grad_fn=<MseLossBackward0>), 2: tensor(0.9907, grad_fn=<MseLossBackward0>)}
0.9991408666666667
episode:  1 training step:  263 loss of agent  0 :  {0: tensor(0.8507, grad_fn=<MseLossBackward0>), 1: tensor(0.8075, grad_fn=<MseLossBackward0>), 2: tensor(0.9907, grad_fn=<MseLossBackward0>)}
0.9991408666666667
episode:  1 training step:  263 loss of agent  1 :  {0: tensor(0.8507, grad_fn=<MseLossBackward0>), 1: tensor(0.8613, grad_fn=<MseLossBackward0>), 2: tensor(0.9907, grad_fn=<MseLossBackward0>)}
0.9991408666666667
episode:  1 training step:  263 loss of agent  2 :  {0: tensor(0.8507, grad_fn=<MseLossBackward0>), 1: tensor(0.8613, grad_fn=<MseLossBackward0>), 2: tensor(0.8582, grad_fn=<MseLossBackward0>)}
0.9991376
episode:  1 training step:  264 loss of agent  0 :  {0: tensor(1.1344, grad_fn=<MseLossBackward0>), 1: tensor(0.8613, grad_fn=<MseLossBackward0>), 2: tensor(0.8582, grad_fn=<MseLossBackward0>)}
0.9991376
episode:  1 training step:  264 loss of agent  1 :  {0: tensor(1.1344, grad_fn=<MseLossBackward0>), 1: tensor(0.3762, grad_fn=<MseLossBackward0>), 2: tensor(0.8582, grad_fn=<MseLossBackward0>)}
0.9991376
episode:  1 training step:  264 loss of agent  2 :  {0: tensor(1.1344, grad_fn=<MseLossBackward0>), 1: tensor(0.3762, grad_fn=<MseLossBackward0>), 2: tensor(0.6199, grad_fn=<MseLossBackward0>)}
0.9991343333333333
episode:  1 training step:  265 loss of agent  0 :  {0: tensor(0.2660, grad_fn=<MseLossBackward0>), 1: tensor(0.3762, grad_fn=<MseLossBackward0>), 2: tensor(0.6199, grad_fn=<MseLossBackward0>)}
0.9991343333333333
episode:  1 training step:  265 loss of agent  1 :  {0: tensor(0.2660, grad_fn=<MseLossBackward0>), 1: tensor(0.2388, grad_fn=<MseLossBackward0>), 2: tensor(0.6199, grad_fn=<MseLossBackward0>)}
0.9991343333333333
episode:  1 training step:  265 loss of agent  2 :  {0: tensor(0.2660, grad_fn=<MseLossBackward0>), 1: tensor(0.2388, grad_fn=<MseLossBackward0>), 2: tensor(0.5486, grad_fn=<MseLossBackward0>)}
0.9991310666666666
episode:  1 training step:  266 loss of agent  0 :  {0: tensor(0.5638, grad_fn=<MseLossBackward0>), 1: tensor(0.2388, grad_fn=<MseLossBackward0>), 2: tensor(0.5486, grad_fn=<MseLossBackward0>)}
0.9991310666666666
episode:  1 training step:  266 loss of agent  1 :  {0: tensor(0.5638, grad_fn=<MseLossBackward0>), 1: tensor(0.2562, grad_fn=<MseLossBackward0>), 2: tensor(0.5486, grad_fn=<MseLossBackward0>)}
0.9991310666666666
episode:  1 training step:  266 loss of agent  2 :  {0: tensor(0.5638, grad_fn=<MseLossBackward0>), 1: tensor(0.2562, grad_fn=<MseLossBackward0>), 2: tensor(0.3445, grad_fn=<MseLossBackward0>)}
0.9991278
episode:  1 training step:  267 loss of agent  0 :  {0: tensor(0.3471, grad_fn=<MseLossBackward0>), 1: tensor(0.2562, grad_fn=<MseLossBackward0>), 2: tensor(0.3445, grad_fn=<MseLossBackward0>)}
0.9991278
episode:  1 training step:  267 loss of agent  1 :  {0: tensor(0.3471, grad_fn=<MseLossBackward0>), 1: tensor(0.2381, grad_fn=<MseLossBackward0>), 2: tensor(0.3445, grad_fn=<MseLossBackward0>)}
0.9991278
episode:  1 training step:  267 loss of agent  2 :  {0: tensor(0.3471, grad_fn=<MseLossBackward0>), 1: tensor(0.2381, grad_fn=<MseLossBackward0>), 2: tensor(0.3230, grad_fn=<MseLossBackward0>)}
0.9991245333333333
episode:  1 training step:  268 loss of agent  0 :  {0: tensor(0.5690, grad_fn=<MseLossBackward0>), 1: tensor(0.2381, grad_fn=<MseLossBackward0>), 2: tensor(0.3230, grad_fn=<MseLossBackward0>)}
0.9991245333333333
episode:  1 training step:  268 loss of agent  1 :  {0: tensor(0.5690, grad_fn=<MseLossBackward0>), 1: tensor(0.4179, grad_fn=<MseLossBackward0>), 2: tensor(0.3230, grad_fn=<MseLossBackward0>)}
0.9991245333333333
episode:  1 training step:  268 loss of agent  2 :  {0: tensor(0.5690, grad_fn=<MseLossBackward0>), 1: tensor(0.4179, grad_fn=<MseLossBackward0>), 2: tensor(0.5139, grad_fn=<MseLossBackward0>)}
0.9991212666666667
episode:  1 training step:  269 loss of agent  0 :  {0: tensor(0.6206, grad_fn=<MseLossBackward0>), 1: tensor(0.4179, grad_fn=<MseLossBackward0>), 2: tensor(0.5139, grad_fn=<MseLossBackward0>)}
0.9991212666666667
episode:  1 training step:  269 loss of agent  1 :  {0: tensor(0.6206, grad_fn=<MseLossBackward0>), 1: tensor(0.7171, grad_fn=<MseLossBackward0>), 2: tensor(0.5139, grad_fn=<MseLossBackward0>)}
0.9991212666666667
episode:  1 training step:  269 loss of agent  2 :  {0: tensor(0.6206, grad_fn=<MseLossBackward0>), 1: tensor(0.7171, grad_fn=<MseLossBackward0>), 2: tensor(0.5144, grad_fn=<MseLossBackward0>)}
0.999118
episode:  1 training step:  270 loss of agent  0 :  {0: tensor(0.5955, grad_fn=<MseLossBackward0>), 1: tensor(0.7171, grad_fn=<MseLossBackward0>), 2: tensor(0.5144, grad_fn=<MseLossBackward0>)}
0.999118
episode:  1 training step:  270 loss of agent  1 :  {0: tensor(0.5955, grad_fn=<MseLossBackward0>), 1: tensor(0.2683, grad_fn=<MseLossBackward0>), 2: tensor(0.5144, grad_fn=<MseLossBackward0>)}
0.999118
episode:  1 training step:  270 loss of agent  2 :  {0: tensor(0.5955, grad_fn=<MseLossBackward0>), 1: tensor(0.2683, grad_fn=<MseLossBackward0>), 2: tensor(0.3697, grad_fn=<MseLossBackward0>)}
0.9991147333333333
episode:  1 training step:  271 loss of agent  0 :  {0: tensor(2.2815, grad_fn=<MseLossBackward0>), 1: tensor(0.2683, grad_fn=<MseLossBackward0>), 2: tensor(0.3697, grad_fn=<MseLossBackward0>)}
0.9991147333333333
episode:  1 training step:  271 loss of agent  1 :  {0: tensor(2.2815, grad_fn=<MseLossBackward0>), 1: tensor(0.9340, grad_fn=<MseLossBackward0>), 2: tensor(0.3697, grad_fn=<MseLossBackward0>)}
0.9991147333333333
episode:  1 training step:  271 loss of agent  2 :  {0: tensor(2.2815, grad_fn=<MseLossBackward0>), 1: tensor(0.9340, grad_fn=<MseLossBackward0>), 2: tensor(1.1461, grad_fn=<MseLossBackward0>)}
0.9991114666666666
episode:  1 training step:  272 loss of agent  0 :  {0: tensor(1.6173, grad_fn=<MseLossBackward0>), 1: tensor(0.9340, grad_fn=<MseLossBackward0>), 2: tensor(1.1461, grad_fn=<MseLossBackward0>)}
0.9991114666666666
episode:  1 training step:  272 loss of agent  1 :  {0: tensor(1.6173, grad_fn=<MseLossBackward0>), 1: tensor(0.6651, grad_fn=<MseLossBackward0>), 2: tensor(1.1461, grad_fn=<MseLossBackward0>)}
0.9991114666666666
episode:  1 training step:  272 loss of agent  2 :  {0: tensor(1.6173, grad_fn=<MseLossBackward0>), 1: tensor(0.6651, grad_fn=<MseLossBackward0>), 2: tensor(0.7831, grad_fn=<MseLossBackward0>)}
0.9991082
episode:  1 training step:  273 loss of agent  0 :  {0: tensor(1.4375, grad_fn=<MseLossBackward0>), 1: tensor(0.6651, grad_fn=<MseLossBackward0>), 2: tensor(0.7831, grad_fn=<MseLossBackward0>)}
0.9991082
episode:  1 training step:  273 loss of agent  1 :  {0: tensor(1.4375, grad_fn=<MseLossBackward0>), 1: tensor(0.4728, grad_fn=<MseLossBackward0>), 2: tensor(0.7831, grad_fn=<MseLossBackward0>)}
0.9991082
episode:  1 training step:  273 loss of agent  2 :  {0: tensor(1.4375, grad_fn=<MseLossBackward0>), 1: tensor(0.4728, grad_fn=<MseLossBackward0>), 2: tensor(0.3152, grad_fn=<MseLossBackward0>)}
0.9991049333333333
episode:  1 training step:  274 loss of agent  0 :  {0: tensor(0.6550, grad_fn=<MseLossBackward0>), 1: tensor(0.4728, grad_fn=<MseLossBackward0>), 2: tensor(0.3152, grad_fn=<MseLossBackward0>)}
0.9991049333333333
episode:  1 training step:  274 loss of agent  1 :  {0: tensor(0.6550, grad_fn=<MseLossBackward0>), 1: tensor(0.5326, grad_fn=<MseLossBackward0>), 2: tensor(0.3152, grad_fn=<MseLossBackward0>)}
0.9991049333333333
episode:  1 training step:  274 loss of agent  2 :  {0: tensor(0.6550, grad_fn=<MseLossBackward0>), 1: tensor(0.5326, grad_fn=<MseLossBackward0>), 2: tensor(0.4225, grad_fn=<MseLossBackward0>)}
0.9991016666666667
episode:  1 training step:  275 loss of agent  0 :  {0: tensor(0.5886, grad_fn=<MseLossBackward0>), 1: tensor(0.5326, grad_fn=<MseLossBackward0>), 2: tensor(0.4225, grad_fn=<MseLossBackward0>)}
0.9991016666666667
episode:  1 training step:  275 loss of agent  1 :  {0: tensor(0.5886, grad_fn=<MseLossBackward0>), 1: tensor(0.3285, grad_fn=<MseLossBackward0>), 2: tensor(0.4225, grad_fn=<MseLossBackward0>)}
0.9991016666666667
episode:  1 training step:  275 loss of agent  2 :  {0: tensor(0.5886, grad_fn=<MseLossBackward0>), 1: tensor(0.3285, grad_fn=<MseLossBackward0>), 2: tensor(0.4893, grad_fn=<MseLossBackward0>)}
0.9990984
episode:  1 training step:  276 loss of agent  0 :  {0: tensor(0.5288, grad_fn=<MseLossBackward0>), 1: tensor(0.3285, grad_fn=<MseLossBackward0>), 2: tensor(0.4893, grad_fn=<MseLossBackward0>)}
0.9990984
episode:  1 training step:  276 loss of agent  1 :  {0: tensor(0.5288, grad_fn=<MseLossBackward0>), 1: tensor(0.2711, grad_fn=<MseLossBackward0>), 2: tensor(0.4893, grad_fn=<MseLossBackward0>)}
0.9990984
episode:  1 training step:  276 loss of agent  2 :  {0: tensor(0.5288, grad_fn=<MseLossBackward0>), 1: tensor(0.2711, grad_fn=<MseLossBackward0>), 2: tensor(0.4061, grad_fn=<MseLossBackward0>)}
0.9990951333333333
episode:  1 training step:  277 loss of agent  0 :  {0: tensor(0.7335, grad_fn=<MseLossBackward0>), 1: tensor(0.2711, grad_fn=<MseLossBackward0>), 2: tensor(0.4061, grad_fn=<MseLossBackward0>)}
0.9990951333333333
episode:  1 training step:  277 loss of agent  1 :  {0: tensor(0.7335, grad_fn=<MseLossBackward0>), 1: tensor(0.3043, grad_fn=<MseLossBackward0>), 2: tensor(0.4061, grad_fn=<MseLossBackward0>)}
0.9990951333333333
episode:  1 training step:  277 loss of agent  2 :  {0: tensor(0.7335, grad_fn=<MseLossBackward0>), 1: tensor(0.3043, grad_fn=<MseLossBackward0>), 2: tensor(0.4051, grad_fn=<MseLossBackward0>)}
0.9990918666666667
episode:  1 training step:  278 loss of agent  0 :  {0: tensor(0.6593, grad_fn=<MseLossBackward0>), 1: tensor(0.3043, grad_fn=<MseLossBackward0>), 2: tensor(0.4051, grad_fn=<MseLossBackward0>)}
0.9990918666666667
episode:  1 training step:  278 loss of agent  1 :  {0: tensor(0.6593, grad_fn=<MseLossBackward0>), 1: tensor(0.4328, grad_fn=<MseLossBackward0>), 2: tensor(0.4051, grad_fn=<MseLossBackward0>)}
0.9990918666666667
episode:  1 training step:  278 loss of agent  2 :  {0: tensor(0.6593, grad_fn=<MseLossBackward0>), 1: tensor(0.4328, grad_fn=<MseLossBackward0>), 2: tensor(0.5463, grad_fn=<MseLossBackward0>)}
0.9990886
episode:  1 training step:  279 loss of agent  0 :  {0: tensor(0.6715, grad_fn=<MseLossBackward0>), 1: tensor(0.4328, grad_fn=<MseLossBackward0>), 2: tensor(0.5463, grad_fn=<MseLossBackward0>)}
0.9990886
episode:  1 training step:  279 loss of agent  1 :  {0: tensor(0.6715, grad_fn=<MseLossBackward0>), 1: tensor(0.2651, grad_fn=<MseLossBackward0>), 2: tensor(0.5463, grad_fn=<MseLossBackward0>)}
0.9990886
episode:  1 training step:  279 loss of agent  2 :  {0: tensor(0.6715, grad_fn=<MseLossBackward0>), 1: tensor(0.2651, grad_fn=<MseLossBackward0>), 2: tensor(0.4012, grad_fn=<MseLossBackward0>)}
0.9990853333333334
episode:  1 training step:  280 loss of agent  0 :  {0: tensor(0.4517, grad_fn=<MseLossBackward0>), 1: tensor(0.2651, grad_fn=<MseLossBackward0>), 2: tensor(0.4012, grad_fn=<MseLossBackward0>)}
0.9990853333333334
episode:  1 training step:  280 loss of agent  1 :  {0: tensor(0.4517, grad_fn=<MseLossBackward0>), 1: tensor(0.5087, grad_fn=<MseLossBackward0>), 2: tensor(0.4012, grad_fn=<MseLossBackward0>)}
0.9990853333333334
episode:  1 training step:  280 loss of agent  2 :  {0: tensor(0.4517, grad_fn=<MseLossBackward0>), 1: tensor(0.5087, grad_fn=<MseLossBackward0>), 2: tensor(0.5252, grad_fn=<MseLossBackward0>)}
0.9990820666666667
episode:  1 training step:  281 loss of agent  0 :  {0: tensor(2.5189, grad_fn=<MseLossBackward0>), 1: tensor(0.5087, grad_fn=<MseLossBackward0>), 2: tensor(0.5252, grad_fn=<MseLossBackward0>)}
0.9990820666666667
episode:  1 training step:  281 loss of agent  1 :  {0: tensor(2.5189, grad_fn=<MseLossBackward0>), 1: tensor(0.6154, grad_fn=<MseLossBackward0>), 2: tensor(0.5252, grad_fn=<MseLossBackward0>)}
0.9990820666666667
